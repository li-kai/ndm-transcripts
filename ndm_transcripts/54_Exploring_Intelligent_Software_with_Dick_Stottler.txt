[ Music ]

[ Music ]

[ Music ]

[ Music ]

[ Music ]

[ Music ]

[ Music ]

Speaker 1: The Naturalistic Decision Making podcast with Brian Moon and Laura Millitello. This podcast series brings you interviews with leading NDM researchers who study and support people who make decisions under stress.

[ Music ]

[ Music ]

Speaker 2: Welcome to the Naturalistic Decision Making podcast. This is Brian Moon from Paragean Technologies.

Speaker 1: And I'm Laura Millitello from Applied Decision Science. Our special guest today is Dick Stottler. Let me tell you that one of the cool things about Dick is that he's been developing innovative artificial intelligence since 1988. Part of the innovation is that he's developed tools to support work in high-stakes domains and collaborated with NDM researchers along the way. I've known Dick since the early 1990s. Everyone's talking about AI today, but Dick really has deep experience in this space. So let me read you his bio to give you a little more context. Dick co-founded Stottler Hanky in 1988 as a software company dedicated to providing practical solutions to difficult problems by skillfully drawing upon a large repertoire of artificial intelligence technologies. Under Dick's leadership, Stotler Hanky has grown steadily and profitatably into a 60-person research and software development company with distinctive expertise in automated planning and scheduling, intelligent tutoring systems, intelligent simulation, sensor data fusion, and autonomous systems. Dick combines a strong applied research record in artificial intelligence with practical experience in rapid and efficient knowledge engineering. He has led the development of intelligent tutoring systems that encode the expertise of instructors to provide practice-based learning and automated evaluation of student performance. Subject areas include Navy tactics, Army tactics, command and control, sonar data analysis, astronaut training, helicopter cockpit operations, and Battlefield Emergency Medicine. He led the development of intelligent planning systems for NASA Space shuttle missions, satellite communications and sensor scheduling, and aircraft assembly. He also led the development of intelligent systems that autonomously control groups of drones to automatically plan and execute tasks to achieve mission objectives and react to unpredictable events. Dick has written and presented more than 80 papers and articles in publications such as the proceedings of the International Joint Conference on Artificial Intelligence. He received his BS in engineering from Cornell University and MS in Computer science from Stanford University.

Speaker 2: Welcome, Dick. Thanks for joining us. We're really happy to get a chance to talk to you today.

Speaker 3: Oh, thank you for the kind words.

Speaker 2: So, I always like to hear how people got started. I know you started Stotler Hanky in 1988 and the vision as I understand it was to use artificial intelligence to solve problems that defy traditional approaches. I think I grabbed that phrase from your website. I'm wondering if you'd just tell us a little bit how you got started.

Speaker 3: Sure. So, when I graduated from from Stanford with a focus on artificial intelligence, I was looking for an AI job. There weren't that many back in 1987. I ended up working at the AI division at SAIC and I was very happy there. I might still be there except after about seven months, they fired my boss and started treating us really terribly and kind of gave us this ultimatum where we gave us two bad choices and they wanted us to vote with our feet. So I voted with my feet and I left. But, you know, while I was at SIC, I had, you know, I was working for clients, one of whom was Gary Klein at Klein Associates. And when Gary Klein found out I was no longer at AIC, he said, hey, Dick, why don't you just work for me as a consultant? And literally I looked around, I was unemployed, otherwise, so I said, sure. So, actually Gary Klein is very much responsible for the existence of Stalinski Associates.

Speaker 2: Well, I don't think I knew that story. That's awesome.

Speaker 3: Well, you can kind of tell from the name by the way.

Speaker 2: Ah.

Speaker 3: I've never been very creative.

Speaker 2: So, so it was pretty early in your career when you got connected with the NDM community. Do you remember how you?

Speaker 3: Yes, actually, it's it's all it's all really an extension of the same story. So, the work I was doing with Gary Klein was specifically to turn the results of their cognitive task analysis into executable software. Basically, to create software that did the same thing as the human beings that the CTA was being performed on. So, really to try to mimic those naturalistic decision processes of humans in software. Most of our work still is that way.

Speaker 2: Do you remember what that first project with the domain was? Was it firefighting or?

Speaker 3: Yes, so, if you go way back, there was something that Gary Klein, I think he probably invented this phrase, but I don't I don't think it survived called comparison based prediction. There was a more well-known analog. And comparison based prediction from Gary Klein's perspective is the way humans often make decisions. And it's a very typical example would be, how do you gauge the value of a house? Well, you look at comparable houses, you make some adjustments, you know, for square footage, a pool or not, that kind of thing. And there's a very analogous thing in the AI field called case-based reasoning, which is basically the same thing, you know, the idea that human beings often solve problems by remembering a past solution and adapting it to fit the current circumstances. And we still do a lot of case-based reasoning work to this day. Not many people do anymore. One of the advantages of being in business for as long as we have is we've kind of collected a lot, a large number of AI techniques. So we have a, we have a lot of arrows in our quiver.

Speaker 2: Nice. And so that first project, do you remember what the domain was?

Speaker 3: Oh, so it was well, it was really, it was trying to be a little bit generic on the concept of comparison based prediction. I can't remember, I think one of the first domains we applied it to was manufacturing, manufacturing cost estimation. So we were working, I remember the system was called Bitter's Associate. I mean we were working with a relatively small manufacturing company and I say we, I mean the big we, mostly Gary Klein, was working with a manufacturing company in Ohio. He was doing all the cognitive task analysis then and sort of feeding us the the results. And we were turning that into software that basically did the same function. created an automated way of estimating manufacturing jobs.

Speaker 2: Yeah, so I was actually peripherally involved in that. I was just starting at Klein Associates and I remember going to that company and talking to those folks in the manufacturing environment.

Speaker 3: So. Yeah, I want to say there was something, I think the first word in their name was friend or friend, F R E U N D.

Speaker 2: Yeah, I don't remember either. Yeah. That's been a long time.

Speaker 3: Yeah.

Speaker 2: So, do you find that some things from your interactions with the NDM community inform the way you think about AI or solutions?

Speaker 3: Well, yeah, I mean, I would say it's again, it's mostly what we do. I mean, my working definition at Dalla Hanky of artificial intelligence is the mimicking of human thought processes to solve useful problems. That means we have to understand what the human thought processes are. And so that's why we on almost every project are performing a CTA and trying to understand, you know, the way human beings, especially in time critical settings, make critical decisions. So, you know, I think that's funny or strange when I give people lay people that definition of AI, they think, well, isn't that what AI is? But in reality, it's a shockingly unique approach to artificial intelligence. There's almost nobody that does that.

Speaker 2: Yeah, no, I completely agree. I don't encounter many other people that think about really understanding how the humans do this and what the cognitive complexity is to inform the design of the AI. I completely agree. Yeah.

Speaker 3: Yeah, and one thing about, you know, when you really look at natural decision processes, natural decision making, human beings are really, the human brain is extremely complicated and human beings have very, very complex algorithms that they execute. And I think so many people in the AI world don't really appreciate that.

Speaker 2: Interesting. Yeah.

Speaker 3: So, so that was many years ago. What sort of things are you and your team focused on today?

Speaker 2: So, one thing about Dollar Inky is we do lots and lots of different things. At any given time, we probably have around 40 projects going on and they're all over the map. So I'm going to naturally focus a little bit on the stuff that I do because otherwise we'd be here all day. So, I grew up in Coco Beach, Florida. So I have always had an interest in space applications. So, a lot of my work relates to work that has applicability to to space. So we do a lot of automated planning and scheduling systems for space applications. A current project we have is going to be on board Astrobotic's Vault Rover on the moon when it launches. That's a system that we have a generic set of projects or technologies looking at spacecraft, subsystem anomaly detection and resolution and that one is specifically looking at the mechanical systems and and very specifically, the Vault Rovers is is very interesting. If you look at it, it looks just impossible because it's got a solar mass that's 60 feet in the air. So you can imagine a relatively small rover and a mass that just sticks up 60 feet in the air and it's top heavy in the top of that because it's only the top half that have solar cells. So it's just it's just a tipping accident waiting to happen. It unfurls when it's not moving, but the concern is that it thinks it's on stable ground, but it's not. And so at some point during or after it's unfurled, there's some kind of instability, there's a moonquake, there's some internal vibration, something causes a wheel or more to slip downward. And when your mast is 60 feet in the air, it turns out their factor of safety is very small. They can't let it tilt more than three degrees. So, if something like that happens, they have to react very fast. There's just no time for the signal to go from the moon to the earth and back again for some human to react in time. So, they have to have an on board autonomous system quickly determine there's a problem in the mechanical system where we consider the soil under the wheels part of the mechanical system and very quickly eliminate any possibility of a false alarm and take corrective action before basically it tips over. So, that's one of the things that we're doing. We also have a project for wildfire fighting, funded by NASA, but it's a terrestrial application to use the team of drones to autonomously task allocate with the primary goal of mapping the fire perimeter and its movement in real time and sending that back to firefighters. I think you probably knew Laura, I think one of Klein's early projects was with firefighting and so you know that it's a very, very dangerous application. We're working with former fire chiefs and they are explicitly stating that the goal is to not go to anymore funerals.

Speaker 2: So that's a good range of projects because on the one end with the moon rover, that is humans out of the loop if you will. It's just about the technology and the technology making its own decisions whereas the second one is really trying to as you was talking about before, understand how people do the task and then build tools to enable that. I guess my question is are you finding that you're working at one end of the spectrum more than the other these days or is it?

Speaker 3: No, it's all over the place. I mean, I would say, you know, it just depends. Just as an example, a very kind of specific example. We build a lot of automatic planning and scheduling systems, automatic intelligent planning and scheduling systems. And when people ask me, I say about half of those systems have user interfaces and about half don't. So, I mean, roughly half of them are creating an automatic schedule with the concept that a human being is going to make the final say, you know, either approve it or make edits to it or adjust it in some way and have it rerun. Whereas on the other end of the spectrum for whatever reason, there's just no, there are no humans in the loop at all. But again, even when there's no humans in the loop, we are closely mimicking the the thought processes of the previous humans who used to do the job. And that gives them a lot of understanding and trust in the system and it does make the system much more explainable to them.

Speaker 2: So I am wondering, so I feel like I'm often in situations where people are saying, we need to create something completely autonomous and get the humans out of the loop, get rid of all that human error. And then a lot of times I'm pointing out, well, really humans are going to know things. There's no way the automation can. Anyway, do you find yourself having these conversations?

Speaker 3: No, not, no, not, not really because I think in a lot of our applications, they're very, they're very life critical or very high stakes and it tends to be people, I'm tending to deal with people who are more skeptical. You can't automate me. I've heard that a lot. So, most people I deal with, I'm having to convince them to get the humans out of the loop and put them on the loop, right? In a more of a monitoring stage. Yeah, I don't, I mean, I think maybe at some level people are trying to push for more automation just because of all the advantages of speed of turnaround, lack of labor, but in most of the domains I work in, they want humans involved.

Speaker 2: It makes sense.

Speaker 3: I mean, it's only what you can't be. Like an autonomous group of drones if they're autonomous for a reason, there's no communication or the communication is very poor for some reason.

Speaker 2: Yeah, yeah. I mean, it makes sense. You're working in a lot of high stakes domains where there's all kinds of last minute perturbations and things where?

Speaker 3: Yeah, and and just to put out something that you said, when we build these automatic intelligent planning and scheduling systems, there are often things that we would just say are unmodeled, right? There's just some preference in there that no one's told us about or that or doesn't come up very often and that some human being, and some of those by the way, especially when they're humans involved, when you're scheduling humans to do things, there's all kinds of weird little preferences and stuff and bathroom breaks and all kinds of strange things that they may not tell you explicitly. And so the scheduling, you know, if they no one's ever told us, the scheduling system doesn't know about it. So if there's all kinds of things they might want to edit to take into account informations never been presented to us or to the software.

Speaker 2: So I'm sure you work on multi-disciplinary teams and people who don't know much about AI. Do you ever find yourself kind of debunking some kind of myths or beliefs people have about AI?

Speaker 3: I would say it's gone back and forth. Let me say this, the most common myth I have to debunk now is to explain that the field of artificial intelligence is not just exactly the same as neural net based machine learning. So people think that's it. They know about large language models and systems that recommend the best book for you or the movie based on your viewing habits. You know, these very typical or systems that recognize pictures of cats on the internet. Those kind of applications that are all, you know, machine learning based on neural networks and they think that's all there is. So I have to explain, no, that's just one part of AI. There's all these other things that I describe a lot of what we do as symbolic processing. Something that a human being can explain to you in words and that we can implement in a way that's very similar to what they do. I mean, we do use machine learning and we do use neural networks, but it's a relatively small part of what we do. And so anyway, that's the current myth we have to debunk all the time. I think in previous eras, there would be a pendulum that would swing back and forth between thinking AI could do everything and solve every problem magically, which is maybe where we sort of are now, or AI can't do anything because there's been all these high profile failures and overhyped publicity from some practitioners. So, I've seen that pendulum swing back and forth quite a bit. And again, as I said, the other, the one that we used to frequently encounter, you can't automate me.

Speaker 2: Yeah. So, you said people seem to be unaware of symbolic processing. So, give us an example. What is that?

Speaker 3: Well, it's pretty much every project we do, but as an example, our first example of a lot of what I've just spoke about, our first large project of Andre and myself, Andre Hanky, by the way, was to automate the scheduling of the processing for the space shuttle. And the main planner there was a guy named Tom Overton and he's the first one who I ever encountered who said, you cannot automate me. And we spent, you know, a lot of time understanding how they made scheduling decisions, how they decided which shuttles would be next in the VAB and into the shuttle processing facility and how they decided when they had to send something out to Palmdale, California, which it happened every few years. Just basically, how they decide what's the next thing to schedule and then what's the resource to put against that task? You know, so we were basically performing a cognitive task analysis and then we implemented software that basically did exactly the same thing, involved exactly the same processes that they they executed themselves inside their head. But of course, you know, a computer is faster than a human being. So not surprisingly, you know, what used to take them weeks or days could now take them hours or minutes. And so Tom Overton actually became our biggest fan because, you know, suddenly he could get a lot more work done. Suddenly, his small office could be much more responsive to lots of what if studies. What if we have a new vehicle that's competing with a shuttle for the same resources? How's that going to affect our flight rate? Hey, what if we want to build a new facility, a safe haven in the case of hurricanes? Can you use this scheduling tool to play that what if game and see how much money we saved? And they saved enough that they spent half a billion dollars based on the results of our software to build a safe haven facility for the space shuttle. So, you know, every project that's what we do. How does a human do it? Okay, let's implement that.

Speaker 2: So, so what you're describing is how does a human do a task, which of course, which cognim task analysis helps us to understand, that feels to me to be a different way of thinking about sort of computational NDM than what a couple of us were contributing to a project in the early 2000s to build a computational recognition prime model, right? Computational RPD model. And it was based on a driver domain, so it wasn't the most complex of domains that we could have worked in. And I think generally folks on the cognitive side sort of walked away at that point thinking, well, this is hard to do and it's probably never going to happen. But to me that felt more like what you said earlier about trying to mimic the way that humans approach the world and and make decisions. So that's more of a general way of human cognition that could go in multiple domains, but it's a generic model.

Speaker 3: Yeah, talk to those terms.

Speaker 2: So, we've done a series of projects that are recognition primed related.

Speaker 3: So, you know, looking at a situation and recognizing aspects of that situation and having that prime you to look for additional cues. That's a large part of several of our case-based reasoning projects. And, uh, I'll I'll explain one of them. So, we do a fair amount of work with the satellite Control Network. This is this network of large antennas that are used to command and control primarily DOD satellites, but other satellites too, NASA, some and Noah, some too. And a lot of times they experience interference at the antenna level and they need to track down the source of that interference. You know, they're they're supposed to own the spectrum they're using. And so when there's interference, when there's electromagnetic interference, someone is not doing what they're supposed to do and they need to track that down. And so, you know, they're looking at this signal, basically a frequency, spectrum analysis of the signal and it has a certain look about it depending on what kind of signal it is. And so they can kind of look at that and say, oh, I know what that signal is. That's a video compression signal. Somebody's flying a plane out. Oh, and and is it moving, right? Because they can track it poorly but get somewhat an idea, is it static? Is it stationary? Is it coming from space? Is it moving? Is it moving fast? And then what does the signal look like? Oh, that looks like a video compression signal or that I've seen that before. I'll bet that's an F-15. Let's go see if there's an F-15 flying around. Or or that's not moving. And the point is, because they kind of recognize some aspects or some evidence from the signal and some of its behaviors, then that prompts sort of an investigative process. And those investigative processes are different depending on the aspects of the signal. So in that sense, the characteristics of the signal, you know, they recognize that and that prompts them or primes them and then kind of execute different investigative processes.

Speaker 2: So an example of recognition prime decision making happening, but still that feels different to me than building an RPD engine, if you will, right? So, a thing that can go do RPD, I guess, is what I'm getting at.

Speaker 3: Yeah, so again, just to be clear, I mean, we we automated that process, right? We took everything the human did and made an executable system that automatically did all those things I just described. And I I have this problem I try to pound out of the people I work with who are younger, but I still have the same problem. We have a tendency to overly identify with our software. So when we talk, it's not clear if we're talking about the software doing something or a human doing something. So let me just say up front, everything we do, it's software doing it, okay? But I tend to forget to say that explicitly. So we created execution software. Now, in terms of, you know, something that's sort of more generic, which is, you know, general AI problem solving is is is pretty much an unsolved problem. But I'll give you an example that was sort of similar. I mentioned comparison based prediction and case-based reasoning. So we sold a tool for a long time called Esteam along with Jim King, partner in that endeavor, which basically was a tool to facilitate the development of case-based reasoning systems. And at one time, it was the widest selling case-based reasoning development tool, you know, on the planet. But again, you know, that wasn't the same, it took inputs. You could use it as a non-programmer, but it took some effort to give it the information it needed to execute that sort of case-based reasoning concept of retrieving similar cases and then processing those to produce some kind of result that was useful for the current circumstances.

Speaker 2: Nice. I'm going to switch gears here a little bit. I'm wondering what you have to say to people who are worried that AI is going to take over the world and humans will wind up serving the AI.

Speaker 3: Yeah, so, so let me say this, they're worried about the wrong thing. All right? AI systems aren't conscious. They don't have consciousness. They're not going to decide on their own to do something, okay? But AI is a tool and like any tool, you know, it can be applied to, you know, good or evil. So, if there is some massive destruction that's caused by AI, it's because some human being used AI to create a destructive system. The human is essentially providing the evil intent and the consciousness required. And a small example of that is humans now using AI to spread misinformation. Again, it wasn't the AI decided, oh, I'm going to spread misinformation. You know, some human being created systems to do that and are using AI as tools to do that. I'm not letting you sleep any better at night, but it's, but it's a different problem.

Speaker 2: Yeah. So I think some people have a belief that once you develop the AI, especially with the neural net, it is starting to be able to evolve ways that you didn't predict. Like it you turn it loose and it evolves, There's no question that most neural net based machine learning systems are essentially black boxes that can behave unpredictably. But again, they're hooked up to something, right? And they can only do what they're hooked up to be involved with. But you are absolutely right that that unpredictability is a real shortcoming in many domains. I mean, in some domains it doesn't matter. You know, I mean, do you really care if somebody mis classifies a picture of a dog as a husky versus a German shepherd, it's probably not going to matter. But, you know, do you care if you've given bad information to a firefighter on where the fire is and how fast it's moving? Well, yes, you care. It's not good enough to be 90% accurate or 95% accurate or even 98% accurate. It's just not. You have to be perfect. So, in our domains, that notion that using symbolic processing means that some human being can understand what it's doing, how it's going to work in a different circumstances and, you know, can know if it's likely to be outside its own parameters. I think one of the advantages, for example, in case-based reasoning is it automatically knows when it's outside its knowledge base because if it can't find a similar case, it can't find a similar case. It's not going to give you a bad answer. It's going to give you no answer.

Speaker 3: Yeah, I think that is really a good point. I think one of the things we hear on the human side is that the technology doesn't know when it's reaching the edges of its capability.

Speaker 2: Yeah, it's one of the reasons I will never, I will never be a passenger in an autonomous vehicle. The only way I would do that is if I wrote the software myself, okay? My life is more important to me than what some, you know, neural net thinks. And if you look at the decades that have been poured into using a neural net machine learning approach to autonomous vehicles and the billions and billions of dollars, you have to wonder, well, you know, at some point, when do you give up? And B, you know, if you'd used a different approach and spent all that money and all that time, could you have solved the problem by now?

Speaker 3: So, isn't the other side though suggesting that what we really need is just more and more cases?

Speaker 2: So, yes, I mean, there's a lot to be said for that. I will say this though, I mean, if you look at a lot of the failures, a lot of times, they're one-off cases. But, one way of thinking about case-based reasoning is, it's a machine learning technology, but it's a symbolic machine learning technology. And yes, more cases make the system better. Just like more training data, more cases make a machine learning system better. But again, there's the difference between knowing you're outside your experience base or not. Now, I will say this, I don't want to dump water completely on neural net machine learning systems. We use them and we use them a fair amount. And we also, you know, use them in sort of a hybrid technology where we're using both symbolic and machine learning systems, either sort of very intimately commingled or separately where each one's a check on the other.

Speaker 3: So I imagine one of your advantages having been in the AI field for many years is having some just experiential understanding of or what the limitations are of various approaches and what's likely to work safely here versus there. Whereas some folks new to this field may they may just know one technique and they're trying to use that for everything.

Speaker 2: Yeah, absolutely. I mean, having lots of arrows in our quiver is is really what I was referring to there. I will say that when I trying to teach cognitive task analysis process, the first thing I have to do is tell them to forget everything they know. Right? Because you don't want to bias the interviewing process by what you think the technology this appcal is going to be because you need to basically have that be told to you. Not in so many words, of course, but you need to understand what the cognitive processes are and then say, oh, this is like this technique or this is like these three techniques combined.

Speaker 3: Yeah, I mean, I tell people the same thing when I teach them CTAs, you have to be open to being surprised. Like, don't assume you know, even if you've read a lot about this, like get the person to tell you. You want to know what it's like from their perspective.

Speaker 2: Yeah. Yeah.

Speaker 3: It feels like the explosion of interest over the past couple years, especially around the generative AI piece is this feeling that when people use it, you know, you mentioned the word mimic earlier, it just feels really good in terms of how well it is mimicking certain outputs. At least on the text side. The visual side feels a little different to me, but I'm wondering if you can sort of comment on, you mentioned your customer earlier who you turned around and became your biggest champion. I'm wondering if you can also reflect just sort of on the the general public outlook if you will on AI over the past couple of decades because it certainly feels like the past couple of years have been explosive, but you've been seeing this for several decades. Have you seen those kinds of shifts?

Speaker 2: I I mean, we'll say, I have seen this pendulum swing back and forth. There was something called the AI winter. I think it probably was like the early 90s. And that was, you know, overhype, disappointment, some successes, the pendulum would swing back and forth. I will say this feels different. And I'll say it goes back a little further than just the large language models and generative AI. And I want to go back to something you said a little bit about humans using these large language models and generative AI feel like it seems like it mimics what a human would do. But when you say that, you mean the results. You mean like the input output and it's still a black box as opposed to what we do, which is actually mimicking the process that they're going through to take those inputs and produce an output. So, same end result, input output, but completely different about, you know, what's going on inside. And I think the reason things have changed is, I guess it was about 10 years ago when they started really using really deep neural networks to do a lot of image processing and there's all this data on the internet, all these pictures on the internet basically, and there was a lot more computational power. So there was a lot of success in domains where you had a lot of data, a lot of sensor data, a lot of images. So the this department started to become more tangible then and it was based on some reality. I mean, if if 90% is good enough, yeah, there's some pretty good systems out there for doing things like that that are also kind of useful from an everyday consumer perspective. And then as you point out for a couple of years ago, there's got a lot of excitement around generative AI and large language models. And again, for the same kind of reason that pretty good, you know, 98% is good enough. You're going to probably look at it and edit it to more, you know, you're not going to just blindly send it out. Hopefully not, although occasionally people get in trouble for doing that. And it's something that everybody might be using, right? So now instead of having tens of thousands of customers using AI, you have hundreds of millions in this country alone, you know, probably billions if you count the whole planet. So it's a real quantitative difference. I don't really think it's going to, the pendulum's not going to swing wildly back and everybody's going to reject this. It's going to be little minor things where they're very excited about some application and sometimes there's going to be some failure and, you know, there will be some retreat in specific application areas.

Speaker 3: Yeah, it does feel like the other big difference is as you mentioned the sort of consumer perspective. People can now put their fingertips on tools and have those tools do something, which again, for folks who had never been around software engineering, just feels different. Like I can actually make an AI do something for me isn't something that we've really had before, whereas you and your team are touching this stuff all the time. So it doesn't feel quite as novel to you, but that feels like another difference to me just in the sort of general public outlook.

Speaker 2: Yeah, and I don't think that's going backward in any significant way.

Speaker 3: So what do you think the next big advancements will be in AI?

Speaker 2: Yeah, so, a couple of things. So one, I think, so first let me say what I'm going to say is is very contrary to popular opinion, okay? But I think people are starting to see some of the limits and issues with generative AI and large language models and deep neural networks and I think there's going to be at least a little bit of a pendulum swing back to more symbolic processing techniques often in combination with large language models and machine learning, but that there are some problems and limitations that can best be addressed with just a different technology, not just throwing more data at something. Let me give you an example that we engaged in. So in some applications, I'm not times it for the military, let's say you build a system that recognizes ships and it's going to have to be totally autonomous for whatever reason. Well, you know, the enemy doesn't give us their ships so we can film them from every possible angle and every possible phenomenology. We literally have, we have no data or very little data. But we generally, you know, know what the ships look like and and how they're built. And so a machine learning person might say, oh, well, let's just generate a ton of synthetic images and just throw machine learning at it. The problem when you do that is it's actually a very hard problem to have a machine learning system that you've trained on only synthetic data and then you apply it to real data, you know, real pictures with, you know, real problems and issues. What happens is the machine learning system tends to learn artifacts of the rendering process. It's very hard to get around that. But if you say, so, so some people worked on that problem, they didn't have much luck with just a machine learning approach and then we said, okay, what does a human do? How does a human recognize a ship? Well, generally, they look at the silhouette, right? Basically the vertical height and then we said, okay, let's instead of just throwing pixels at the machine learning system, let's think about the way the human solves it. Instead of throwing pixels, we'll process the data to be basically these vertical bars. So we're transforming an image, which is a bunch of pixels into a set of features. And those features are basically the vertical distance above the water. And then throw that at a machine learning system. And it performed, it performs amazingly well. Like in the upper 90s in percent. Another thing about that is those features are exactly the same whether it's synthetic data that you've generated or whether they're actual pictures and you've done the processing on those. So, in some sense, you've transformed your training data and the data you're going to have to perform on real time into a common representation that is identical to each other. And we proved the concept out, you know, using, you know, our own warships and it was uh it was as I said extremely accurate. So that's the idea of using at least some concept of the way a human being makes decisions inside the machine learning system. Another system that I mentioned the rover on the moon. So that's a machine learning and symbolic system hybrid where we have a model based reasoning system analyzing the data from the way a human does it, looking at an engineering schematic and looking at the sensor data and try to understand what you should be seeing in the sensor data and what you're actually seeing and when they're different from that, you know, what's the likely cause of the problem? And running completely in parallel, a machine learning system that's just been trained on a lot of mostly simulated data, but some real data that was picking in the lab. And it's doing the same thing basically, looking for anomalies and on things it's been trained to recognize, looking for the kinds of problems that to recognize those as a machine learning system. And then you get these two very independent technologies, producing two independent results and you take a look at them. And when they agree, that gives you a whole bunch of confidence. And when they disagree, you can generally determine the reason they disagree. Like, you know, we use the case-based reasoning to keep track of the sensor data. If they disagree, but the machine learning system has never been trained on that kind of data before, well, then you don't have much confidence in the model-based reasoning system.

Speaker 2: Wow, I'm loving these examples. So I just I wanted to share this story. We were doing a project with helicopter pilots avoiding wire obstacles. And so there's collision avoidance that potentially can detect these wires, but they're really hard to detect under some circumstances. And so as we're interviewing the pilots, they're saying we look for where wires are likely to be, so along a river. And we know we aren't going to be able to see the wires so we're looking for the stantions on the edges on the hill that holds them up. And so we were going back to the software people and saying, is there any way the software can look at these things too? So I feel like we were kind of gravitating toward what you're describing unknowingly.

Speaker 3: Yes, yes. I mean it seems obvious maybe in this little group, but of course you should always look at the way human beings do it, right? To inform the way, you know, that your software should function and what it should provide to the human.

Speaker 2: Nice. All right, so we are getting toward the end here. I wanted to ask, what advice do you have for young people who are starting out and wanted to build AI, build really cool AI?

Speaker 3: Yeah, so I think what I would say is to try to have a broad education and broad experiences, not to just focus on computer science and artificial intelligence. I mean, for example, myself, you know, I'm a undergraduate in mechanical engineering and I took a lot of electrical engineering and other types of engineering while I was there. I've always been a very kind of curious person, so I would say that you want to be curious, just want to know everything about everything. I mean, I was always interested in tactical decision making. I would just for fun, right? I would read novels that were typically involved tactical decision making. And, you know, science fiction that was hard science fiction. So it was actually real science. And then as and your career to go and talk to the end users and the domain experts and go visit the environment where your software is going to execute to really understand what it's like. I'll never forget. We got the insiders tour at Kennedy Space Center and when you go and look at the VAB and you see a shuttle hanging there, it is really big. It is really big. You get a sense, oh my god. And then you look at how you see how high the VAB is and then you say, oh, yeah, that's why you can't have any operations at the top when there's something going on at the bottom because somebody drops a hammer, it's fatal. You know, it's just understanding the environment the software is going to be executed in. And really, you know, doing that cognitive task analysis of what kind of decisions that human has to make and how can you facilitate those. And or to the degree you're trying to automate a lot of that decision making, really deeply understand how they're thinking about it. But all of that requires broad knowledge, broad experience, a lot of curiosity. Just going through life, being curious about how does your battery work in your cell phone? How does that charging work? It's not that different from a electrical power system on a spacecraft. But you you have to be at least a little aware of it.

Speaker 2: Slightly different question, but along the same lines, you've worked with Gary and Laura for a long time now. And so you're probably pretty used to what they provide in terms of products and CTA outputs and that sort of thing. I'm curious if you were to give some recommendations to folks who were maybe new to NDM and sort of wanted to provide the same sort of capabilities that a more experienced NDM person could provide you. I'm wondering looking forward, is there more that your teams need from the NDM community? Is there more that your teams need from NDM researchers or do you need things in a different way? You know, we've been teaching CTA pretty much the same way for a few decades. Are there other representations that you can think of that would be helpful or other parts of the data that you want to know more about?

Speaker 3: I would say if I had to be critical of the community, which is I think it's what you're asking me to be, okay? Is just the level of detail. As a software engineer, I would always say, hey, you really know you've captured it is if your software gives you the same answer that a domain expert does. And if you can execute it and it gives you the same answer, you obviously you've really captured it. And I feel like sometimes when I get products that are a result of CTA, they haven't gone that last extra step of, you know, could you really, you know, from a cognitive psychologist perspective, you're not a computer scientist, but could you really from what you're looking at and no one told you anything else and then you've got the inputs, could you really make the right decisions? Or is there is there a little like one level more detail of information that you really need to get to really be able to do it, which is that's what the software engineer needs because he needs to be able to build a system that can really do it.

Speaker 2: Yeah, that's super important. I'm I'm glad you said that. If I can push a little bit on that, detail, what kinds of I don't mean like what types of detail, I mean, detail provided to your team, how? And I'm thinking about the products that come out of a CTA generally, decision requirements, tables, and stories and that sort of thing. When you're talking about detail, what what are you meaning?

Speaker 3: I think, so, you know, of course, you need the overall understanding, right? You of course you need that. And that's always been good. So I'm not going to focus on that, okay? But given you have the high-level perspectives, what we tend then given our next step is to really look at the detailed scenarios, you know, and very specifically, you know, CTA, you know, you would have asked the question in this very specific circumstance that you remember, tell us about the inputs and the outputs and then how did you make that decision? And it's got to make sense to you, right? As the CTA practitioner, that there's no magic involved. And sometimes, I think, people can do a good job thinking non-linearly, right? They don't always look at the inputs, think about it, and then just magically come up with the output. Sometimes they have to, they have to do a few things, right? They have to try a couple things. Well, maybe it's this. Oh, that didn't work. Maybe it's this. Nope, that didn't work. Maybe it's this. Oh, that worked. And then when you ask them, they have a tendency to go straight through the one that worked. They have a tendency to either forget or not think it's important maybe. Why should I tell you what didn't work? Well, because you considered those other options, right? Because you, the expert, didn't know immediately the right answer. You knew there were three options you had to investigate all of them. So, that's an example of sometimes when we're looking at something and we say, well, how why this one? Because there are these other two things. How did you know, you know, how did you know to eliminate those? So I think that would be that kind of detail. Not letting the domain expert or the person courting it, get away with it.

Speaker 2: Yeah, I think we have a a new motto, Laura for our next CTA workshop, which is we we got to squeeze the magic out.

Speaker 3: And yes, exactly. There should be no magic, right? There should be no magic.

Speaker 2: I like it. I'm really happy with that response. I find sometimes people say, can you just give it to me in a box and stick diagram? And I often say, no, like there's I would have to put so many words in that little box. Like I I mean, I can give you that, but there's all this other detail you need to know whether you want to or not.

Speaker 3: Yes. I'd like to think that wasn't a software engineer asking for it that way, but it might be. See, that's the point of being curious, right? You shouldn't want it to be easier. You should want it to be harder and more complicated, right? Because you really want to understand, how do they make those decisions?

Speaker 2: Well, I think the other thing you're also getting at here is again, this collaborative engagement with the CTA practitioner, the software engineer and the subject matter expert, right? That's that's that's sort of the tri factor that we all know works and there's various reasons why it doesn't work. Sometimes we never get access to that SME and whatever we got is what we got. We don't have that second chance. Or, you know, sometimes those products don't speak to each other and even the software engineer and the CTA practitioner have trouble sort of communicating because they've got different goals and those sorts of things. But it feels like, you know, the decades of work that you all have together offers a nice case study if you will, of of the right way to do all of this.

Speaker 3: Yeah, I mean, I think one thing when we get involved in the project, I always say, give me your best subject matter experts because you don't want me to automate your worst ones. And or, you know, we did a lot of work in intelligent tutoring systems and hey, give me your best instructors. You don't want me to automate your worst ones. And when people ask like in the public school system, you know, what makes the most difference? Well, it's having a good teacher. I mean, there's nothing better than having a good teacher. There's if you had really good teachers, you'd have a really good public school system everywhere. Well, similarly, you need to have a good open-minded software engineer and good detailed open-minded CTA practitioners. And even if an SME who's good at his job, doesn't mean he's good at describing it for whatever reason, you know, he might be old and grumpy and too busy, or just he doesn't he can't be introspective enough to tell you how he makes decisions. So there's nothing like having a team of three good people.

Speaker 2: Absolutely. Yeah.

Speaker 3: Okay, I have one last kind of fun question. If you could instantly be expert at anything, you don't have to do any training, you just all of a sudden you've got it. What would it be? What would you choose?

Speaker 2: Probably to play a musical instrument, because I have no ability now, something that would that you could just kind of like a harmonica or something you could just kind of whip out because I have no musical abilities whatsoever and it would be fun to have that.

Speaker 3: That would be fun. Very cool.

Speaker 2: And the magic just came right back into the conversation.

Speaker 3: We try. Yeah, maybe I should find a neural network that would play my harmonica for me. There you go.

Speaker 2: Well, thank you for speaking with us today, Dick. This has really, really been a pleasure.

Speaker 3: Very much on my part too.

Speaker 2: Great. Well, on that note, thank you for joining us for the NDM podcast. I'm Laura Millitello.

Speaker 3: And I'm Brian Moon. Learn more about Naturalistic Decision Making and where to follow us by visiting naturalisticdecisionmaking.org.

[ Music ]