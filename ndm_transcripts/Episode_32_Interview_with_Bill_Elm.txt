[Music fades in]

**Brian Moon:** The naturalistic decision-making podcast with Brian Moon and Laura Millitello. This podcast series brings you interviews with leading NDM researchers who study and support people who make decisions under stress.

**Laura Millitello:** Welcome to the Naturalistic Decision Making podcast.

**Brian Moon:** This is Brian Moon from Paradyne Technology.

**Laura Millitello:** And I'm Laura Millitello from Applied Decision Science.

**Laura Millitello:** Very happy today to have Bill Helm with us. Bill is the founder, president, and CSE fellow at Resilient Cognitive Solutions. Bill's military career includes an ROTC commission in 1977 from Carnegie Melon University as a distinguished military graduate, with further military education including Airborne Ranger, military intelligence officer, basic and advanced courses, CAS 3 and Command and General Staff College. He's held leadership positions up to MI battalion command, Brigade S3, as well as intelligence and staff officer positions at various levels up to joint command, receiving numerous decorations and awards including the meritorious service medal. His military career spanned 27 years of active and reserve service, retiring as a Lieutenant Colonel, military intelligence in 2004. Bill's civilian career began as a cognitive systems engineer designing advanced control systems for commercial nuclear power plants, including an alarm management system that still defines the state of the art. He's one of the longest practicing cognitive systems engineers, combining over 34 years of applied CSE experience in domains ranging from process control to national intelligence. Currently, he's focused on advanced decision support using big data analytics to team effectively with corresponding advances in decision making tradecraft. Thanks again for joining us, Bill, and welcome to the podcast.

**Bill Helm:** Thanks, Brian, and good morning to you and Laura.

**Laura Millitello:** So I want to um go a bit back over uh that bio I just read, you you're one of the few uh podcast guests we've had who who have sort of these dual careers. You bring real world experience to your research and development roles. So I'm wondering if you can kind of walk us through how your dual career path developed and and how you came to um to found resilience.

**Bill Helm:** Oh, it it it's way too long a winding a journey, but um as as a friend of mine was inducted into the the Ranger Hall of Fame recently and and he described uh he described that process as a crucible and I I came to realize that was sort of a a crucible in my life that sort of changed my perspective on what's possible and what's impossible and um, sort of been a challenge seeker ever since. Um, so as I got off active duty and and went reserve and and went looking for a civilian career after uh my Bachelor's and Master's at CMU and then later going back again, um, the and I and I walked into Westinghouse as they were, they were uh reacting to three-mile island and trying to uh imagine what a different kind of control room would look like, uh, since the the uh industry was not satisfied with basically the decision-making of that of that crew. Um, and I I asked the key question, well, how how do you how are you going to do that? What process are you going to use to do that? Uh, and I'll never forget the engineering manager looked at me and said, well, that's the first thing we have to figure out is how to do this stuff and then we're going to do it um for an advanced control room. Uh, and that that sounded like a hard challenge, so I signed up right there. Um, and that was that was a year before Dave Woods wrote the uh the paper that coined the name cognitive systems engineering. So I've been doing it sort of a year longer than it's been a named field. Um, and I I got introduced recently as, you know, uh fractions of a century. So I've I've kind of my career's now changed in a whole different unit of measure and I I'm usually the old guy in the room. So, uh, and that it's it it has been a challenge and it's just been sort of sparking my curiosity ever since and and I can't put it down. So, um, evolved through a series of uh moves and acquisitions and and went to work for a one of the original AI companies here in town a CMU spin-off and that went public, I bought and sold and bought and sold, um, ended up as a cognitive systems engineering center of excellence and a big defense contractor after they bought and sold each other. Uh we spun the company out um in 2007 when I had a chance to do that, um, to focus exclusively, to no longer be a small center of excellence in a huge organization, but in fact to have an exquisitely honed organization dedicated only to this. So we built the company from the ground up to be a cognitive systems engineering operation. The facilities built that way, the recruiting programs built that way, um, the accounting system is built as a cognitive systems engineering organization.

**Laura Millitello:** Yeah, I I want to hear a lot more about uh resilient cognitive solutions, but um, to to go back, making a jump, I that that's a pun I guess from from Ranger to uh Ranger and Intel to nuclear engineering. Can can you kind of talk us through uh what that was like for you to make that big leap between domains?

**Bill Helm:** Oh, well, in in fact, I'll I'll go the other way and say, um, that that's where I I I first realized that the the issues of cognitive systems engineering are in fact domain independent. Um, the the intelligence community in general has been, um, has recognized the critical nature of human decision making as as core to the business, right? An intelligence analyst is a is a is a knowledge worker, is a is a thought uh engine uh and getting those decisions right and and being a high performance, they use the term analytic rigor rigorous uh agent, uh is critical. So they've recognized that and then since that's what we were doing for these nuclear uh power plant operators, making decisions. Now, in that case it was process control decisions. But it was all cognitive systems, joint cognitive systems kinds of stuff. So, um, over my career it was about both performing uh as a as an intelligence operator, but also then training uh training the unit to be an effective decision support. And in a sense that's what the intelligence community is, it's a human decision support system. Um, so it's trying to advise the decision makers uh on and provide that those key information resources, um, so that those those action uh decision makers can make their action decisions or operational decisions. So it's to me it's the same. It's not different at all.

**Brian Moon:** Right. All right, I'll I'll I'll buy that a bit, but but also also having worked in both domains, uh the the sheer technical knowledge that is required to even understand what's going on in a nuclear uh plant control room uh is is complex to say it uh gently. Um, I I'm wondering if um if you can think back to those early days, uh were there things that you discovered kind of about your own work or ways to quickly get up speed up to speed uh on what was because it's one thing to say it's about process control, but you got to wait through a lot of technical stuff just to get to the process. I'm just wondering if you had any lessons that you recall learning about how to how to quickly jump into that domain and make a difference.

**Bill Helm:** Well, I I could I could maybe share a war story. We in a in a slightly different example, we were doing some work with the Canadian Navy uh on a command and control system for them. uh and I had I I had a a navy Canadian Navy Admiral grabbed me, dragged me out in the hall and and asked, how could an RCS cognitive systems engineer possibly know that much about naval command and control, she's only been out of school for two years.

**Brian Moon:** Right.

**Bill Helm:** And and that's exactly to me that's an indicator that our that our upfront the the upfront end of our engineering methodology where we just as you described, we sort of peel back that the domain specific issues to really fundamentally understand that decision-making basis that and and we do that with a variant of a cognitive work analysis uh as as you know. Um, and that is core to everything that stands on top of that. Our designs, our testing, um, in in fact, our our knowledge elicitation downstream of that is all built on that on that foundation of that core understanding of the cognitive side of that work domain. So, uh, when that when that Admiral dragged me out in the hall and was confused about how somebody so young could have uh senior officer equivalent uh mental model, if if you want to kind of go toward the the whole C-Map thing again, uh that to me was an endorsement that um that that front part of our process was working. We were getting to the right model. We understood it at a level that an expert recognized as a near expert equivalent uh understanding. And that so in a sense it is part of to me, part of the the fundamental work of a cognitive systems engineer is doing that up front on a brand new domain, clean sheet of paper. I don't know anything about X, let's go.

**Brian Moon:** Right. Well, that there had to be some trust there though from that admiral that that you were going to bring something even if even if they couldn't see it at the moment, that you were going to bring something of value.

**Bill Helm:** Uh and that that is part of the part of the challenge of this business is it is relatively unconventional to to people that uh have built systems a different way their entire career. The the acquisition programs are not designed for cognitive systems engineering. The, you know, even even agile software engineering in a sense is a completely different approach to how do you generate, how do you generate those profound ideas at the heart of a joint cognitive system and we have a different way to do that. We find some people uh will make that leap uh after hearing about it, seeing examples and understanding it, they sign up. Other people I find it takes a cathodic moment. So we had a we had a recent meeting with uh the US Navy after the McCain collision, uh where I pointed out that basically their systems engineering process was lacking that cognitive systems engineering JCS insight up front and that contributed to to the quote disconnect that they assess to be human error, which is which is typical. So they the ones and zeros all flowed adequately through the tech side, so it must not have been at fault. The people in front of it take the blame. Uh to us, it's a failed joint cognitive system uh that that doesn't have that integrated perspective from the start.

**Brian Moon:** Right. All right, so so getting back to uh, to how you set up shop here, um, tell us about uh, the the the sort of processes that you put in place to ensure that um, uh resilient cognitive solutions uh remains true to its core and I'm particularly interested in that accounting piece. So so.

**Bill Helm:** So.

**Laura Millitello:** uh, tell us how you tell us how you've set up so that you can remain true to your principles.

**Bill Helm:** Uh, well, the the there's a there's a book that uh that resonated with me, the, if I get the title right, the discipline of market leadership. Uh, and uh, in the in the book it it really says as you as you set up an organization, you have to decide what you want to be. Um, so do you want to be kind of a turn key, uh one stop shop? Do you want to be, uh, low cost provider, what do you want to be? Uh, and and in fact, um, when I set up uh RCS, we wanted to be, um, one of the world's best practitioners of cognitive systems engineering, build fundamentally, profoundly different cognitive affordances that that just fight uh elegantly well. And with that, with that focus, now, so so again, as a small organization, uh we know who to blame if it comes off the rails, me. Uh so that everybody's introduced to that, in fact, even during the recruiting process or even during during our intern process, interns are are told that right up front, if it doesn't have a cognitive basis, we're not doing it. Uh, we turn away work uh where the the customer is not fundamentally interested in a in in making a difference in a in a in the in the decision effectiveness of their of their fight. And I've I've kind of a old school word rollerdex, I have a I've a a contact list full of uh full of other companies I refer folks to when they when they insist they just need a mobile app. Okay, if if if you just want a mobile app, we'll send you to a mobile app company. If you want a mobile app that is a true cognitive affordance, you're at the right place.

**Laura Millitello:** Well, those sound like interesting conversations. Um, what uh, so I I I understand someone comes to you and and looks for a solution up front, but um, can you give us uh maybe another war story where you've sort of helped bring someone around, uh, who may have started with that approach, but you brought them around to the focus on cognition.

**Bill Helm:** Well, sure. So, um, in that sort of uh smoke jumper kind of role, we get uh, we get asked into programs that are maybe mature late phase programs, projects, research efforts where they they find that they are struggling uh at the end. We we got called into one where they had they had made a major investment in uh AML computer vision analytics with the typical assumption that this AI agent was going to replace person X on the crew. Um, and that's typically how the kind of uh ROI uh case is made. We have a four-person team, we're going to make AI do person two's job. Now we have a three-person team, save a lot of money, pay us for the AI. Classic, um, classic way the program gets justified and paid for. Um, but they don't really understand what they've done when they try to do human role replacement with an AI agent that is that is focused on sort of one specific thing. So, object detection in an image is not just is not everything a human does as a member of a a an imagery processing team, uh, they call it a process exploitation and dissemination of ped cell, uh for uh full motion video. And they they just don't realize what they've what they've done when they when they take a human role and give it to a vision analytic. So they had done that. So they had built all this, they'd spent a lot of money on uh very good performance from object detection, computer vision analytics, but the analysts wouldn't use it, couldn't use it, and it was not being adopted. It wasn't having the the the work benefit, the mission benefit that they that they claimed and they didn't understand why. It was working fine. All the tech scores were fine, all the system testing was fine. Why didn't it work? So, uh they reached out to us, somebody somebody had my business card and gave it to somebody else and gave it to somebody else, so they called. Um, and we walked in and basically inverted the entire model. Uh so no longer was it, these are the AI uh detections, uh, you the human analyst go use them and be a better analyst. We inverted the whole thing, embedded those object detections as sort of a deeply embedded service within a larger joint cognitive system design concept, and suddenly it worked profoundly different. The analysts were able to uh, in a sense without realizing that it was AI under the hood, uh without sort of directly feeling the agent, uh they were supported in their core decision making in a in a profoundly different way.

**Laura Millitello:** Hey Bill, I wanted to circle back, uh, when you were talking about being uh at Westinghouse early in your career, as, um, you know, and this was an exciting time. Dave Woods, Emily Roth, you, all these, you know, now legendary figures, um, were early in their careers and, um, working on this really important problem and figuring out a process and what to call it and and all of that. And I just I wondered if you could tell us more about, you know, being one of the people there as as cognitive systems engineering was being envisioned and articulated and put into practice all at the same time.

**Bill Helm:** Oh, it was as you it was a it was a ton of fun and it was it was hard and it was fast-paced kind of all at the same time. So and anybody that knows Dave Woods, uh, he he he generates ideas faster than you can write them down. So, uh, he's he he was literally this sort of, you know, CSE idea engine on one hand, and then applying it into this advanced control room project specifically on the other hand, and back and forth and back and forth between, well, if we did it this way it would look like this. Uh, well, let's see if we did it this other way it would look like this. So there was there was just tons of of of generative um process thoughts and also then results uh coming out the other side. Very very cool stuff uh and and in fact that same that engineering basis that we came up with, uh that approach for how to do it as an engineering discipline, not as not really as just a research effort, but how do you do cognitive systems engineering with sort of a capital E as an engineering practice where you have the um, the audit uh the audit trail, the repeatability, the traceability that you need uh to to stand up in a in a systems engineering world with the same level of engineering rigor informed by the that the cognitive science difference uh integrated together. Every meeting with Dave was a brainful. Uh the the meeting would end when my head couldn't hold anymore and we had to kind of then uh digest and do something with it.

**Laura Millitello:** So, so one one thing I'm I'm I'm curious about is is that you mentioned one of the um big contributions was really about alarm management in nuclear power at that time. And and so I wonder is there a a key insight or something that seemed revolutionary then that maybe now is kind of ordinary, um, that you could um, share with us as as you think about that that early work you were doing?

**Bill Helm:** Well, I I'd almost say the reverse. The the key insights of what we what we worked on, whatever year that was, 86 maybe, right in there is is really is true today and is being missed today. So the the the short version of it is is really the cocktail party effect, if you want to sort of just use that that typical teaching point where you have to recognize that the alarm system is really a cognitive affordance helping you to helping the users plural to focus their attention where their work is needed right now. So it becomes um this this as as Herb Herb Simon said, uh human attention is the most precious resource. So how do you help a human focus their attention? That you innately do perceptually in your in your real world, in your in your physical world every day. How do you do that in these virtual digital environments that we're building in a non-intrusive elegant way. So if you're consuming somebody's attention with a loud alarm noise or a scrolling list of alert messages they have to read, you've consumed their attention to invite them to divert their attention from the work they're doing. So you get this catch 22 effect if you treat the alarm system as a sort of of a data output three thread, and that's common today. I will build an AI agent, it will detect this signature, when it detects the signature, it will compose this message, it will then send this message to your email, your cell phone, uh and maybe go uh text to speech and and speak to you through your through your uh your Google device or Alexa or whatever. Um, but you've interrupted the person by doing that. The the the practice in the nuclear days was was one of these incredibly loud claxons that goes off uh to make sure that that nobody's sleeping through the alarm. And and the and the the real indicator, there's a there's a now to me famous photograph of uh there's a an acknowledge button the operators had to press to silence that audible attention getter. Um, and it was going off so fast during three-mile Island the crew couldn't talk to each other. It was it was basically constantly on during that alarm avalanche, um, of it was like 3600 alarms per per minute. Uh just pouring in. Um, and they pushed the button in and stuck a penny in it, um, to to keep the button in so the audible alarm wouldn't keep going off, um, so they could talk and actually work the problem. So there's a case of where the the attention support was so disruptive that that it was defeating its intent to be an attention manager. So you have to build this very exquisite elegant, um, sort of whisper kind of an effect that just is perceptually uh effortless in terms of inviting the person to change their focus, their attention focus or not without consuming their attention to manage their attention. But but the current state of the practice is my AI agents, many of them, lots of them, huge numbers of them are generating these huge volume of alert messages and scrolling them up the screen and then you write a procedure that says, the user must read and acknowledge and react to every one of the alert messages because they are anomaly indications.

**Laura Millitello:** Yeah, I know, this is this is I'm nodding right along. I mean, I think we're seeing this in health care, like in many domains there are the this approach too. And and it's the human's job to then sort through which are relevant and which are not.

**Bill Helm:** Yep. So so what we're finding is you now have in in in the world of big data, you have uh humans who can't possibly process all the data in the big data. So now you overlay a a layer of AI agents who are grinding on that data and keeping up and are scalable and all that, generating a massive volume of AI alert messages coming out of the AI agents. So you've got a data overloaded user now facing alert message avalanche on top of that, so you've you've I don't know, you've kind of buried them twice by by this sort of sort of single-minded if I can generate a message, the human will do the rest.

**Laura Millitello:** And often the alert messages are are cryptic, so it's it's hard to sort them out.

**Bill Helm:** Yes. Uh so that that gets into a bit of the craft of how do you how do you make a uh an anomaly indicator of any kind of message or whatever. How do you make it informative? So treating it as an information resource, not just treating it as I mean the extreme case is the big red uh group alarm like the master caution light uh like in uh the Apollo spacecraft for example, you just get a big red light on the uh on the avionics panel says something's broken, bang and you get a big red light. That's the that's the most extreme case of a data sparse uh alarm indication.

**Brian Moon:** So so the thread I'm hearing you pull both in terms of your many of your war stories, but also back to your marketing and and business development approach, uh is it sounds like there's a heck of a lot of engineers coming to you saying we want to build this thing. And it sounds like uh your conversation with them, um, if it does move forward to work and I understand some some of it doesn't uh because you don't want to work with those people. Uh but but I imagine your your education here uh is about, okay, you want to build this thing, let's let's talk about, um, you know, the effects of you building that thing on the people that you're trying to build it for. Is that is that part of your education you have to do uh to those engineering types?

**Bill Helm:** The short answer I guess is yes. There are there are lots of different uh sort of customer stakeholder perspectives at that at that table. So the engineering, the technical teams are only one aspect of it. The technical team stands up and says, I know how to build the tech, but you've got you've got kind of the management leadership perspective, the the economic buyer who's looking at cost, so there's all kinds of different customer perspectives that all have their point of view uh to be satisfied in all of this, but but underlying it all, Brian, you're exactly right. Uh we had one of our new interns actually comment uh to me the other day that that they they wanted to intern at RCS because they see this everywhere. So in a sense, every system that we build, every system is essentially a joint cognitive system. There's a human that rubs up against that and uses that technology to get real work done in the world. And it doesn't matter if if it's your fitness watch on your arm or a control room for your process plant or AI engines to help your intelligence analysts or your business staff, you are building joint cognitive systems. If you stop your thinking at, can I get my software to run? Your system boundary is too small. You're building a computer system, not building a joint cognitive system and everybody is always building a joint cognitive system whether they recognize that or not. If they don't recognize it then we've we've actually been working lately then then they are they're going to bake, we call them latent brittleness into that technology that are going to break under pressure at the worst possible moment. Uh so you'll you'll crash a ship uh in the straits of Singapore and kill sailors because the conditions align, the holes in the Swiss cheese aligned in just the right way and suddenly a mistake was made, human failure, but it's really a fundamental latent brittleness in that joint cognitive system just waiting for the conditions to line up right, which is why all of these big accidents and and events, you get that cascade of dominoes uh leading up to it. Somebody got up late, they were distracted, the coffee got spilled, somebody yelled their name. There's just this sequence of small little things that all happen to align together and suddenly the ship crashes. But it's predictable. If you if you expand that boundary out, you can find those latent brittlenesses uh and we are we're in fact publishing what we call a brittleness finder for uh system developers and customers to kind of self- diagnose a little bit. So if if you are thinking about doing something, building a system in this way, be careful, uh you you may be building a latent brittleness into your operational environment when you field it.

**Laura Millitello:** Is it I mean the brittleness finder sounds like a a really important contribution. I I was going to say one of the things I admire about your approach is this acknowledgement that there's still skill involved. So even if you have really good methods, uh CSE, designing the alerts so they are informative and they're not overwhelming, it's not straightforward. And and identifying the latent brittleness, there are some things to look for, but there are still some gotchas and and so, um, some of this is an art and some of this is years of experience, um, the perspective, you know, gets you a long way, but it doesn't get you all the way there. And I, um, I just over the years I've heard you give talks, I feel like that's one of the things you often, um, bring up is this is this is hard stuff and uh, and and they're challenging problems.

**Bill Helm:** Yes, I agree and we, as I mentioned earlier, our recruiting process, we're looking for those folks that that sort of we call it the gleam, that sort of get that twinkle in in their eye about working on hard things that take a skill and a craft and a dedication, uh, and if we can get that, if we see that gleam in that in that candidate, uh, we we make them an RCS employee. Maybe it's a little quixotic of me, uh, but it is a it is a quest. But you do in fairness, you do see it in other disciplines as well. You can go to an architect and get a very plain looking building or you can go to a different architect and get a very elegant, sophisticated functional design. You know, what what separates the two to some degree it's their dedication to their craft and their skill. And and again, we want RCS to be right up there as uh in that in that frontier world leader position of of cognitive systems engineering practice, which means we have to be good at our craft.

**Brian Moon:** So so earlier, Bill, you mentioned Dave, um and I I think you were referring to him as as someone who's uh who's inspired and influenced you. Uh I wonder if you can um tell us about a few other folks along the way that have uh helped influence how you go about your work.

**Bill Helm:** Oh, interesting. So the I I'd have to give uh Yens Rasmussen some credit in in in the meetings and discussions with Yens, which are where some of those sort of initial ideas and principles all all churning around in that early work with Dave. Yens was a very down to earth kind of guy and and he looked at me one day and said, look, just because I wrote the book doesn't mean it's right. He said, it's uh I'm a control systems engineer trying to do this difficult thing. So if if there's a good idea in here, use it. If it's a bad idea, do something different. So I I I really enjoyed kind of talking to Yens in that in that fashion where it's not a tome, it's not something to be sort of dissected for every comma, it's it's an engineer trying to talk to engineers and I and I really enjoyed that interaction with him. But right up, I'm going to I'm going to cheat a little bit here, Brian. I'm going to hyphenate the next one. When I went back to to CMU for some uh PhD level AI studies, whatever year that was, and I got to work with or learn next to Herb Simon and Allen Null and I kind of hyphenate them because they uh they they traveled as a team and worked complemented each other as a team. So just just to hear Alan Null and and I I would steak into his post doc seminars because they were really cool and if he didn't take roll he wouldn't throw me out and that was really cool. And I and I could hear him look at these post doc AI researchers and and try to get them to understand the fundamental technique that they were using and how it compares to the sort of, you know, biologically or human inspired idea of how people think and and and how they were trying to trying to riff off that as a as an AI technology. And again, a very realistic guy who understood the characteristics of what he was building and was looking for their strengths and weaknesses and how they could be applied. Uh and then Herb Simon from that other perspective understanding how humans solve puzzles and and think and what makes an expert an expert and the two of them talking together was just was inspirational uh to to hear the two of them debate from the sort of their different perspectives the same thing. That was a big deal. I I really enjoyed that.

**Brian Moon:** Right. Yeah, that sounds like a lot of uh a lot of fun and and inspiration. So so where are you going next? Uh you mentioned uh your brittleness finder, which I agree with Laura that sounds really important. Where is RCS taking their research and applications next?

**Bill Helm:** Yeah, well, we are just constantly uh constantly on the hunt for hard problems to to solve and and and like I tell the crew, that's why I come to work every morning is because a new a new hard problem walks in the door and, you know, boy, that sounds like fun, let's work on that one. So we're constantly looking for people that that are that are struggling with a hard JCS issue and uh and we can help. So that that really does keep it fresh. We're we're working on several projects at the same time and it's a constantly evolving portfolio. Those we finish one, start another one, here we are, a whole new work domain, what are we going to do with this one? I don't know. Let's roll up our sleeves and figure it out. So that's all the way back to 1982 when I when I signed up to that in a in the first nuclear control project, control room project and and here we are today doing the same thing. It walks in the door, I don't know what the answer is, but we're going to have a lot of fun finding it out. Alongside that, we are uh we are partnering now with Texas A&M to stand up a human machine ecosystem laboratory, which is really dedicated to this idea of joint cognitive systems and cognitive systems engineering as the right way to be building sophisticated technical solutions. And it has a uh it has a complementary arm of it, well to apply experiential learning best practices for the human side with cognitive systems engineering best practices to build the cognitive affordance technical side. So we now have a different way to help humans work the advanced tradecraft of their their now advanced cognitive affordances. So we're really excited about that. It's a a brand new effort that I think is really going to scale the field a bit and and I'm really hoping that it sort of turns the the systems engineering ship just a little bit, uh so this becomes more accepted uh and more mainstream and and less less of a of a of a niche kind of effort.

**Laura Millitello:** That is exciting. So you said Texas A&M is that who you're partnering with?

**Bill Helm:** Yes.

**Laura Millitello:** And and will there be a like physical location for this?

**Bill Helm:** Yes, it's uh and COVID has kind of messed up some of that a little bit here uh recently, but yes, it's on their their new campus just outside College Station, Texas. So yes, there'll be physical, we're doing it this year virtual. We've got some some pilot studies uh in in in the pipeline right now. So it's actually very exciting, very, very exciting time for cognitive systems engineering sort of next step up.

**Laura Millitello:** Very cool. So one of other exciting thing happening this year is that there's going to be a the Naturalist decision making uh conference is being hosted um in collaboration with the Resilient Engineering Association. Um, so I think these are are, you know, communities that know each other and there's some overlap but we're going to have a joint meeting this year. So we're really excited about that and I note that the name of your company is resilience and I wondered if you could just tell us a little bit about your perspective on on resilience.

**Bill Helm:** Yeah, and it it may be a bit of a different perspective on this, but our our approach to that is if we can design and implement a a well cognitive systems engineered joint cognitive system, the decision making behavior is innately resilient. and I know it sounds a bit circular, but rather than try to stir in a half a cup of resilience into the effort, we think resilience is an emergent effect that comes from having a we can use these words, having sort of a non brittle JCS if you want to kind of approach it that way. So if if we can design and build a proper joint cognitive system, one of its innate properties is its resilience and robustness to novel situations and surprises and errors. So we we don't focus on resiliency per se, we focus on effective decision support and resilience is the emergent effect uh resilience is the emergent effect that results from that. It it's a very similar discussion, people ask me about how do we fix cognitive bias. Right? So when when the buzz was everybody talking about cognitive biases, and my answer is we don't go in and and put a bandaid on a bias or or look for a remedy for a bias. We go in and fix the underlying joint cognitive systems and then you don't see these symptoms that you describe as bias. So if we can get the decision making to be right, it will be resilient. It will it will not exhibit these biasing effects. It will be it will recover, it will heal, uh it it will it will be a resilient joint cognitive system.

**Laura Millitello:** I really like that um kind of flipping the question around because it it really doesn't make sense the other way. You can't change the way people think by, you know, limiting their choices or uh and you can't just give them a resiliency injection somehow or yeah, it doesn't it doesn't make sense to think of it that way. Right.

**Bill Helm:** Exactly. Right. So so as an engineer I'm always asking myself, how can we help, right? What what can we do as an engineering practice and and it always has to be it it has to be some something we can deliver, something we can analyze, design, implement, uh that changes things because I can't I I can't invent a resiliency injection and just wish it to to happen. You know, gee, I wish people were more resilient. Well, that isn't that is not an engineering solution uh to achieving.

**Laura Millitello:** Right. Okay, so I have a kind of a fun question here. I want you to imagine that you meet a complete stranger who claims that resiliency is central to their work. On pain of death, you're given one question to determine if that is indeed true. What would you ask?

**Bill Helm:** A pain of death, interesting. Um, I I I I ask our interns a similar kinds of kind of high pressure kind of question like that. Yeah. Um, it's very good. I think the the question is going to have to be something like, what decision are you supporting? And it it they they really should have a spectrum of and they should have a big bag full of cognitive work at hand. If they can't answer that question, it indicates they are they do not have a decision centered perspective on their on their effort. So, the that would tell me they're building technology, they're building a data centered system, a technology centered system. It is not really a cognitive centered effort. Therefore it can't uh it it is not going to get there.

**Laura Millitello:** I like that. I like that a lot. So what what are you trying to how are you framing the problem kind of? Yeah.

**Bill Helm:** Yes, and and and I need that cognitive decision kind of word in there because they'll give you some framing. They they have some framing for for how they're thinking about the problem and how they're trying to engineer the solution. The the the real, you know, sort of pain of death question is, is it a is it a cognitive framing or is it a data framing or is it a textcentric framing? Um and and that's what you're trying to get to and and frankly, the if if if you ask me to guess if if we asked that question 100 times today, what would we get, um, we'd probably get about 60% AML techcentric framing. Uh, we'd probably get about 39% data science, big data, data data framing. Um, and there's somewhere there's this outlying sort of 1% that would say something that sounded cognitive or decision.

**Brian Moon:** Yeah. But that sounds like the same kind of question you're asking not only your interns, but your prospective customers as well. Yes.

**Brian Moon:** To really to really capture the their incoming perspective.

**Bill Helm:** It it so I joke I joke with some of our some of our senior engineers that to some degree it's like, um, you you feel like the the guy that knows the winning lottery ticket number and nobody will listen. Uh, so you can you can see the the path that they're on, uh, you can see the brittleness in the joint cognitive system concept that they're that they're attempting to implement, but you kind of can't stop them. They they they're dedicated to it, well. And that that really frustrated me has always frustrated me for my whole career. Um, and I if somebody has an idea on how to fix that, I'm I'm looking for a I'm looking for the cure to that one.

**Brian Moon:** Right. Another need for inoculation, it sounds like. All right, last questions uh a bit of a fun one, but uh I'm very curious to hear your answer to this since you have worked in uh so many different domains. If you could instantly become an expert in anything, what would it be?

**Bill Helm:** Oh, that's interesting. Um, lately I've been thinking a lot about sailing and I I've I've only been on a sailboat just a handful of times in my life and have have never personally sailed one but just sort of the elegance of the physics and the the nuance of all that is is is kind of is kind of fascinating to me. I I'm not sure I'm not sure that's uh the answer you're looking for, Brian, but it just seemed it it's just been in the back of my mind lately.

**Brian Moon:** That works. So this is something you haven't really done much of, but it it obviously struck you as uh interesting and and something you you could see yourself spending time developing expertise in.

**Bill Helm:** Yes. And and very different approach to, you know, put diesel fuel in the front and, you know, and and and light up a bunch of horsepower and and go abuse the ocean. Uh, there's something about sailing that's more maybe more JCS like, you know, sort of at at one with the forces as opposed to opposing them.

**Brian Moon:** Right. Well, JCS uh seems to infuse everything you do and and that's it is obvious in your work and apparently you want to make that in your play as well. So um, so uh I I think that's inspirational as well. Um, all right, so on that note, Bill, I want to thank you for speaking with us today. It really has been uh a pleasure and um, uh we we will look forward to learning more about your uh joint system that you're developing with uh Texas A&M. That sounds like a really uh really great direction, um, and the the in uh as you mentioned, the experiential learning piece, it'll be really interesting to see how that uh plays out.

**Bill Helm:** Well great. I I appreciate you having me and and we should do this again soon. I don't get to talk to the two of you often enough.

**Brian Moon:** Right, I agree. So on that note, uh thanks again for joining us. And for the NDM podcast, I'm Brian Moon.

**Laura Millitello:** And I'm Laura Millitello. Learn more about Naturalistic Decision Making and where to follow us by visiting Naturalistic Decision Making dot org.

[Music fades in and out to end]