**Speaker 1:** The Naturalistic Decision Making podcast with Brian Moon and Laura Milatelo. This podcast series brings you interviews with leading NDM researchers who study and support people who make decisions under stress.

**Speaker 2:** This is Laura Millatelo from Applied Decision Science.

**Speaker 3:** And I'm Brian Moon from Perigen Technologies.

**Speaker 2:** Today we are talking with my friend and business partner, Gary Klein. Gary is one of the pioneers of the naturalistic decision-making movement. He's known for moving the study of decision-making out of the lab and into the wild. In 1989, Gary invited a small group of international researchers to a meeting in Dayton, Ohio to talk about decision-making in natural environments and how to study this phenomenon. It was that group of 32 researchers who came up with the term Naturalistic Decision Making. Gary's many contributions include the recognition prime decision model that has greatly influenced how we all think about decision-making and expertise. He and his colleagues developed early cognitive task analysis methods that have become a core set of tools for studying decision-making in real world environments. He has mentored countless researchers, including both of us. Gary now lives in Boston, Massachusetts. He runs a company called Shadow Box LLC and is also part owner and advisor to Applied Decision Science. So Gary, thank you for speaking with us today.

**Speaker 4:** Very much. I appreciate being invited.

**Speaker 2:** I wanted to start by going back to the very beginning of your career. Can you remember the first paper that you ever published?

**Speaker 4:** We're going back 50 years. You really want to go back that far?

**Speaker 2:** I do.

**Speaker 4:** Okay. Yes, I do remember. I remember it very well. Um, it was my dissertation. And, uh, the reason I remember it well is up to that point, that coming out of graduate school, I had no publications. I hadn't done very much research and I had pretty well identified myself as somebody who was not going to be a researcher. I was just hoping to find university jobs where I could teach and somehow make uh, get by that way. And, uh, so I got my first job at uh St. Lawrence University in Upstate New York, Canton, New York. And it's it's a long winter there. I mean, think about Syracuse then drive two hours further north and there's not much there. It's pretty bleak. So, I had lots of time and I said, maybe I should write up that dissertation. I know it won't get published in any of the top journals, but uh, I hate to, you know, leave things untied. So, uh, I wrote it up and then the question is where do I submit it? And I decided, I'll start started at the top and then when it gets rejected, I'll keep working down until I find somebody, uh, some journal that'll take it. So I started at the top, Journal of experimental psychology, which is one of the flagship journals of the American psychological Association. I sent it off for them and braced myself for their criticism. I figured I'd modify, use their criticisms, improve the article and work downward. And to my amazement, they accepted it. And I said, what? I had to read the letter several times to to make sure that that that it was serious, and there were almost no corrections, no revisions required, and it turned out to be like the lead article in that issue. And all of a sudden I said to myself, maybe I can be a researcher after all. But it really, uh, turned the the importance for me wasn't getting the article published in that journal, but changing my own professional identity. So I remember it very well.

**Speaker 2:** Interesting. So what was the article about?

**Speaker 4:** It was about uh, short-term memory. This was in 1970 when, uh, cognitive the field of cognitive psychology was still in the early days and it was a a technique for trying uh, semantic and uh acoustic confusion effects and seeing what the what what those effects might be and what the time course would be. And I found something a little bit counter intuitive that I expected that the acoustic effects would would, you know, dominate and then die out. And in fact, both kinds of confusion effects occurred. And so the the the the model of of of working memory needed to be modified.

**Speaker 2:** So, um, so this sounds very, uh, far from naturalistic decision making. Um, and I'm wondering, can you think of an event or a turning point or some how you found your way into, um, naturalistic decision making from from that start?

**Speaker 4:** So, I was in universities doing standard university kinds of research. Um, and I and that was my going to be my identity. And then, um, my wife Helen got a sabbatical in Jerusalem, in Israel. And so she, we packed up uh two daughters and Helen and me went to Israel and she had the sabbatical and I had nothing and I was just starting um, uh, my company, uh, Klein Associates, but we didn't have much work so I wasn't going to be missed. And, uh, I and I, I found, um, through a friend of mine, uh, there was a an applied project involving, uh, maintenance officers. So I, I, I, I did this as a part-time job and I'd be in their office with them and see how they were doing their, their job and, um, how they were filling out in certain kinds of forms, which was very formulaic. And the are the Air Force was funding them wanted to build an expert system to replace them. So they wanted me to see if it was feasible. So I found that I could fill out 80% of the form just by, you know, it was pretty obvious. But the other 20% would take me about 20 years to fill out because they had expertise that I couldn't even begin to, uh, to approach. And so I was just learning from them and learning about the kind of expertise they had. And then when I came back, there was a a small business Innovator research project, uh, announcement, I think it was the very beginning of the small business innovative research program and it was from the, um, from the Army and the Army wanted to know, um, how did people, especially army officers, um, make life and death decisions under extreme time pressure and uncertainty. And I decided, well, I was sort of interested in decision making under uncertainty. I had uh speculated about it before and decided, let's put in a proposal, but the the the usual way that you would do this research is take a laboratory task and vary the time pressure and vary the uncertainty, but because of my experience in Israel, I said, let's not do that. Let's talk to people who have expertise, um, because they probably know things that we can't even imagine. So, how can we design a good study if we don't fully understand the phenomenon? So we picked firefighters, uh, and, um, it was, you know, it just seemed like a convenient population. And again to our amazement, we got the, we got the contract and it was a six-month contract and that got us started studying naturalistic decision makers.

**Speaker 2:** Interesting. So the, the maintainers in Israel, um, that project got you started thinking about expertise even for a task that seemed really well described. Right? There were these forms to walk you through, but but but somehow as you were doing that, you you you you started getting very curious about the expertise component.

**Speaker 4:** Right. So let me tell you want to hear a little bit more about that because I've never really talked about that. Um, so there was a natural experiment that occurred in Israel and, um, it had occurred just a few years earlier and it was called the Yom Kippur War. And it was a war and, uh, Israel was invaded by, uh, by Egypt and by Syria. It happened almost without warning and it lasted for a month. And, uh, the Air Force was interested in, how did Israel's maintenance, uh, uh, community, how did the maintenance organization respond to this rapid acceleration of of of of need, um, how did how did that happen? And so, uh, to to maintain the the aircraft. And so after the war was over, there were all these log books, uh, that the the maintainers had kept and they were trying to and and so this project that that the the people that I was studying were were engaged in, they were taking those log books and putting them into a digital form and the log books were in Hebrew so they were translating it into English, putting it into digital form that could go into a a a US database for for analysis. Um, and the log books, I mean, most of it was fairly routine. So that's what that was the trivial part and and so because it was so trivial, uh, the Air Force and and and the the the companies that company was working with said, why don't we just build an expert system and and and just automate it that way and we won't have to pay these maintenance officers that much. So, um, so that was what I was sent out to to investigate and as I said, I could fill out 80% of it because it was fairly obvious. But the other 20% involved the cases where maybe something was left blank or maybe there was an error. And then I would interview them about, how did you fill this in? And they said, okay, so look at this damage. This was a damage look at the size of the damage and they said, you know, they it was and then this plane was back in service two days later. Now for this kind of damage, I know that if if it severed any of these lines, it's going to take a week or two. So obviously it didn't sever those lines, so here's the way the damage must have propagated that they could get it back in service. In other words, these guys could go beyond the log books and imagine what what it was like to be a maintenance technician working on these kinds of systems. So I started to read up on, um, critical incident method, John Flanagan's critical incident method and I realized that that's what I was getting at to to to try to capture their expertise was these critical incidents. And so when I wrote my report, the report was, you know, here's what I can do that's formulaic, but here are the critical incidents that I've collected, so you can understand their expertise. And as a result, the Air Force decided they could not build an expert system to replace these guys, they would need to keep paying them.

**Speaker 2:** Wow. So, so even in that, that, that first project that was one of the first times you really got people to start telling you stories and and acknowledged or or or highlighted kind of the power of that for, for describing expertise.

**Speaker 4:** Exactly.

**Speaker 2:** In a way that was convincing, it was convincing to the sponsors who weren't expecting that at all.

**Speaker 4:** Right, they weren't expecting to hear stories or to see stories in a report and I hadn't been thinking about the power of stories up to then and it was all around the you know, just using critical incidents which are essentially stories.

**Speaker 2:** Okay. So next, I'd like to kind of, um, you know, fast forward to today and I'm wondering what NDM related research direction are you most excited about now?

**Speaker 4:** So the the the work that I'm doing now that has me so excited is about, um, uh, what has to happen to help people develop cognitive skills, uh, improved decision-making or sense-making or problem detection or or things like that. This is something that I've always been intrigued about since, uh, I've been intrigued about since I came up with my recognition prime decision model. But I was never able to to identify any kind of method or technique to try to improve cognitive skills. And then about five or six years ago, I ran across a method, wasn't even mine. It was, um, developed by a person who's now a friend of mine, Neil Heinz, uh, a firefighter, New York, uh, uh, uh, fire department retired as a battalion, uh, commander, uh, battalion chief, and, um, what it is is it's it's uh the the shadow box method. It's a way of having people see the world through the eyes of experts without the experts being there. And Neil, uh, figured this out and developed it as a technique and we did a workshop together and I didn't know anything about it and he said, I've got this technique, can I just include it in the workshop? It was a two-day workshop and I said, how long is it going to take? And he said, about a half hour. And I said, okay, uh, let's let's see what what it's about and when he he demonstrated it, all the firefighters in the workshop just loved it, and I loved it and so it's become, uh, my, my, uh, primary focus right now is using this method as a way of of helping people to develop cognitive skills.

**Speaker 2:** Nice. And so are you continuing to work with firefighters or?

**Speaker 4:** We are working with many other populations, not so much firefighters, although we're, we're in the process of putting in a a grant proposal to do some more work with firefighters. Um, but the firefighting community, um, hasn't had the kind of, uh, research funds for projects like this. So, um, I mean, our original project was with was with firefighters. That was the small business innovator research project. Um, but that wasn't funded by firefighters, that was funded by the Army. And the project with Neil Heinz was just a workshop that the Seattle Fire Department asked us to put on to improve their decision-making skills.

**Speaker 3:** So Gary, you you've shared initial book end, uh, in your first paper and now where you're at today, I wonder if you could think back over those 50 years and maybe share with us a couple of mistakes or maybe the biggest mistake you think you've made with regard to theories you've put out, work that you've done, uh, career trajectories. Any, uh, any mistakes come to mind? We know that experts think a lot about their mistakes.

**Speaker 4:** I think about mistakes a lot. I prefer not to talk about them in public, especially if I'm being recorded, but, um, because the two of you are such good friends, I'll trust you with this. Um, I've I've I've been, you know, thinking about the kinds of mistakes and the one that emerges for me that's that's probably the most memorable for me was when we did this small business innovative research project that I mentioned earlier to work with firefighters and this project was going to be a real sprint. It was only going to be a six-month effort and we were going to try to come up with a better model of decision-making, which was, you know, totally unrealistic and and and ridiculous now that I look at it in hindsight and we had, but I wasn't just starting from scratch. I I had a theory about how they were making decisions, because I knew because the standard decision-making models said, you've got to generate a whole variety of options, and then you can evaluate them systematically by comparing them across standard dimensions. And I knew firefighters couldn't possibly do that because fires spread so rapidly. So what were they doing instead? And I had come across a master's thesis that described a an alternative strategy where people generate just two options, a favorite and a comparison. And then they keep they compare the two and, and, and try to get to a point where the favorite is better than the comparison on all the different comparison dimensions they can think of. And I said, that feels like it's going to be more efficient and that's probably what people are doing, and that was my theory going in. And then to investigate it, the way we were going to do the research, we were going to be, uh, stationing our observers in fire stations and, uh, we would go out with them on runs and, uh, and watch what they did, observe them in action, keep notes and maybe talk to them a bit at the end. So that's the way I was going to do the research. Both of those ideas were terrible. They were, um, uh, horrible mistakes that, um, staying with them in in fire stations and going without with them on runs assumed that there's lots of fires, but there aren't that many fires, especially in today's day and age with even 50 years ago when you had all kinds of sprinkler systems and, uh, better materials. And so, after a week of of of sort of sorting ourselves out, we realized, we're not going to we're not collecting hardly any data at all. And, uh, our our our methodology is going to fail. And so we had to call a halt to our research design, and we had to come up with a better design. And I thought about that study in Israel where I had uh used the critical incidents and I said, okay, let's, you know, shift gears and let's just stay and we'll go out and and observe them if if if there are calls, if there are, um, you know, alarms that come in, but let's interview them about tough cases that they've had in the past in their career. So we were using the critical incident technique as a way of interviewing them, and even the critical incident technique wasn't good enough, working with my colleagues Roberto Caldwell and Ann Clinton Syraco, we we uh extended the critical incident technique. So we were collecting more structured material from each of the interviewss and that became a cognitive task analysis method that we've relied on ever since, the critical decision method. So that was the first mistake, thinking that we could just uh stay in the stations and observe, uh, and and go out with them, that would have, uh, six months would have passed, all the money would have been spent and we'd have learned nothing. The second mistake was the the theory that I had, the model of comparing two options, and I realized from the very first interview I did that that that was a mistake because the firefighters would tell us, we never compare options. And I said, how do you know what to do? If you don't generate compare different options to see which is the best. And they said, no, we we don't do that. I never do that. Um, I just, you know, it's just procedures. You just follow the the procedures and and that keeps that gets the job done. And I had just gotten this contract to study how they made decisions and they're telling me they never, they never make decisions. And I said, um, okay, before I leave here, can I can I see your procedure manual? And they said, oh, there's no manual. It's not written down. You'd you'd just know. And and I thought, okay, so it isn't just following the procedures and that's I got start got us started. But because I still believed in this two option theory, every time we did our our critical our critical decision method interviews, I'd ask them how they compared different options and so for each one, they would almost almost always tell us they never compare any options. For about, uh, over over, you know, maybe 90% of the cases, and these were the tough cases, they never compared options. So that gave us useful data, even though the theory was wrong, we were collecting data to, um, to try to confirm a flawed theory and that that gave us some some valuable data that allowed us to, um, to to make a publication out of it.

**Speaker 2:** So those early, early mistakes helped guide the rest of your career really.

**Speaker 4:** I was fortunate that I made that I was able to recover from those both of those mistakes and I've written about that in my the first book that I authored, uh, seeing, uh, sources of power, but, um, yeah, every everything changed because we were able to recover from those mistakes and uh, and and and and not get crippled by them. Yes. And everything followed from there. All the model building I did all built on, um, the recognition prime decision model and the the research methodology that I've relied on is all from the cognitive task analysis and none of that was part of our original thinking or our proposal.

**Speaker 2:** Um, so I think, I think you were saying that, um, if people wanted to learn more about this, they could look at your first authored book Sources of Power.

**Speaker 4:** Right. I wrote about, uh, I wrote about that first study in in Sources of Power. Yes. I was just affirming what Ryan had said that, um, Ryan was exactly right. The rest of my career, um, evolved from those mistakes and from that very first project that the the the model building that I've done has, uh, evolved from the recognition prime decision model that we formulated in that project and the way we've done I've done my research is primarily using the critical decision method that that we threw together, uh, in a frenzy as we were trying to collect data. So everything everything followed from our our fortunate ability to to recover from those mistakes.

**Speaker 2:** Thanks. Okay, so this next question, I want you to think back over the all the many interviews you've done these last 50 years and I'm wondering is there, um, a favorite insight or story that you heard from one of your interviews with an expert?

**Speaker 4:** A favorite story. Well, I'm going to I'm going to try to answer your question. I'm not going to answer it exactly the way you want. But I'm I'm going to start off by telling you, I think that's a terrible question. And the reason I think it's a terrible question, first of all, because I have so many stories that I love that it's it causes me pain to it's like asking me, you know, which uh, which child or which grandchild I like the most. So, uh, so it's really hard, but it's also a difficult question because it's hard to think of a story outside of a context, outside of a purpose. So when I tell stories, it's usually um, to make a point or to illustrate something rather than just here's a good story. But and I and I it's hard for me to just pick out one. So I'm Do you mind if I tell you two or three?

**Speaker 2:** That'd be great. Sure.

**Speaker 4:** or four? Okay, so the first one that comes to mind is actually the second interview that I did in this firefighting project where I interviewed a firefighter and, um, I asked him about the tough decisions he made and things like that and the only one, the only incident he could think of was 14 years earlier and and he made a a life-saving decision and he and and it was because he had ESP. And and all that was running through my mind was 14 years ago, I don't want to hear about. I mean, how can how can I get any useful details of a story that's 14 years old and I don't want to think about ESP, but he wouldn't think he couldn't think of any other example in his whole career. So this is a story that he had brought his crew. He was a young fire officer, brought his crew into a house, there was a fire and he assumed it was in the kitchen, they went through the the house through the living room with a hose, hit the the the what they thought was the seat of the fire with the hose, turned off the hose to see the effect, the cat fire came roaring back, it had no effect, they tried it again, no effect, they retreated back into the living room and then all of a sudden, this was his ESP moment. He's, he said to his crew, everybody out of here. And everybody fled the house. Some of them jumped through windows and they escaped and just as they were leaving, the floor where they were standing collapsed. Of course the fire was right below them in the basement and had they stayed there another minute, they would have dropped into the fire. And he said, and it was ESP, something just came over me. So I asked him, okay, um, it was ESP. So imagine, think back 14 years, you're in that, um, you're in that that, uh, home. What are you noticing? And he said, well, I was noticing, I thought the fire was in the kitchen, but, you know, if we were in the living room, it was awfully hot. And that surprised me, that it was so hot. And he said, I always I I in those days, I'd leave my ear flaps uncovered even though my ears would get burned, because I wanted to know how hot it was and I could feel how hot it was was much hotter than I expected. And I said, what else were you noticing? And he thought and he said, it was awfully quiet because fires are noisy. But given how hot this was, I wasn't hearing much noise. And my ear flaps were were off and I could I still wasn't hearing much noise. And so we realized what was he was picking up on was some a situation that didn't make sense. And he didn't know that the fire that there was a basement where the fire was there. He just knew that this was not a typical situation. That's why he told everybody to leave. And by the end of the interview, he realized he did not have ESP. What he had was expertise. And, um, he was unhappy because he had assumed that ESP had protected him his whole career and he realized it it had not. So, he, he, he was not really pleased with the interview, but he had he had learned how he actually had made the decision. The reason I'm not crazy about this story is because other people have picked it up and it's it's such a an an evocative story. So others have written about it. I think Malcolm Gladwell included it in Blink and other people have included it in other sources. So this is a a fairly well known story. Um, another story I have that's about ESP that I that I I usually don't tell the firefighter story anymore because I've sort of gotten tired of it. I like to tell the story about a British naval officer who was off the coast of Iraq in, uh, in Desert Storm and, uh, he was, uh, in in, uh, uh, a British ship that was protecting a US, um, battleship and, uh, uh, and he was watching, um, American planes, uh, I think they were A6s coming off the coast of Iraq. And some of them would fly right overhead, which which you don't like if you're in the Navy and then he saw one blip on his radar and as soon as he saw it, he said, that's not an American A6. That's a silkworm missile and it's coming right at us and we've got one minute left to live. And he ordered it be to be shot down. And you don't want to shoot it down just because you're afraid of it, because if you're wrong and it's an A6, then you've shot down an American plane and killed the pilot. Um, but he ordered it shot down. They and, uh, and it turned out that it was a silkworm missile. And nobody could figure out how he had made that, uh, decision. And he was pleased because he also thought he had ESP. And so we spent about two hours interviewing him about this. We we had the tape because we we had the radar signature, uh, that that we had gotten to, uh, to to see what he was looking at on his radar scope. And it was like just a 10-second, uh, interval, and we spent two hours trying to break it down and we couldn't come up with anything and the intelligence community had studied it intensively and they couldn't figure out how he knew it was a silkworm missile, which just made him all the more proud that he, you know, that that it wasn't physically possible, it was his powers of extra sensory perception. And, um, but what he had told it and the reason it's so hard to distinguish a silkworm missile from an A6 is, um, they're both about the same size, they fly at the same speed and, uh, there really is no way to tell them apart. They they they show the same kind of radar signature on on on the radar screen he was using. Um, but a super missile flies at about 1,000 feet, maybe 2,000, whereas the A6s were much higher, they were flying 3,000 feet or higher. And, uh, that was the only distinction between the two, but his radar system didn't pick up altitude. After he thought it was a sluckwork missile, they used another radar system to acquire the track, that one showed that it was flying at about 1,000 feet and, uh, and that's when they shot it down. So they weren't just going on guesswork, but he knew it was a silkworm missile right as soon as he saw it because as he said, it was accelerating as it came off the coast. Well, it wasn't accelerating. We looked at it. We looked at the first blip, the second, the third and they were equidistant. There was no sign of acceleration. So, uh, that's what got everybody slimed. And then that night, one member of our team figured it out and he said, You know, if if if this, um, entity, this missile or A6, if it's flying at 1,000 feet, it's not going to break out of the ground clutter until it's much further off the coast. Whereas if it's flying at 3,000 feet, you'll see it much closer to the coast. So because of that, it looks like it was accelerating when it came off the coast because it was much further out than any other the others and then would fly at the same speed as the others. So if you started clocking it at the first time you saw it, it was flying at a constant speed, but perceptually it looked like it was accelerating off the coast. So and we brought this guy back the next day and told him what we found, and again, he was crestfallen because he realized that's what he had done and it wasn't that he was his ESP that he had ESP. So we managed to disappoint two people now. Um, and now, um, the important parts of this interview for me were that he was wrong. It was not accelerating off the coast. So what he was telling us simply wasn't true. But we needed to listen to him, because if we hadn't heard him telling us that it was accelerating off the coast, we wouldn't have realized that it looked like it was accelerating and we wouldn't have solved the puzzle. So that taught me the lesson that we have to listen carefully to what people say, even though we shouldn't believe what they tell us.

**Speaker 2:** Those those are great stories. And, um, I just wanted to amplify that point you made, um, that that the goal is not to always have the person have an insight and and tell you the answer during the interview, but but to understand what it was like through their eyes. And and then sometimes you learn something they didn't even understand about the situation at the time.

**Speaker 4:** Yes, exactly. We often have people thank us at the end of the interview because they learned things from the way we ask them questions. It helped them gain their own insights into what had happened and how they'd made decisions.

**Speaker 2:** So it sounded like maybe you had one other story you hoped to tell? Is that true? Is there another good one?

**Speaker 4:** Um, there's many others. Let me tell you, uh, one of the others. It's not even, um, not even an incident, an interview I was in. It was an interview that a former colleague of ours, Beth Crandall was in, doing a project with nurses in a neonatal intensive care unit. And, uh, what was interesting there is these nurses were able to pick up early signs of sepsis in these neonates, in these babies, some of them very premature and they could pick up signs of sepsis even before the blood work came back positive. The nurse would say, this baby doesn't look right. Let's start her on antibiotics. And and this one interview she, uh, she did was with a nurse, um, who described how she was like a senior nurse on the ward and she was walking around and there were and there were, uh, individual nurses working with, each of them working with one or two neonates and she comes past one of the neonates and she looks at the baby and she stops and she looks at the baby's chart and looks over at the baby and the young nurse who was monitoring the baby wasn't seeing anything wrong. And the senior nurse was seeing all kinds of alarms that made her worried that this baby was coming down with sepsis. There were a number of cues that the baby they had done a heel stick to get blood and, uh, so the the baby's, you know, they put a bandage on, so, but but the blood was still flowing a little bit more than she expected. So that was one sign and she looked at the baby's chart and the baby had usually been able to do a fair amount of feeding, but in this case, uh, and the last, uh, half day or so, um, the baby hadn't been feeding as well. And there was a way in which the baby's belly was distended. There were just several small things like that, but all together it said, this is a baby that looks like it's in trouble. And so they started it on antibiotics, even though the blood work didn't look like, didn't show any sign and the next day, the blood work came back positive. And the reason I like that story is it's a natural compliment illustrating expertise because you had the nurse, the young nurse who was watching the baby and the nurse was sort of, um, not noticing that as the baby's temperature was going down, the nurse kept turning up the the heat and the and and in in in the, uh, system that the baby was was being, uh, uh, cared for in and and and each one was small, but it was all in the same direction. But she never, you know, noticed that nothing was ever dramatic enough to, to, to, uh, trip her alarm system. Whereas for the senior nurse, she always she saw things that that the the young nurse hadn't and to me that exemplifies expertise is seeing things that others don't. And here we had a direct comparison of the two nurses side by side, one of them becoming panicky for this baby and the other one oblivious and thinking that the baby is fine.

**Speaker 3:** Gary, I heard you say there at the end that she saw things that others don't, which is of course the title of uh of one of your books. I like how you snuck that in there.

**Speaker 4:** Thank you Brian, much appreciated. Yes. Yes, that's the most recent book seeing what others don't.

**Speaker 2:** So I think we would be remiss if we didn't talk a little bit about coronavirus. Um, we are right now in the midst of a global pandemic, lots of uncertainty and I'm wondering what do you think the NDM community can do to help, uh, with this crisis?

**Speaker 4:** Right. Um, so stepping away from an immediate reaction, this is, uh, the crisis is, uh, in a in a callus way an amazing opportunity for us to understand individual and organizational decision-making and dysfunction. Uh, being in the middle of it and to document that, so it's something that we can capture and ponder and and, uh, and learn from and, uh, and and prepare for. So that's one thing that that that comes to mind as a research opportunity. But what can we do about it? Well, one thing that we could have done, um, is apply some of the work in NDM on problem detection. Uh, because we have done research on what it takes to detect problems in their early stages before they're, uh, they're diagnosed and categorized and flagged. And we developed models of individual problem detection and team problem detection and also, um, barriers, what gets in the way of accurate problem detection at the individual and team level. And in hindsight, I wish we had gone back to those models and identified those barriers and seen which of those barriers were in operation because it's taken, uh, some countries longer than others, it's taken some states longer than others, uh, for a variety of reasons and perhaps we could have made suggestions to try to break the mindset of some of the decision makers who are sort of fixated on inappropriate diagnosis, diagnosis. And that would be a second type of intervention that might have been put into play, uh, is techniques that we've been working out to try to, uh, help, um, uh, reduce fixation errors so that, uh, people in organizations don't stay stuck as long as they were and, uh, and and realize that they need to respond. Um, so that's a a second thing that we could have done. A third is to have applied our pre-mortem method as people were trying to plan and prepare for various interventions, um, or or non-interventions, is to help people run through a pre-mortem to allow them to to do a better job of anticipating, um, what might come be coming about so that they could revise and improve the planning or non-planning as it was going on. A fourth, uh, thing that we might have done was apply some of our thoughts about management by discovery or flexible cution because when you're dealing with an ill-structured environment and that's certainly what we're in, where even as we're understanding the disease, we're not understanding what are the implications in the social and, uh, the social implications and what goals should we be pursuing, we're having to change the goals as we're going along. And so rather than thinking that we can come up with a plan and just execute that plan, to try to help individual individuals and organizations, um, expect that they're going to be modifying the goals based on what they learn so that the planning can be more adaptive. And, um, I think maybe the most important thing that we can do, and we've been involved in some, uh, preliminary efforts along this line, is in terms of training cognitive skills. And I I've told you that's a, uh, of great interest to me right now, and we're hearing that even in countries, in in some countries that are experiencing say shortages in ventilators. They're thinking, how can we get more ventilators quickly? That's part of what they need, but the other part is, how can we train, um, personnel to operate these ventilators, not just to follow the steps, but to build the expertise and the tacit knowledge of using the ventilators effectively and, uh, maybe, you know, some of the the techniques for cognitive training can be put into practice here because the shortage of trained personnel, uh, is going to matter. Even if you have the equipment and you don't have people who can operate the equipment effectively, then, um, equipment isn't going to serve the needs. Oh, and then there's one, one, one last thing that comes to mind is for the public to try to use some of the methods that we've all developed, to try to change people's mindsets, because we're seeing, seeing that people are sometimes adopting mindsets that are going to get in their way of keeping themselves safe and, uh, and trying to contextualize, what am I supposed to do in this situation or that situation and trying to understand how they can, um, adapt their own their own behaviors in light of in light of this danger. And so I think there's an I think in NDM, we've developed some tools to try to help people change mindsets and I think that those could be quite valuable now.

**Speaker 3:** As as you were talking, I was sort of reminded of post 911, uh, when there was a lot of funding thrown at the intelligence community, uh, and and folks like us to go study the intelligence community to see if we could, uh, help improve intelligence analysis. I'm tempted to think that that sort of thing might happen, uh, after, uh, or perhaps in the middle of as we are, uh, the coronavirus pandemic. And I'm wondering as a hypothetical if, um, given all the ideas you just came up with, uh, those would all be tremendous projects and they'd offer tremendous value. Hypothetically, let's say a funder had $500,000 and and really liked all one or more of those ideas you have, but for some reason couldn't give you the funds. Uh, where would you like to see money be spent? Uh, this could be with with ideas that, uh, that you're seeing, uh, in the media or ideas that you know about. Where, where do you think the investments should be headed, uh, to ensure that we're better prepared?

**Speaker 4:** Um, okay. So of the things that I've mentioned, my temptation would be to reprogram it towards, uh, the barriers to individual and team problem detection, to try to, um, break the, uh, the rigidity of organizations assuming that life is going to continue as it has and and and and not wanting to create disruption. We don't want organizations to to overreact to every small problem. So that that's not a solution either, but we want organizations to to start preparing to to react and to become, um, better able to to improvise and and and start to go down that road rather than than than getting stuck. So I would I would try to, um, to see what what could be done at an organizational level, uh, to address some of the barriers to to reframing a situation. A second thing, which you haven't asked is, um, I would go along the lines of lead uh Green and uh Jurish Patrawski and their work in training cognitive interviewing skills in healthcare providers, so that, uh, organizations themselves can harvest the expertise that they have and do it more, uh, in a in a rapid fashion. And the Green and Patrawski demonstrated, uh, that was possible to do that. And this could be a an enormous resource for healthcare providers, um, to be able to to harvest their own expertise quickly.

**Speaker 2:** Great. So we, uh, I'd like to switch gears up now and move on to an associative thinking exercise. Um, so Gary, I'm going to say a word and I want you to tell me what comes to mind and your responses should be three words or less. Are you ready?

**Speaker 4:** No, I hate this exercise.

**Speaker 2:** All right. The first one is bias.

**Speaker 4:** Bias.

**Speaker 2:** Bias.

**Speaker 4:** Bias. Um, uh, the one word comes to my mind is unfortunate that, uh, um, um, yeah, uh, it's just, uh, an an unfortunate research mindset that, uh, the research community has adopted.

**Speaker 2:** Okay. Next word is risk.

**Speaker 4:** Risk. Um, I the word that comes to my mind is challenge and excitement. Can I amplify on this or, or not?

**Speaker 2:** Yeah.

**Speaker 4:** Um, please. Because, uh, I see too many cases where people assume that you've made they they've made a plan and and and if life is good, they'll just carry out that plan the way they designed it. And then when something comes along and things always come along, not always, but often come along that makes it impossible to follow the plan. They get frustrated and and angry and and and discouraged. Um, but in fact, if they, um, this is where they make their money. This is why they have to have expertise is to be able to adapt and and manage risk and figure out how to proceed, uh, despite risk and to try to, uh, to say, I'm going to only work in situations where I can eliminate all risk, means that you're not ready for real world situations.

**Speaker 2:** Cool. Okay, I'm going to get back into the three-word answer. Ready? Next word is work.

**Speaker 4:** Work. Uh, cognitive versus procedural.

**Speaker 2:** Hmm, yeah. Okay. How about travel?

**Speaker 4:** I have a four-word answer is, those days are over.

**Speaker 2:** Okay. Soldier?

**Speaker 4:** Soldier. Uh, the word that comes to mind is hero. Uh, hero and courage.

**Speaker 2:** Okay. Surgeon?

**Speaker 4:** Surgeon. Um, expert, uh, decisive expert and decisive.

**Speaker 2:** Okay. And then scientific method.

**Speaker 4:** Often a trap.

**Speaker 2:** Do you want to elaborate on on that last one?

**Speaker 4:** Yes. Okay. So, um, so you we're all encouraged to try to follow the scientific method. However, there is no scientific method. There's no official scientific method. So that that's sort of a an illusion. But there's a general agreement on what the scientific method is. And there's usually people talk about four or five stages. So you, uh, um, you become interested in a question, you observe the phenomenon, then you start to, uh, gather observations and put the observations together to form a theory, then you test your theory and then you try to generalize it. And what I see is that when people talk about the scientific method, they pretty much lock into that testing the the the the hypothesis component. And, um, and that's that that's what they think is the essence of the scientific method. And the reason I said it's often a trap is that's the only part of the scientific method where you usually don't make any insight. When you're asking a question, you may find that there's a better question. When you start observing a phenomenon, you may see things that surprise you and that you need to investigate. When you're formulating a theory, you have to give up old ideas that don't fit the the observations you've collected and formulate some some new ones that you hadn't anticipated. When the last stage, when you generalize, you may realize that your ideas don't generalize as well as they should and you need to modify them. So in all of those stages, you are opening up yourself to creating insight. However, when you test a hypothesis, ideally, you set up your experiment, collect your data, and if everything works well, the data supports what you're uh what you expected. And now you can publish your results, but as a result, you haven't learned anything. You've just confirmed what you already believed. So this is the most, to me, um, uh, the one of the, um, least exciting parts of the scientific method. Unfortunately, because in naturalistic decision making, we mostly live in the beginning of the scientific method about formulating questions and observing phenomena and, uh, people don't, uh, give that enough credence or don't treat that with the same seriousness, uh, that they should because they think if you can't study it in a laboratory, who cares about it? I once talked to a senior decision researcher. We had an opportunity to, uh, travel to Fort Knox to, uh, observe tank platoon leaders during an exercise. And we could watch them making decisions. And I said, we we can actually ride in the tanks with them. Do you want to come? And he said, no, why would I want to do that? And I said, we can watch them make decisions and then interview them afterward. And he said, but how could we control the conditions? How can we carefully collect the data? No, he refused to come. And to me, that's that's why I'm talking about, uh, the scientific method being a trap rather than being excited as a chance to make discoveries. He was, uh, shocked by it because it was so unsystematic.

**Speaker 2:** That's such a freeing perspective to not not feel like the only part that matters is is the testing.

**Speaker 1:** Okay, well, the time has really flown by. Um, it has been a pleasure, uh, speaking with you, Gary and hearing your stories. Just want to thank you for talking with us today.

**Speaker 4:** Thank you for the opportunity. It's always a pleasure to to, uh, to talk with you and and and Brian and, uh, you you you always, um, make me think about things more more deeply than than I had in the past the going back 50 years or even, you know, looking at, uh, at today. I wasn't expecting the free association test. So, you're always pushing me, so thank you for that.

**Speaker 2:** This was fun. So on that note, thanks for listening. I'm Laura Millatelo.

**Speaker 3:** And I'm Brian Moon. Learn more about naturalistic decision-making and where to follow us by visiting naturalistic decisionmaking.org.