(Music)

**Speaker 1:** There's nothing that's more cognitively effortful in the moment than trying to convert principles into practices. It's much more efficient to have those practices ready to go, regardless of whether you understand the concepts.

(Music)

**Speaker 2:** The Naturalistic Decision Making podcast with Brian Moon and Laura Militello. This podcast series brings you interviews with leading NDM researchers who study and support people who make decisions under stress.

**Speaker 3:** Welcome to the Naturalistic Decision Making podcast. This is Laura Militello from Applied Decision Science.

**Speaker 1:** And I'm Brian Moon from Perigean Technologies. Today, we're really excited to welcome David Feldon, Professor of Instructional Technology and Learning Sciences in the College of Education and Human Services at Utah State University. Dr. Feldon's research examines two lines of inquiry that are distinct but mutually supportive. The first characterizes the cognitive components of expertise as they contribute to effective and innovative problem solving, as well as how they affect the quality of instruction that experts can provide. The second examines the development of research skills within stem disciplines as a function of instruction and other educational support mechanisms. He also conducts some research into technology facilitated instructional approaches and research methods for examining them. David's work currently focuses on cognitive load and motivation with brain imaging and eye tracking, longitudinal study of PhD students in biological sciences, and simulation based assessments of open-ended problem solving. So welcome David. Thanks for joining us again. We're really excited that you're here.

**Speaker 3:** Thank you for having me. I'm excited to be here.

**Speaker 1:** So I I didn't mention your educational background in the introduction because I actually wanted to dive into it a bit now. So as I understand it, following your undergraduate degree in cognitive science and philosophy at Johns Hopkins, you pursued graduate degrees at the University of Southern California in the Rossier School of Education. And as some of our audience might know, there's a long tradition of cognitive task analysis at USC, particularly through the work of Richard Clark and Ken Yates. So I'd kind of like to start there and start a discussion by hearing your story. So how did you end up at USC? What did you focus on there and what were those early days of exposure to CTA like for you?

**Speaker 3:** Sure. So let's start back at the beginning. I had never set out to to get a PhD or get involved in research or any of those sorts of things. I ended up studying cognitive science at Johns Hopkins, somewhat by by accident. I originally wanted to be a journalist. And so I showed up at college and uh declared a writing seminars major, but I quickly realized I neither wore enough black nor smoked enough cigarettes to be a writing seminars major based on what what it was at that place and time. So I said, okay, this isn't going to work. I need to find something else. And I sat down on steps of the library with the course catalog, which, you know, back in those dates of course was in print rather than online. And started flipping through the table of contents and I saw this thing called cognitive science that I'd never heard of before. And I said, I wonder what that is. opened it up and it was this fantastic interdisciplinary collection of cognitive psychology and artificial intelligence and philosophy and linguistics and I thought, this is this is great. So I jumped in and started along that course of study and ended up deciding around the era of the .com boom that what I should do professionally was develop educational games for learning. So went to USC originally to get a master's degree in instructional technology, which I did and spent a lot of time over in the cinema school working with game design people as well. And started doing that and then of course the .com bubble burst and I had to pivot a bit and was working with schools and teachers on technology integration and curriculum development and ran into a challenging a little bit of a challenging situation. I was talking with a teacher in her classroom explaining all the things we were going to do and she said, look, I've been teaching in this room longer than you've been alive. And I've done it my way for 25 years and I'm going to continue to do it my way for the next 25 years. So at that point, I realized I either needed more gray in my hair or more letters after my name or I was going to have a really hard time making forward progress in in the school space. And my hair hasn't yet cooperated but I went back to USC to get my PhD and while I was there, I started working with Richard Clark and got really excited by the notion of expertise and the development of automaticity and its impacts on instruction and that sort of took me along this path. You know, you've read my bio at the outset and my inquiry has taken me to a lot of different spaces but sort of at the the core of it is the notion that expertise is important that one of the things that makes experts expert is that they're able to do things with very little or even no mental effort that other people can really struggle to do. But the cost that's associated with that is that they are often less able to explain how they do what they do in an accurate and complete way and that poses instructional problems. So I work at the intersection of learning from a cognitive and social cognitive frame with performance and assessment and at high levels. So a lot of the work I've done has been in stem disciplines in part because that's one area where there is a lot of research support for folks to understand what expertise looks like, how we can know it when we see it and how we can best capture it to build it into instruction and training to prepare the next generation.

**Speaker 1:** Awesome. That's a different path I think than uh we've heard some other folks come down and so you're you're really in the educational side of disciplines generally, but you have written a lot about CTA, you've used CTA a lot. I just wanted to rattle off a few of your publications. So, a case study of instruction from experts, why does cognitive task analysis make a difference? instructional implications of cognitive task analysis as a method for improving the accuracy of expert self-report. CTA is an empirical framework for identifying cross domain training opportunities. CTA for complex learning. So you really have spent your career focused on this education and learning side. The one paper that I want to focus a bit on was the CTA based training meta analysis that was published in the Journal of CEDM, cognitive engineering decision making. I think this is a super important paper for our community and hopefully this podcast will encourage folks to go read that because particularly in the light of what we're doing as an association now with this CTA and effect initiative, which is kind of trying to explore what is the return on investment in cognitive task analysis. So, I wonder if maybe, you know, from that paper but even your other work, if you could sort of walk us through what you've seen in terms of both the good outcomes and the, you know, the challenges to CTA and in particular what you found in that paper in terms of looking across a number of studies that use CTA and trying to grasp what is that core set of outcomes that we should expect and the kinds of directions that CTA can take us.

**Speaker 3:** Sure. Let's see, where to where to start. So one of the things that I think was particularly valuable that came out of that meta analysis and I do want to uh to acknowledge a lead author uh on that paper, Dr. Colby Tole who's also at Utah State University with me. I think the most important thing that came out of it is the recognition that not only is CTA based training highly effective and more effective than non-CTA based training based on the availability of published studies, but that not all forms of CTA are created equal in terms of their ability to capture instructionally relevant and instructionally formatted information. That's something I think is is particularly powerful because, you know, in a in a world where we talk about really valuing and needing evidence-based practice, we sometimes get in our own way, right? There are many, many folks who are doing cognitive task analysis work. There are many schools of thought and techniques, some named and some not named. And there's sort of this tricky balance between trying to advocate for CTA as a productive mode of work in supporting the world of work and training and automation, on on the one hand and recognizing that some are more optimally suited for particular purposes than others. So the work that I've done with cognitive task analysis experimentally. So what there's uh actually one study, well two studies that didn't make the list but I think actually provide some of the stronger information and they're very much framed in terms of the educational and educational psychology literature. We use cognitive task analysis to work with expert biology researchers to develop a protocol and decision tree of how to effectively design and interpret biology studies and then we used it as a foundation for supplemental instruction in an intro level undergraduate biology class. And it was a double blind study, which is sort of unusual in the context of education. Usually meeting the ethics requirements and so on require that we tell people what version of the world they're getting. But in this particular case, we set it up so that we didn't need to do it that way. So neither the students nor their section instructors knew which versions of the videos they were getting. So it was a random uh double blind randomized control, you know, over a thousand students and what we found was that the CTA based version of the videos, oh and I should say that the videos were done by a colleague who was a university teaching award winning faculty member in the Department of Biology and basically we asked him to do a series of short lectures on video the way that he does them and then we gave him a script that was based on the cognitive task analysis and asked him to to do ours instead. And what we found was that there was both a significant impact on there were several significant impacts. One is that the students who got the CTA based version of those supplemental lectures performed significantly better than their peers, particularly on lab reports where that the discussion section, the interpretation of the data was really a key component. So that that was the first piece. The second piece we found is that there was actually uh motivation improvements as well. That attrition, dropout from the course was lower, particularly for women. The the original publication that came out in 2010 actually showed about a uh seven-fold difference between the number of women who dropped out in the control versus the number of women who dropped out in in the CTA based version. And the way that we interpreted that was that the one of the advantages that CTA based training offers is that it reduces extraneous cognitive load. Basically, when we provide instruction that leaves gaps in content, we force students to try to figure out what goes into those gaps on their own. So we when we don't provide them the information, they have to work extra hard to try to fill in those gaps with information we didn't give them. And so that extra effort can be interpreted as a lower level of ability by those students. We had a paper come out in 2018 looking at these data as well where we basically found that student self-efficacy, their confidence in their ability to succeed dropped as a function of the extraneous load that they were experiencing, which was due to unintentionally omitted information. So as a total package across uh across two or three different papers, we nail down with a very large sample that CTA really makes a difference. It makes a difference not only in how much people learn and how well they're able to perform, but also can really impact the extent of their motivation and their willingness to persist when they're learning in a new area.

**Speaker 1:** Yeah, this is really fascinating stuff. If you could go back to the differences between what this award-winning instructor was doing both historically and then also in the CTA infused version, what what did the what were the key differences between those two approaches?

**Speaker 3:** Okay, so the in-class lectures were identical. They all the students were in the same lectures and they had lab sections that were run by graduate teaching assistants. And before they attended those labs, those lab sections, they needed to watch the their supplemental videos that were available to them in their learning management system and nobody but us knew that there were two different versions of these videos. In the control version, it was our award-winning instructor who was a a tenured full professor, research active in biology. In the control version, he was just explaining the different concepts and and processes for how do you design a study? How do you decide what things you need to do in it? What kinds of data do you need? How once you get those data, how do you analyze them? How do you draw reasonable conclusions? So he was providing that kind of information. And when we gave him the scripts, we we completed a cognitive task analysis with three other experts in the field and had distilled down what they had to say into a set of decision rules and action steps and then we converted that into a narrative form and and had him speak off the script. And so we actually published a paper on the differences between the non-CTA and the CTA versions and it was a content analysis. I published it with Kirk Stowe back in 2009 in a journal called Technology, Instruction, Cognition and Learning. And what we found is that in the control version, the non-CTA version, there was a lot of talking about and some sort of storytelling. I remember one time I was dealing with a problem and this is what happened. In the CTA version, the treatment version, it was very procedural. It was much more on how. So less talking about ideas and more talking about how to do them, how to implement them. And so in that way, the difference was that instead of giving students ideas and stories and sort of reference points that might guide them to figure out actually how to do the stuff on their own, we were focusing the content specifically on the how to do and not relying on experts to overcome their expert blind spots, right? Not not relying on people who we know omit as much as 70% of the information that's needed to replicate their performance when they explain it, we didn't rely on that, we filled in those gaps. And so we were providing instructional gaps. We were providing instruction of the procedures in place of leaving gaps for a lot of those sort of fine-grained procedural steps.

**Speaker 2:** So this is really fascinating work. One of the things that's kind of running through my mind is that so a lot of your work is focusing on students and the NDM community is often focusing on professionals, so firefighters, surgeons, pilots. And anyway, I'm just I'm just kind of thinking about that and the same goal is to get at that those aspects of expertise that are hard to articulate. I'm wondering, do you have any observations about how expertise might be different for a biology lab course versus a professional in the world?

**Speaker 3:** I'm sorry, can I ask you to repeat that repeat that question? I want to make sure that I respond to it fully.

**Speaker 2:** It's not a very well-formed question. I'm just kind of spontaneously reflecting on that you are describing work in the context of education, so helping people learn principles, concepts and how to apply them in a laboratory, you know, a biology lab. And then I think the NDM community is often trying to get at the expertise associated with, you know, a firefighter on the job or a pilot on the job. And I'm just I'm wondering if there are some inherent differences in the kinds of expertise or knowledge you're trying to extract with CTA in a more educational setting versus the professional world.

**Speaker 3:** So from my perspective, I would argue there really isn't a difference. Ultimately, what we care about is getting people to proficiency to competency in solving specific kinds of problems and accomplishing specific kinds of goals. One of the differences that I don't think is so much about content is it is about the focus of what type of CTA you're doing is whether you're focusing on collecting procedures and decisions and the criteria that get pulled into making those decisions or whether you're focusing on conceptual information. So you you mentioned Dick Clark and Ken Yates, when the three of us did a large project together for the army a number of years ago that was developing CTA based training. And one of the things that we sort of distilled down as a principle is that the more concepts you need to complete a training module after you've included the CTA, the poorer quality your CTA was in the the style of CTA that we were doing, which is to say that, you know, we were focusing primarily on how do people perform procedures? How do they accomplish their goals? What are the decisions that go into them? What are the things that they need to look at to uh appropriately and effectively make those decisions, choose amongst multiple available strategies and possibilities. And so if we build out the CTA in that very procedural sense, there's less need to rely on concepts. And so like I said, basically the quality control, when we'd go back and look at CTAs that we're developing working with our subject matter experts, the question was, can you do what's needed in this particular task even if you don't understand the concepts based on what's in the CTA? And if the answer was no, then it meant that we needed to circle back and go a layer deeper on the procedure. And the reason for this is not because conceptual knowledge isn't important, it's not because experts don't have a wealth of conceptual knowledge, but because at the end of the day, there's nothing that's more cognitively effortful in the moment than trying to convert principles into practices. It's much more efficient to have those practices ready to go, regardless of whether you understand the concepts. We can liken it to driving a car, right? There's a whole lot of science and technology and engineering that goes into making your car work and run and be safe. There's a whole lot of physics that goes into the activity of driving and navigating road conditions, but those aren't the things that we use when we're driving a car. We have proceduralized knowledge, we have skills that have been developed over time, and those skills are what allow us to accomplish that task, whether or not we understand the concepts that are behind it. And certainly nothing I can't think of anything more effortful or slower in the moment than trying to decide how to react to a condition on the road by thinking about the physics that are involved.

**Speaker 1:** Right. Yeah, that makes makes a lot of sense.

**Speaker 3:** So and that that was part of what also prompted the comparisons in the meta analysis that we talked about earlier, right? Different modes of cognitive task analysis, different approaches to it have different emphasis. And so it makes sense that different emphasis yield different results and different results might be optimized for different kinds of applications. So I have no, you know, no objection to, no problem with CTAs that are heavily conceptually focused. There's plenty of good things that we can do with that. But for my focus on training novices and trying to make them less novice, we we like to start with the procedures.

**Speaker 2:** Yeah, that completely makes sense. So the other thought I'm having is exceptions. So, so one of the things I'm always interested in is when is this procedure not going to work? So if you're teaching someone to drive, when is the car going to not behave the way you expect because it's icy or it's wet or or whatever. And so is that something you're trying to work in or would you save that for a more advanced kind of training? Like in the novice training, we're just trying to get the basic procedure down.

**Speaker 3:** So when I talk about the procedure, what I mean when I say that is it's not just a set of steps that we follow one, two, three, four, five. It's not a recipe. When I talk about a procedure, it includes decision points and so functionally, a procedure is a branching decision tree. And an essential part of learning the procedure is understanding what did to use medical terms, what are the indications and contraindications for going down one branch of that decision tree versus another. Now, obviously, we can't capture every single possible scenario that ever was or ever will be, right? That's and that's not the point of training. Especially when we're focusing on novices, roughly speaking, if we say that a CTA protocol that we've developed and are incorporating into to training, if that covers 80 to 90% of the scenarios people are most likely to encounter, then we've done the job for that, right? Expertise comes from time and experience and diversity of problem solving opportunities. We've known that for a long time. But again, the training that gets delivered most often are to novice to who are really starting at the beginning and no matter how good the training itself was, you simply can't get them to expertise because that requires time and real world experience. So if we can do a good job of moving people from novice to solid intermediate and prepare them with sort of a modeled on a decision tree that covers 80 to 90% of what they're most likely to encounter, then, you know, we think that that does a good job. And we have the data that uh across a number of studies that shows that that's the case, that people do perform better and better over time when they're trained on cognitive task analysis, particularly of the type we use, you know, in our studies.

**Speaker 1:** Yeah, this is very nice. This is, you know, really resonating with me this notion that if we're working with novices, can we give them the kind of foundational mental models that's going to serve them most of the time? And then as they obtain skill, they're going to start to understand all this corner cases and build out that foundational understanding that we've given them. Yeah. That's really nice. So, one of the things that's fascinating about you is although these two CTA communities, the NDM folks and the the folks that kind of grew out of that USC school have operated pretty much in parallel, you've kind of been connected with both communities and I'm wondering when did your path start to cross with the NDM folks and are there kind of complementary similarities, differences that you've have observed across these communities.

**Speaker 3:** Sure. So, uh you know, I'm I'm really a newcomer to the Indian community overall and one of the one of the things that sort of brought it back to the top of my awareness was when the CTA project that's going on started getting disseminated. I think Brian sent me an email and provided me with the the announcement, the CTA in effect stuff. So I'm really new, I've, you know, obviously in my studies and in in my learning and in my execution of empirical studies, both, you know, I've used a lot of the literature. I've, you know, academically grew up on, you know, Hoffman's work and and, you know, and Klein's work and you know, a bunch of folks like that. But I'm really sort of a newcomer to the organization as a whole. You know, one of the ways that I think that things have differed across sort of the more academic community and the NDM community is that as a tenured professor, I have a lot of luxury in that I don't need to worry about whether or not the way I think is best ends up being best, right? I have we have job security and I, you know, I can go out on a limb and I can say I think that this way of doing CTA or this way of designing training is the best way there is out there. And I can run a study and if it turns out I'm wrong, I lose nothing. When you have folks who are doing this professionally day in and day out, reputation is very important. And so I think in some ways constrains the types of risks that people are really able to take and exploring, you know, and testing ideas, right? So in the NDM community, you have people who are professionals who have livelihoods and who are trying to in a very real and legitimate sense build brands. And on the one hand get a very committed group of folks who are trying to move the field forward and who are doing a wonderful job getting a lot of visibility and raising opportunities for CTA and other effective methods of building training and instruction and human performance systems, getting it out there, getting it into practice. But it's not a a scenario that involved a lot of controlled studies, right? It's not something where we have sort of the full liberty to follow the evidence wherever it takes us because we're bound to clients, we're we're bound to projects, we're bound to brands. And so one of the things that I found is a real difference between the two areas is to what extent are we free to really challenge our our presumptions and to change course radically saying, well, we thought this was a really great idea but actually it turns out it's not a really great idea and we should try something different. So on the one hand, you know, this is basically a lot of the difference between research and practice, I think in any field and in any area of endeavor, which is that the folks who are doing practice are the ones who are actually making it happen and taking it out of the world and doing things that help move our society and and improve the world around us in a very hands-on way. And the scholarly community has different goals where it's often less about implementation and more about trying to take things apart and understand how they work and put them back together again and and understanding that there that there's risk and that something may not work the way we want it. I mean, Lord knows there have been studies that we've run, studies that I've published where, you know, you look up six months later and go, oh, that was a terrible idea. That was a mess, right? Absolutely absolutely happens, but that's not sort of the type of conversation that happens in the same way. So that's really one of the bigger differences I think between the the scholarly and non-scholarly communities is are the types of endeavors and the types of risks that we're we're willing and able to take on based on what we're trying to do.

**Speaker 2:** Yeah, that's a really insightful comment. I think yeah, that's definitely resonating with me too. I think as you were describing the study, the the double blind study, I was thinking, I have no idea how I could ever pull something out like that off. Even if I designed it, like I wouldn't have the right connections, I wouldn't have the way to make that happen. So one thing you're saying is that you don't have to worry so much about a brand. So you can design riskier studies where you might really learn that you're wrong. It sounds like you're in a world where you really can design studies and collect data that are hard to do if you're studying firefighters or military command and control. So that's a nice thing about your world too.

**Speaker 3:** Yeah, no, it definitely makes for interesting conversations and of course, the biggest challenge is where those two pieces meet up, right? The question of transfer and generalizability is one of the biggest challenges because you're absolutely right. There's no way to do some of the things that I did with the biology students, there's no way to do with fireground commanders or submarine drivers or, you know, any number of other folks. And so then we are faced with this question of, okay, so we did a controlled study where we were able to really isolate some specific things that seem to make a big difference, but we didn't do them with those folks. And so did the lessons that we learned from our study, to what extent do those apply? And it it becomes a theoretical question, not an empirical one because we can't do some of these studies in some of these contexts.

**Speaker 1:** Right. But the lesson I took from that study really wasn't about the expertise of the students, it was about the different content and presentation from the expert instructor.

**Speaker 3:** Yes.

**Speaker 1:** So you were able to tease out that the content derived from the CTA with other experts in that field, other expert instructions, when presented to the students, made a difference in the student's performance.

**Speaker 3:** Yes.

**Speaker 1:** And that kind of efficacy teasing apart is, I think, what I'm jealous of and I imagine Laura is too because the other piece of this, you know, brand versus academic practitioner versus, you know, whatever you want to call it, is that we so rarely on the practitioner side have the opportunity to follow through and get the data, get the evidence that what we've done here with the CTA and the findings in whatever applications they go into are actually making a difference and that feels to me like a sort of an next horizon for the NDM community is to be able to do those sorts of things and you've done it in a number of these papers that you're talking about. And so what I'm foreseing is, you know, the exploration of more ways that we can bring CTA into these experiments where it is the intervention that we're actually testing, right? And so that's what I think is so important about your work is because you've done that and you've clearly shown that it it works and I think that that's definitely something our communities can pursue together.

**Speaker 3:** I'm wondering David if, you know, you've sort of been looking at CTA applying, studying it for for a couple of decades now. Are there trends that you've seen sort of in the communities that you've been in with CTA over the past 20 years, Any any changes you've seen or from your perspective are we still sort of doing the same thing that was developed in the 80s and 90s and we're doing it because it works or have you seen any innovations or trends in CTA practice that have been noticeable to you.

**Speaker 3:** Well, you know, I think it's it's hard for me to to say because of course one of the features of how we do CTA and how we do CTA in the field is that we're a little bit tribal, right? There are different sort of groups of folks who orient around particular strategies and tend to talk mostly to each other and, you know, have a particular way that they go about doing things when they work as a team and go and, you know, go out and work with a client, you know, or or do a CTA for a study. So I don't feel like I'm really in the strongest position to to evaluate how CTA is evolving across fields or across ways of doing it, but it's also CTA is a part of what I do, it's not all of what I do. So it's completely possible as well that if I were, if CTA were were the primary or, you know, exclusive focus of what I did, I might feel a little better in in that regard. You know, overall, I would say though that I think one of the big changes that's relevant now is conceptualizing how CTA and related performance systems and training systems uh kinds of design considerations interface with artificial intelligence, right? So, you know, one of the things you mentioned when you read the bios is that I do some simulation based performance assessment work in open-ended problem solving. And one of the things that's come up on a a few different projects now has been, you know, the question of, well, do we need CTA to map out all these decision points or can we just harness artificial intelligence to sort of figure out what the full range of permutations is of how a learner might act within a simulation and then just use the simulation itself to determine what the the optimal solution paths are. So, for me, that's an area of sort of tension and and a need for future exploration because on the one hand, you know, AI is very powerful and can solve through brute force things that humans simply don't have enough time, effort, or processing power to figure out that way. But on the other hand, artificial intelligence does not do a good job of it it's sort of a black box. It's hard to pull out in human understandable terms, why each step was done the way it was done. And of course, that's a strength of cognitive task analysis is that literally we're looking step by step at how and why people are are doing each of the things they're doing, making the moment to moment decisions they're making. So, like I said, I don't really feel like I can speak to trends and changes in how various folks are doing CTA. I do think that one of the things that we need to be thinking about and engaging as a community is what are the intersection points, the overlaps and the disconnects between a CTA oriented mindset for fueling human performance systems and training versus an AI or probabilistic computational model approach to those same things.

**Speaker 1:** Yeah, you said a number of important things there that back to the beginning about the tribal piece. I think that's something also that our community could be talking more about. We do have our pet methods and we use them because we know they're efficacious but we're also good at them, right? So, you know, I use content mapping a lot, probably more than lots of other folks in the community except for maybe Robert Hoffman. And not so many people use that method. We all use things like critical decision method and some of the verbal protocols a lot as well. So, there's probably a from my perspective, a sort of a core set of methods that we use, but yes, I agree there is this sort of tribalness in the community around methods that I think is just interesting for us to talk about. I think at the end of the day, we're all on the same side of the fence to say do something like this when you're trying to understand problems and when you're trying to help others get smarter. But that second piece, I think that's also really interesting and I've kind of been wondering about the idea of, you know, if we do turn some of these large language models and and other tools loose on data that was collected from experts. I think there's some interesting implications there because it seems to me like these tools are are crunching on data that come from lots of different places but I've been curious to see what might happen if you turn some of these tools loose on CTA results. What would that look like both for the analysis piece and maybe even for giving us new ideas about new augmentations that these tools could be used for. So, yeah, I I do think that's you know, there's a lot of work in our community going on around things like explainable AI and and those kinds of concepts, but there's more potentially more interesting ideas available if we maybe expand our own communities thinking on on how these tools might be useful on our own data. I think that's I don't know, I'm intrigued by that.

**Speaker 3:** Yeah. Ken Yates has actually been doing some work on this with Richard Clark. They had some uh support from the Schmidt Foundation and we're digging into issues of how AI and CTA interact, both in terms of how can we utilize AI to more effectively accomplish CTA and more efficiently accomplish CTA, but then also uh ways in which CTA can interface with with AI. One other thing uh Brian that you mentioned that brought another paper to mind and talking about the the different kinds of of CTA and the different methods that are elicited. Ken's dissertation that he and I then co-published as an article in theoretical issues and ergonomic science, actually broke down, did a literature review, like a meta and analysis style literature review, looked across the publications of CTA and broke it down into knowledge elicitation and knowledge and analysis pairings and then looked at the relative frequency with which these were used. And I felt like it's a it's a neat little piece of work, but I think it's one of the things that helps to empirically illustrate where the ways that we label particular approaches to CTA can sometimes mask similarities and differences. So if if that caught anybody else's imagination as you were talking about, well, you know, I used this piece and this piece together, it kind of provides a survey of the landscape of the field orienting not around how we label the approaches to CTA that we use, but fundamentally what the operating pieces are within those. And so, you know, as we work as a community to try to be a little more effectively communicative about what we do and how we do it and and where there are similarities and not, I think Ken's work provides a good framework for facilitating that conversation.

**Speaker 1:** Right. That's excellent. We'll get all these resources posted along with the podcast so folks can get directly to the papers. So, so we've talked about Richard and Ken a lot. I'm wondering, we usually ask for three people who have influenced your approach. It sounds like those would be two of the three. Who else? Give us a couple of their names of folks that have kind of influenced your work and your approach over the years.

**Speaker 3:** Well, let's see. So, you know, obviously Dick was my doctoral advisor, Ken was a student whom I uh taught and Dick chaired his dissertation as well. So the three of us really have worked together for a long time, but, you know, the the work that folks have done in sort of some of the foundational cognitive task analysis approaches have really been influential. You know, the going back to PRI and critical decision method and some of those foundational works really have been uh very influential in in terms of shaping how I think about what kinds of information we need, what kinds of ways we can go about getting it, most especially the types of information that are so core to expertise that experts are no longer aware of what they're doing there. So those are really I think the two really pivotal points, thinking about procedures and thinking about how people make decisions in the moment, particularly when things are unexpected. And sort of what's the the science behind how we can most effectively pull out that information.

**Speaker 2:** So I've wanted to ask, before this podcast, I was looking at your Google profile and all your publications over the years and I was wondering if there's one particular CTA project that really stands out for you as as particularly fulfilling in the course of your career.

**Speaker 3:** So, I think the one that I got the most personal satisfaction from, actually ended up yielding no publications. And the first time that uh it even got written up outside the agency was the project that I put in for the the Schmidt competition that just wrapped up yesterday. I was working with the South Carolina Department of Health and Human Services. The branch that they have over there that works with Snap benefits with helping people get uh food assistance. That program having a bit of a hard time when state departments who administer uh that kind of assistance make errors in terms of how they allocate benefits, they get fined by the federal government and it can really cause a a big problem for their ability to operate effectively. And when I ended up getting connected with those folks, they were staring down the barrel of some pretty hefty fines that were really going to hinder their ability to service the community and to provide the benefits that people in lower income areas that really rely upon to for their their health and well-being. And so the deal that they worked out was that they were going to refine and revise their training for new case workers for mitigation against the the fines that were associated with some of these decision-making errors. And so we did a cognitive task analysis working with expert case workers to who people who decide who qualifies and what level of support they qualify for under the Snap program. And so we worked with some other experts and their trainers and instructional designers and built out a training. We were actually able to run it in a little bit of a controlled study format. We had data from folks who had gone through the traditional training and then we had data from a couple of cohorts that went through the new form of training. And so I think I feel like it was it was an it's an important thing and a necessary service and we know literally we're helping put food into the mouths of children. Like this is a this is a really important thing to do. And the data that came from it that uh that I I found really meaningful beyond sort of, you know, we always do kind of pre and post test kinds of things around a training, but because of the nature of the performance and the nature of the data, they were doing long-term quality assurance audits, right? So they were actually pulling cases that people had worked on where they had decided to give or not to give food stamp benefits and analyzed them to see whether or not they were making mistakes and uh and what we found is that the cohorts of of folks who went through the CTA based training had significantly lower incidences of errors. Everybody makes errors. The decision-making on this is really complicated. There are lots of different factors, there's a huge amount of documentation that people who are applying for benefits need to bring with them and, you know, that information isn't always wholly reliable and it just it it really gets very complicated but the stakes are really high, right? Because we we were talking about whether or not people have enough to eat. And what we found from the folks who got the CTA based training is that longitudinally over the first six months on the job following training, they were less likely to make mistakes and that meant that uh people were more likely to get the support that they needed.

**Speaker 1:** I love that study. I voted for it, just so you know. That's I really enjoyed reading about that. Yeah.

**Speaker 3:** Thank you.

**Speaker 1:** Yeah, it's interesting as we were, if anyone takes a look at all those studies, it's really interesting all the different kinds of returns on investment, right? So, you know, we we think about financial gains and, you know, that sort of thing, but there's so many different studies in this initiative that really talk about those returns in different ways and we can quantify and and sort of qualify them in in different ways. But yeah, yours was, yeah, it was a great study and I'm so glad that you included it. So, we are running out of time. This is a conversation that could probably go two podcasts at least worth, but we are getting close to the end. So I want to kind of ask you a kind of a fun question, but actually with a little bit of serious edge to it. So, the fun question is, if you had an unlimited grant in which the only requirement was that CTA must be the core method, what kind of program would you put together? What would that program look like?

**Speaker 3:** Wow, that's a that's a big one. You know, and if we did if it's truly unlimited, I'd say, well, it's everything everywhere all at once, right? But uh

**Speaker 1:** That's right.

**Speaker 3:** But but scaling that back just a little bit, I mean, for me, there are for me, there are a couple of really core issues that we need to to sort out. One, and this is something that's been kicking around for, boy, at least 15 years that I can think of is how do we take on sector-wide multiple levels of training that rely on CTA? Right? So, a lot of times when we develop CTA based training, we're focusing really uh just on like the introduction level, right? New employee onboarding, novice training, but what we haven't done is really understand how we can build into whole vertical systems that support people in an ongoing fashion across their the their professional lifespan. So that's one piece that I definitely want to uh to dig into. Another piece that I wanted to dig into which is what I mentioned before that Ken and Dick have have spent a good amount of time working on over the last few years, it was with Schmidt Foundation support is understanding how we can use artificial intelligence as a way to bolster the efficiency of CTA. Because of course, one of the barriers to having CTA based training implemented is that the front-end cost is relatively high. It takes people who have expertise in their own right in doing CTA. It takes the it takes extended amounts of time from subject matter experts. It's not a fast process if it's done well. And that upfront cost can often be, you know, a major barrier. Certainly for smaller clients, but even for larger clients who aren't looking at the long-term returns. And I mean, for for years, uh back into the 90s, Clark had uh published some different analysis showing the cost the cost beneficial nature of CTA based training, which is yes, you have a higher upfront cost, but in terms of uh avoiding time lost for retraining, cost due to errors, those kinds of things, it pays for itself. But it would be better just to not have uh as substantial of barriers to entry to begin with. So that would be, I think really the other area that I that I'd want to look at is how do we use the, you know, rapidly evolving AI technologies to help streamline and bolster and enhance the ways that we do CTA now and then how can we look at an entire ecosystem within a field, a training ecosystem within a field and understand the role that cognitive task analysis supports can play across the whole spectrum, not just with sort of novice entry-level onboarding type performance. And actually I think I'm going to add one more piece into that as I think about it. And that is how CTA based training interacts with the cultures of the organizations that I might be engaging with it. You know, one of the things that we encountered during our work with the army that really made an impression on me, you know, the since Napoleon, the military tradition has been that you don't tell officers how to do something. You tell them the task and the purpose and then the expectation is that the officer is going to decide how that gets executed. But of course, if you're doing CTA based training in a very procedure oriented way, literally the training you're providing is telling people how to do things. And so some of the places where we encountered some challenges had to do with a culture clash, right? Whose job is it to decide how something is done? And from an expertise perspective, I look at it and I say, if everybody could perform it in the same way that that true experts could perform it, we'd all probably be better off. But, you know, but but we don't operate in a vacuum. And so I guess really the third area that that I think would be important is understanding how institutional cultures, cultures within professions interact with some of the underlying assumptions that drive cognitive task analysis based systems.

**Speaker 1:** This sounds like an awesome program you're going to be running here in 2024. Excellent.

**Speaker 3:** Thank you.

**Speaker 1:** And that means you have that funding for me.

**Speaker 3:** Oh. Oh, oh I didn't realize. Yeah, uh completely agree with the program. Uh would add a few more pieces as I'm sure we all would, but this has been really great David. Like I said, we could go on and on and my hope was in part with this podcast that we would start that bridge building although I don't know that there's really necessarily bridges to build so much as getting everybody more familiar with each other's work and so you've definitely helped us to do that today. So on that note, I think we can wrap up. It's been a pleasure talking to you and thank you again for joining us.

**Speaker 3:** Thank you so much for having me. It's been a pleasure and I look forward to to more conversations and other venues in the future.

**Speaker 1:** Indeed. So for the NDM podcast, I'm Brian Moon.

**Speaker 3:** And I'm Laura Militello. Learn more about naturalistic decision making and where to follow us by visiting naturalisticdecisionmaking.org.

(Music)