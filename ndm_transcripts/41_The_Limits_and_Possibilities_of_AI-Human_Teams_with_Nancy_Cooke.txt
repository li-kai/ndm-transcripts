(Music)
Speaker 1: Well, there's a lot of issues. So one that we've been dealing with lately at the university is chat GPT, and a lot of universities are talking about outlawing it, but I don't think that's the approach. And so we're instead talking about embracing chat GPT and teaching students how to use it. How to make your work kind of along the lines of the center model even better than it would have been without it and teaching instructors. What is it good for? What is it not do right? You can run some interesting tests on it and find that it is not perfect, but it's it's pretty good. But, you know, we introduced calculators a while back and we didn't have to take them away. And we told students when they could use one and when they couldn't.
(Music)

Speaker 2: The Naturalistic Decision Making Podcast with Brian Moon and Laura Militello. This podcast series brings you interviews with leading NDM researchers who study and support people who make decisions under stress. Welcome to the Naturalistic Decision Making Podcast. This is Laura Militello from Applied Decision Science.
Speaker 3: And I'm Brian Moon from Pergin Technologies.
Speaker 2: I'm so happy today to get a chance to speak with Nancy Cook. For those of you who don't know her, Nancy is a professor in Human Systems Engineering at the Polytechnic School, one of the Ira A Fulton Schools of Engineering at Arizona State University. She also directs the Global Security Initiative Center for Human AI and Robot Teaming. Professor Cook's research interests include the study of individual and team cognition. Some of the applied research topics she addresses include the development of cognitive and knowledge engineering methods, Sensor operator threat detection, cyber intelligence analysis, remotely piloted aircraft systems, human robot teaming, health care systems and emergency response systems. She specializes in the development, application and evaluation of methods to elicit and assess individual and team cognition. Dr. Cook is a past president of the Human Factors and ergonomics Society and the past chair of the Board on Human Systems Integration at the National Academies of Science, Engineering and Medicine. She also recently chaired a panel for the National Academies on enhancing the effectiveness of team science. Dr. Cook was a member of the US Air Force Scientific Advisory Board from 2008 to 2012, and in 2014, she received the Human Factors and ergonomics Society's prestigious Arnold M Small President's distinguished Service Award. Welcome, Nancy. Thank you for joining us today.
Speaker 1: Thank you, Laura. Thank you for that really kind introduction.
Speaker 2: In these podcasts, I always like to talk about how people got started. And so I was wondering if you can remember the very first paper you ever published and tell us what that was about.
Speaker 1: Oh, yeah, it was a really interesting experience. I was in graduate school and my advisor Roger Schwant was on sabbatical, so he left me in charge with submitting the final version of our paper on basically the right stuff. What do does fighter pilot expertise look like compared to novice fighter pilot people who are just learning how to do fighter pilot maneuvers. And uh we used multi-dimensional scaling and Pathfinder network analysis and compared the conceptual models of expert fighter pilots to novice fighter pilots. Um, and so I was left sort of finishing up the paper and submitting it to International Journal of Human Machine Systems, which is where it went. My very first paper and he came back from sabbatical and he's like, okay, so how's that paper coming? And at by that time, I I already gotten it back and it was accepted without any revisions. And I'm like, this is really easy. And my advisor couldn't believe it. He's like, you mean they didn't ask you for any revisions? I'm like, no. Was I supposed to do that? And it was just it was one of those experiences that kind of set the stage for what wasn't to come.
Speaker 2: That's amazing.
Speaker 1: Yeah. Yeah, I thought so. Yeah. Now in retrospect, I think it's a miracle. Wow.
Speaker 2: Well, and so but your first paper was on a really substantive topic.
Speaker 1: Yeah, it was we had some interesting experiences with the data analysis too. I remember another graduate student story that uh, we had similarity ratings uh, of these concepts that we had people do and we had to convert them to proximity ratings by basically inverting them. Um, well, unbeknownst to the people in the lab, I I uh did the conversion and then another graduate student went ahead and did the conversion again, reverting them back to similarity ratings. We got the results back and we aren't expert fighter pilots. And so we looked at them and we're like, hmm, I wonder what this means. Um, you know, these concepts are related for novices and not experts. And finally, we showed it to expert fighter pilots and they're like, this doesn't make any sense at all. Things that should be related aren't related. And we finally realized our error and it's a data analysis, uh, mix up I tell my students all my students in my lab. And example of poor teamwork and coordination.
Speaker 2: Ah, yeah. So even in your graduate work, you were really looking at at real world problems. Like you did not start out doing kind of basic research lab studies. Is that right?
Speaker 1: That's right. And you know, what was really interesting is I went to New Mexico State uh for my master's of PhD in in human computer interaction is what I wanted to study. And they hadn't engineering psychology program. Well, I got uh my my mentor and the person who funded me while I was there, was Roger Schwant and I was telling my fellow graduate students that I'm in engineering, I want to do human computer interaction and engineering psychology and they said, well, you're working in the area of cognitive psychology, I hope you know. And I'm like, okay, uh, but I liked it and the kind of cognitive psychology we did the whole time I was there was very applied and so I think it kind of led nicely into what I'm doing right now.
Speaker 2: So interesting. Not many people start out in the applied world. That's that's really cool. So, do you remember at what point you heard about or connected with the NDM community?
Speaker 1: I I'll started with uh Gary Klein, I think and some of the um NDM meetings that they had. I can't remember in what year my first exposure was, but I know I read a lot of uh Gary's work on recognition Prime decision making. And that really inspired me.
Speaker 3: Nice. So for you there never was like this kind of dramatic turn. Like I I I thought I was going to be studying micro cognition in a laboratory and then that that wasn't your path at all.
Speaker 1: Yeah, I always had some really applied interests and starting with the fighter pilots, you know, that was pretty applied. What I really would like to do and and why I was thrilled to have published my interactive team cognition paper in cognitive science was to do really good cognitive science or cognitive psychology in the applied world in the field. I I don't think those two are not are incompatible.
Speaker 2: Right. And so you were using methods like Pathfinder, which I don't know a ton about, but um later you started looking at different interview techniques and ways to elicit expertise. Was that a direct path also or was there a shift in the way you thought about good cognitive science and what that meant?
Speaker 1: The the knowledge elicitation work correctly followed the fighter pilot work because we were trying to figure out what the right stuff was. How do you elicit knowledge that a fighter pilot knows? And so that led to the interest in in knowledge elicitation and at the same time I was taking some artificial intelligence courses if you can believe long ago, using I think uh Winston's book and uh learning about expert systems and that kind of also primed the pump for more work on knowledge elicitation because that was supposed to be the thing at the time that would would feed the expert systems. My abrupt turn really was probably when I went from individuals to teams. That was a big turning point.
Speaker 2: Ah, I see. So tell me about that. Tell me about that transition for you. Was there a project in particular that?
Speaker 1: It was it was Eduardo Salis.
Speaker 2: Ah.
Speaker 1: Yeah, I um I was working on, you know, individual knowledge elicitation at the time, uh doing still applied work for the Air Force and other agencies and Eduardo Salis contacted me. I didn't really know who he was. I didn't know that much about him because I wasn't in the field. I quickly learned about him though. And he said that they were working on this um Tadmis program, tactical decision making under stress. And he was at Nac at the time and they said that there were a lot of uh industrial organizational psychologist working on teamwork and teaming and how that played a role in the Vincennes incident, which is what started the Tadmis project and or just for those who don't know the Vincennes incident was when a US ship mistakenly shot down an Iranian Airbus full of civilians and part of the blame was put on um poor teamwork, also poor displays, but Eduardo was looking at the teamwork aspect of things. And because they were looking at decision making, he realized that they needed to talk to some cognitive psychologist and bring cognition into the area of teamwork. And so I said, well, I don't know that we know how to do this at the individual level, but why not? And so I uh, I went and met with him and from then on, I started working on on teamwork and team cognition.
Speaker 3: So it's you've mentioned two people so far, uh Eduardo and and and Roger Schwant. I I wonder uh we we've had Eduardo on the podcast. I wonder if you could tell us a little bit about Roger and kind of his work and his influence on you.
Speaker 1: Yeah, Roger was a, he was a great advisor. He's still around. He's I believe still in Las Cruces, New Mexico. He's actually the one who brought me to Arizona State because he moved from New Mexico State to Arizona State. They started an East campus, which is now the Polytechnic campus and he actually brought me out here and so I owe that to him because great things have happened in Arizona. His area was lexical decision making and semantic memory. So he was not really an applied psychologist, but kind of became one through the development of the Pathfinder technique and then through this grant that was from the Air Force to look at fighter pilot expertise. So I I kind of got to New Mexico State when he was kind of making this transition himself.
Speaker 3: Interesting. So, so was he familiar with knowledge elicitation or was that something that that you later developed with him?
Speaker 1: That was something, well, so I count Pathfinder, which is like multi-dimensional scaling is one way to elicit knowledge, sort of conceptual knowledge. And through the development of Pathfinder and by doing studies that made use of Pathfinder, that was the beginning of more work on knowledge elicitation and then it became my my dissertation work for that I later published.
Speaker 3: Got it. So, so you started with Pathfinder, but were you reaching out, you mentioned Gary's work obviously, but were you reaching out to others doing cognitive task analysis and then those kinds of methods?
Speaker 1: Uh, yeah, I was I was learning about that too as I went. We actually um I did some short summer period of research with the Air Force Research Lab in San Antonio and they were doing some cognitive task analysis there and I I connected with some people who were doing cognitive task analysis on predator operators out at Creech Air Force base. And that uh was really nice because it led to the development of our remotely piloted aircraft synthetic task environment. So, everything kind of connected.
Speaker 3: Right.
Speaker 1: Kind of way.
Speaker 3: Do you have anyone else who you think of is kind of being an influence on you?
Speaker 1: Yes, Frank Durso.
Speaker 3: Tell us about Frank.
Speaker 1: Yeah, Frank was interesting too. He came as a visiting professor while I was in graduate school uh for several periods of time. And just like Roger, he was kind of learning applied psychology at the same time. So we all kind of were on this boat together. And Frank is the one, so I thought that I was going to do human computer interaction and I didn't think I was going to be in academia because I thought that sounds kind of scary. It sounds like you might have to do a lot of public speaking, you might have to do a lot of teaching and you might have to do a lot of traveling. I'd rather do something like work doing human computer interaction, you know, nine to five at IBM, that was the place at the time. And Frank is the one that convinced me that that wasn't the path for me. I'm really glad he did too because I really loved what I've done in academia and I think I would have been pretty bored at IBM.
Speaker 3: Right. So you're your initial plan was to not even be in the field or even outside the building?
Speaker 1: Yeah. And and Frank said I I'll never forget this. He said you need to consider going into academia. I'm like, well, why? I really thought I was going to go work in industry and he said, well, if you do that, we will never know what happened to you.
Speaker 3: Okay.
Speaker 1: In other words, I would never be publishing and going to conferences. I'm not sure that's true.
Speaker 3: But but it sounds like the going out to the field piece was just not personally of interest. Is that true?
Speaker 1: Going out to Yeah, sort of doing field work and uh and that sort of thing. Was that was that not a personal interest?
Speaker 3: Okay.
Speaker 1: It it ended up being one, yeah. I guess I wasn't aware, you know, it's funny all the things that I was dreading like traveling and public speaking and maybe getting my hands dirty in other people's uh work.
Speaker 3: Right.
Speaker 1: They were things that I ended up liking the most. So I tell people who are beginning in their careers that keep an open mind and kind of sees opportunities as they come because you never know.
Speaker 2: So, I know the the first time I really became aware of your work, Nancy was that paper you wrote on varieties of knowledge elicitation. And I was a young research assistant and working for Gary Klein and learning the critical decision method and that paper was just hugely influential to me. Begin to understand who else was working in this field and I remember you wrote about the work Jared Gott was doing and Sally Gordon and just, you know, all these people who were doing really interesting things with knowledge elicitation at the time. So for me that was like one of the most influential things I remember at early in my career, but I'm wondering when you look back on your work, what what has been some of the most fulfilling work you've done?
Speaker 1: Hm. So I think my my work on interactive team cognition and developing that theory, along with lots of other colleagues with, you know, Jamie Corman and Chris Myers, Mustafa Damir and just a whole lot of Jasmine Duran. We developed it through a series of seminars really where we just talked about it and brought in some fresh graduate student minds and really hammered the assumptions and theory out completely, then wrote the article and it was really fun. And I had this great experience again a year ago in a class that I taught on team cognition that had 25 students in it, undergraduate and graduate. And we had a great time. We developed lots of new ideas and lots of new directions for research in team cognition. We talked about the definition of teams needs to be greatly expanded and there's many, many dimensions to teams. So we came up with a concept of teamness. And we recently just resubmitted with our revisions this at the forefront paper for uh human factors journal and so hopefully, fingers crossed, they'll accepted. It's not definitely wasn't accepted automatically like my first paper, but but uh it was really fun and the students got a lot out of it and we're publishing it as a group.
Speaker 2: That is so cool. So so tell me about teamness. That is a I haven't even heard someone say teamness.
Speaker 1: Yeah. hopefully you'll be hearing it more. So we we talked about different kinds of teams, like take dancing for instance. You could be in um Tango dancing and um it takes really good teamwork. You could be in a a group of folk dancers and that's that could be considered a team. What about line dancing or just dancing in a club? Are those teams? And so we talked about different, you know, degrees to which team members interact, different degrees to which they're dependent on each other, interdependencies, sizes of teams and how teams can come and go, they can be very fluid, you know, how long teams stay together and and are they more of a team if they're they stay together and just a lot of different dimensions like that. And now we have machines on teams, does that change the degree of teamness, if you can't speak in natural language to your teammate? And and we had lots of examples through the class of different kinds of teams, cooking teams for instance versus search and rescue teams, or medical teams, and different teams may change the degree of teamness over time. You may have to do a lot of interaction when you're doing one particular part of the the surgery for instance, and and less maybe in the beginning or at the end.
Speaker 2: So is part of the notion of teamness is that there aren't like hard and fast rules you're either a team or you're not a team, but that you could be more teamy or less teamy?
Speaker 1: Yeah, and I think where that's relevant is that we have all these findings about, you know, in the area of teamwork and there could be some things that are true for teams that are high in teamness, but not for teams that are low in teamness or vice versa. And so we're concluding that when people do research, they need to position their team in this multi-dimensional space and it's maybe not, you know, one size fits all as far as the results go.
Speaker 2: Very interesting. Wow. So yeah, so I'm wondering about is is there a kind of classic or an extreme example of what's what's a a team that is high in teamness?
Speaker 1: Well, the example of dancing I think is the best one we came up with where you have, you know, folk dancing where you're all very coordinated and synchronized and then you have line dancing or just freestyle club dancing. And I think you would call the folk dancers, the dance troop a team and they're high degree of teamness. Line dancers, although they are sort of dependent on one another, they seem to have less teamness, basically trying not to knock each other down.
Speaker 2: I see. So the interdependencies is a big component.
Speaker 1: Yes.
Speaker 2: Gosh. And then I know there's kind of been a lot of controversy about humans and machines and are they teams? How are you thinking about teamness in that context? And maybe high in teamness?
Speaker 1: Yeah, I so I I think so and there are people who do who want to say that there is no way that a machine can be a teammate. And I think that boils down to trying to define team and teamness in purely human terms. So if you say that the only thing that could be a teammate is a human, then a machine can't be a teammate. But that's not the way I'm thinking about it because I think about it in terms of human animal teaming. We have some very good human animal teams in the area of military working dogs, the marine mammal program where dolphins are finding ordinance under the sea, they're working really closely with their human handlers and they're doing specific things really well and I definitely would call the dog or the dolphin a teammate. And along the same lines, I think you could call this machine a teammate, but however, it's a maybe a teammate of a different species and there may be lots of variability about how good that teammate is and so how much teamness it has maybe compared to other machines. So I say yes, the machines could be teammates just like animals could be teammates, but it doesn't mean that we're going to go have drinks with them after work or we're going to um communicate with them even in natural language.
Speaker 2: Yeah, that makes sense. And so it seems like part of the teamness has to do with the task and how much interdependence is required and part of it has to do with the actual components of that team.
Speaker 1: Yeah, I think it's highly task dependent. But also, you know, some people are better teammates than others more interactive and or they've been doing it longer so.
Speaker 2: And I suppose people it's a skill, come better team members.
Speaker 1: Yes. Yes, well we see it in the lab. We see that over time people will begin to understand what it is that they need to do, what information they need to give other people, what information other people need to give them and they'll develop this pattern and become much more effective efficient over time. Interestingly, in our RPA environment, our synthetic teammate did not do that. It was a synthetic teammate taking over the pilot role in a remotely piloted aircraft task and it would not unlike what people do, it would not anticipate the information that fellow human teammates needed. So it just sat back and waited to be asked for it. So, you know, waited to be asked for what is the current altitude. Humans don't do that. They eventually learn that I'm going to give this information to you now so you have it when you need it and we can process the target quickly. But the synthetic teammate didn't and the interesting thing to me was that eventually the humans started printing on the synthetic teammate and they stopped anticipating information needs. Everybody was just waiting to be asked and then we'll give it to you and that became really inefficient and poorly coordinated and as a result their performance suffered.
Speaker 2: That's very interesting.
Speaker 1: Yeah, so we think what was happening there is that the people were told uh and in many cases it was true they were interacting with an artificial intelligence agent or program. We did use Wizard of Oz in a few cases, but in this case, I don't think we did it and they believed that this artificial intelligent agent should better know how to do this task than they do because they just got there today and took their one hour of training and so they may have been had less confidence in their own skill and so relied on what this synthetic agent was doing.
Speaker 2: That makes sense. Yeah.
Speaker 1: But it kind of shows how one agent in a three agent team can change the behavior of the entire team. And it can go the other way too. You could have a really good synthetic teammate and they're and we've done that too and it actually improves team coordination, gets them up that learning curve more quickly.
Speaker 2: And so maybe that notion that people who are new to the team look to the more experienced members whether they're humans or machines. When I think about teams, like there are some kinds of teams where you always have new people rotating in, not maybe the whole team doesn't turn over but there's always some new people joining.
Speaker 1: Mhm.
Speaker 2: Anyway, that's just kind of resonating with.
Speaker 1: Yeah, there there are these ad hoc teams that kind of get thrown together a lot of times like a search and rescue people might be thrown together or even a code blue resuscitation in the hospital, people who are there come together.
Speaker 2: Yeah, so what are some of the big kinds of questions you're you're looking at right now in this in this research?
Speaker 1: Yeah, so well, we're working on several projects. One is um the Dpa project. Dpa Assist. We're we're working with Aptima as the evaluator of this huge program of organizations that are building artificial social intelligent agents and other organizations that are looking at theories of teamwork. But the whole project is about how can you make AI socially intelligent so that it can assist a team. And we're getting to the stage where we're going to roll out. We'll see if this works a huge experiment to collect data on hopefully thousands of teams uh in the context of a bomb disposal test that's meant to sort of be like a game, although I'm not sure how great of a game it is, but people will want to play and we'll have a competition, they'll win so much money if they're the highest scoring team by the end of the data collection. But it's all about trying to make this agent assist them in a way that's socially intelligent. And we always contrast it to Clippy, you know, who is not socially intelligent and kind of annoying and we don't want the agents to be annoying. We want them to be helpful. So that's that's one set of questions. We have other questions around explanation and a lot of them are still about later to knowledge elicitation. How do you measure like resilience on a team?
Speaker 2: Well, so what is what is a socially intelligent agent?
Speaker 1: So we we talk about theory of mind, so the agent should understand the intents of the team members, should understand their beliefs, understand what they might know or or not know about the task and be able to step in and give them advice when they need it, exactly when they need it and the kind of advice that they need. So a socially intelligent agent shouldn't say like turn right, turn left, that we call that joystick. A socially intelligent agent shouldn't just say, you need to communicate better or even nice job. A socially intelligent agent should say, I I think your teammate may need your help in this quadrant of the building.
Speaker 2: I see. I see. So are there good examples of this now or is this all just brand new?
Speaker 1: Uh, this is brand new with this project trying to make these socially intelligent agents. Although you might ask, well is is Alexa socially intelligent? Uh, probably not and maybe Alexa doesn't have to be for what Alexa is doing. But if you want to have agents and complex like military settings helping people, I think frankly, I think that a Centaur socially intelligent agent would work best and a Centaur means half human, half AI because in our experiments you could see that there's a lot of information that the artificial intelligence can get from the simulation from exactly what behaviors people are doing in this environment and when they did them, that the human can't track, especially when there's three people running around in a Minecraft environment. But the human can do other things that the agent can't. So I think the best solution would be to put AI together with a human to both assist the team.
Speaker 3: So that starting sound like Star Trek like the Borg but that's not what you mean.
Speaker 1: Yeah. But that you know this whole idea of Centaurs I'm pretty intrigued by it and it makes a lot of sense to me because AI is a completely different beast than a human. They have different strengths, different weaknesses and why not capitalize on the the strengths of both?
Speaker 2: But so is a Centaur a human and an AI working very close together with a lot of teamness? Is is that what that is?
Speaker 1: Yes, so and it started out uh the whole idea was Quintin and chess where a Centaur chess player, it was a pretty good human teamed with a pretty good AI beat the very good AI and and chess masters. I and I think there's more and more of this being done using Centaurs to play other games.
Speaker 3: I'm so tempted to ask about half human, half a lot of other things but um I will restrain myself.
Speaker 1: So
Speaker 3: So Nancy, sometimes we ask people about, you know, times in their career when they've advocated for particular perspectives and maybe kind of struggled to get traction. I I I kind of want to put a different spin on that. A question for you because um, you and uh Frank Durso published the stories of modern technology failures and cognitive engineering successes.
Speaker 1: Yes.
Speaker 3: Back in 2006, which effectively advocated for a particular perspective. I'm curious in particular in the context that we are as an association trying to gather some more of these success stories with, you know, particular evidence of improvements across applications of cognitive task analysis. And so in some ways, I I almost kind of look at our CTA and effect project as being a bit of an update frankly of uh of your book. So I'm I'm curious to hear sort of any stories you have about that particular book and and effects that you saw it it have.
Speaker 1: Yeah, I think what was most notable about that book after it came out for me at least was when, I'm pretty sure it was Don Norman was talking at a National Academy's meeting where we were celebrating so many years of both the Board on Human Systems integration and he brought up that book in association with, you know, human factors and human systems integration and said he liked the book. However, it was very telling that these success stories, they really weren't success stories. They were really stories of failure and the um human factors wasn't able to get out in front of it. So they kind of come along as Dave Woods would say sweeping up after the parade and we were kind of putting band-aids on things post talk. And that that was true. So you can find lots of examples of failures and I think we'll have lots more coming up even in every day we have them. And, you know, why is it can we find examples where human factors has gotten out there or cognitive engineering has gotten out there and fixed something before it went wrong. And those are hard to come up with because you, you know, it's like a non-event.
Speaker 2: Right. Yeah, you don't know that you have avoided a disaster because.
Speaker 1: Mhm.
Speaker 2: So let me let me switch gears about and talk a little bit about another aspect of your work. So you've done a lot of interesting uh work measuring cognition, which is notoriously difficult. How how do we know if we're really supporting people with our decision support tools? How do we know if they're making better decisions or paying attention to the right things? And I just wondered, are there a couple of insights given your experience in this space that that a lot of people don't appreciate when it comes to really trying to do this in a way that makes sense and has some relevance, real world relevance isn't isn't just about counting errors or or something very superficial.
Speaker 1: Yeah, so a couple of things I I find that it's I think easier to do on teams than individuals because individuals we have to either ask them or make inferences based on their behavior, errors that they make. But with teams, if you really believe that cognition is in the wild and it's in their interactions, you can definitely observe those. So it's it's a little bit easier um to measure. And in addition, I think a lot of measurement in cognitive psychology and human factors in general is kind of obtrusive and not done in real time. So you take somebody away from their task and you ask them to complete a a survey, which we're also doing in the Dpa study. And or you you have to put lots of electrodes all over people and measure, you know, EEG. But I think um there's a lot of ways in in the team tasks that we're doing at least that you can measure team cognition unobtrusively and in real time and that's by again observing the the interactions, the communication, the coordination that's occurring in in real time, not interrupting them. And there's a lot that can be said about the cognition that way as compared to the other way. And so we like to measure team situation awareness by how they respond to a particular perturbation for instance, and um by observing those interactions around that perturbation. I think it's you get getting a lot of traction, we're trying to now measure human machine trust, team resilience, proactive behavior in this way by focusing on the interactions. And I don't think that was something I covered in my varieties of knowledge elicitation techniques. I wasn't dealing with teams then at all.
Speaker 2: So, tell us I know a little bit about but maybe our our readers don't this lab you have built that allows you to do all this amazing work with um teams and and remotely piloted vehicles and, yeah, tell us a little bit about the setup you have.
Speaker 1: Yeah, I've been really lucky to get uh funding through, I think it's been now four different Durp grants. Durp stands for Defense University Research Instrumentation Program. If you have some DOD grants, you can apply for that funding to enhance your research by um buying more equipment. And so I I had two Durp grants that led to the remotely piloted air vehicle testbed that we have, call it the Cert lab. That's been really the Drofolo for us where we've conducted uh up to 13 different, you know, year-long studies in that testbed. There's other Cert labs around the country too that we can connect to. One's at the VA in um Seattle, then there's um one at Air Force Research Lab in Dayton. Uh there's one at Clemson and there was one at Georgia Tech with Jamie Corman, but Jamie Corman, I don't know if you know this as of uh last fall has moved to Arizona State. So now we have multiple cert labs, we could do distributed teaming studies in them. We have other test beds too. Our newest one's called the Ghost lab and ghost stands for General Operations by humans and systems as teams. Something like that. But it uh I was able to buy full-scale robots, a fetch robot, a Yumi, a husky robot, some stacker bots and turtle bots and we have now a facility, also with the help of Lance Garavi, an artist. It's also an art installation, full of lights and sound that we can make into an environment for studying human robot teams. So that was another Durp grant. We also have, thanks to uh the pandemic, a whole lot of virtual environments now. Primarily based on Minecraft. We have a environment, a Minecraft environment that does next generation combat vehicle, kinds of uh tasks where you have unmanned vehicles preceding the manned vehicle and looking at how all that interaction works. We use it in our Dpa study to do urban search and rescue tasks and bomb diffusal tasks. So we have lots of these rich environments and we're now thinking about time some of them together to do more complex experimentation.
Speaker 2: So you have this gosh, this whole network of synthetic environments. So I, you know, I think there are some folks who are always out in the field just trying to observe and learn what they can there and um there are some people that are very lab based and you're kind of in this middle space where you're trying to represent the key components of the real world in the synthetic environment.
Speaker 1: Yeah, yeah, it is a a middle ground where you can do semi-controlled experimentation, flat completely controlled because everybody doesn't experience things the same way, but it's it's not sterile like some lab experiments are. So you can still get at the complexity of the the system.
Speaker 2: Right. And so one I've never, you know, then built one of these synthetic environments. I mean, how do you figure out what needs to be of reasonable fidelity and what can you just kind of let go? Like we don't need to reproduce this part of the world, but this part's got to be just right for this research to to make sense.
Speaker 1: Right. So it depends on your research question. What questions do you want to ask in that environment? So for instance, with the RPA testbed, we looked at the cognitive task analysis um done at Creech. Um looked at what people normally do. We also looked at the interface to the predator, which is notoriously really bad. It takes it would take trained pilots about a year and a half to learn how to use the interface to the predator because it was designed by people who design nuclear power plants and didn't look at all like what they were used to. So we did not want to replicate that interface in our lab because we wanted students to be able to do this task and learn it within an hour or hour and a half. So we we change the interface uh so that we could preserve the kinds of tasks that they were doing, but it had a different look and feel than the actual predator. And we also wanted to preserve the kinds of interactions that they had as a team because that's what we were interested in, the kinds of roles on the team. So you have your pilot, your sensor operator and your mission planner. So we preserve those roles and the kinds of information that you would have on those roles and the kind of information that others would need. So that was the high fidelity part. The low fidelity part was the actual interface. And students were able to use our interface to do the task in an hour and a half.
Speaker 2: Yeah, so one of the things I admire about the work you do is is that you've really got to get that right, right? So you've got the right test bed and the right synthetic world before you can even do the work do the the the studies themselves which are hard enough to design and implement. So you've got to kind of create the world, then do the study. And then I imagine sometimes you tweak the world. Now now that we've done this study, we realized this piece of it needs to be better than it was.
Speaker 1: Yeah, it's a it's a very difficult design task. scenario design is I mean the test itself, the hardware, I rely on other people to do, but it's the actual design of the scenario that's so, so difficult and it has uh lots of moving parts, lots of simultaneous constraints and yeah, it's hard to to get it exactly right. And so we have to iterate again and again on the same design. Once we find something that works though like our RPA scenario seems to work really well, but others aren't working so well. So, yeah, it's it's the most difficult part. And I'm very lucky to have lots of uh creative and energetic students and lab partners and colleagues that have helped to do all of this.
Speaker 3: So we're we're talking about some synthetic environments. This is the naturalistic decision making podcast. I I wonder if you have sort of principles in mind where you feel like you've crossed certain thresholds and are no longer doing NDM. Does that make sense?
Speaker 1: Yeah, I think it's, you know, when you say decision making, that means a lot of things to a lot of different people. You know, it's a lot of kinds. Some decisions are are very big decisions that people make like whether to get married or not. And other decisions are like what button to press next on the keyboard. Those are all decisions of sort, but not the kind that NDM studies. I think, you know, NDM's more about being in the field and decisions that are made in the field. And so I think I'm closer, I'm more interested in those field decisions and not so much in the little mundane decisions and and our tasks sometimes have more decision making in them than others. Like the RPA task is kind of it's an action-oriented team, a team that uh just gets something done, has to coordinate and collaborate.
Speaker 3: great.
Speaker 1: We had another task it was a humanitarian relief task in which five people sat around a table basically communicating about making a plan and that involved I think more decision making. So yeah, just like team this, there's maybe an NDM this to the tasks that you do.
Speaker 3: Right. It sounds like when you think of NDM, you think of the content if you will, the the models as opposed to the methods. Is that fair to say first of all and then a bit of a more fun way to ask it is, so if you were to meet a complete stranger who says I I practice NDM and uh and you had to give him one question to determine if they do indeed practice NDM, what would you ask?
Speaker 1: Who is Gary Klein?
Speaker 3: I mean, that was maybe that's unfair question, maybe that's just my first exposure to NDM was through him. But I think, yeah, you're right. It does have to do with the methods as well as the content of the decision. So, but I would think that we are doing the methods that we're using that is synthetic task environments, I see that as a NDM kind of method. It's not in the field but it's still getting at a lot of the complexity of the field.
Speaker 2: Right. And so I feel like there's the methods and the models but there's also the way you frame the questions. That's why I think what you're doing is NDM is because the questions you're asking are about how do people do this in the real world and and and how can this synthetic environment help me help me understand that in a way that I couldn't in the real world.
Speaker 1: Right. How do you make machines do this in the real world?
Speaker 2: Yeah, yeah. Let's see. So we're kind of getting to the end so I'm going to move to some of the more um fun questions. So will you tell us one thing about you that the audience probably doesn't know?
Speaker 1: Oh boy. Well, maybe there's two things that they would know. One is that I used to be a powerlifter back in the day of doing my dissertation. I guess I needed to release a lot of energy some other way. And I also am a hot air balloonist, not a pilot. I'm a crew, but we have a hot air balloon that we actually just gave to our son-in-law and we fly hot air balloons every um October in Albuquerque and I actually have a new student who I basically I've been I've known her since she was born. Savannah Bradley, she's a new PhD student of mine and she's a hot air balloon pilot. So it's kind of like a a family event. So those are two things.
Speaker 2: Those are both very interesting things. So did you compete as a powerlifter?
Speaker 1: Yes, I did. Not too many times though because it got kind of weird and disgusting when in the women's class when there were people on steroids who are women.
Speaker 2: So you never got into the steroids?
Speaker 1: No, no. It's not really a good look.
Speaker 3: That should become a standard question in this podcast though for everybody else that we interview from here on out.
Speaker 1: What's that about? About steroids?
Speaker 3: Pro or or against?
Speaker 1: Yes, exactly.
Speaker 3: Okay.
Speaker 1: And the hot air balloons, that that sounds amazing. How did you get into that?
Speaker 1: Uh, through my husband, when I met him, he was coming back from a balloon rally and the rest is history. We even took the balloon to France when one of my daughters was just I think six months old and flew the balloon over there in a a balloon rally.
Speaker 2: Wow.
Speaker 1: Over the vineyards and farms. It was pretty amazing.
Speaker 2: It sounds amazing. Gosh. Okay, so I have one last question. If you could instantly become an expert in something or anything, anything you wanted, what would you choose?
Speaker 1: Hmm. I think I would like to become an expert right now in machine learning because a lot of what I'm doing revolves around machine learning and I I work with people who do it, but I need I feel like I need to have a better understanding of it to actually answer some of the questions that I need to answer. How do you keep machines from being biased?
Speaker 2: So I'm thinking a lot of people answer that question with something more like, I'd like to be an expert in hot air balloons, but you're already that. So.
Speaker 1: Well, you know, I've thought about maybe I want to be a pilot, hot air balloon pilot.
Speaker 2: Yeah, but the machine learning one that is true. You, there is a lot a lot there and I don't know that as a community like a lot of expertise exists yet. I mean, well, I not that there's not expertise but there's still a lot of questions. That issue of bias I don't know if we have great answers for that yet yet.
Speaker 1: Yeah, well, there's a lot of issues. So one that we've been dealing with lately at the university is chat GPT and uh, you know, a lot of uh universities are talking about outlawing it, but I don't think that's the approach and so we're instead talking about embracing chat GPT and teaching students how to use it. How to make your work kind of along the lines of the center model even better than it would have been without it and teaching instructors too, you know, what is it good for? What does it not do right? You can run some interesting tests on it and find that it is not perfect, but it's it's pretty good. But, you know, we introduced calculators a while back and we didn't have to take them away. And we told students when they could use one and when they couldn't.
Speaker 2: Yeah, I like that. I like that approach. Gosh. Well, thank you, Nancy for speaking with us today. This has been so fun. I've really really enjoyed it.
Speaker 1: Oh, you have great questions. Thank you.
Speaker 2: Yeah. So on that note, thank you all for joining us for the NDM podcast. I'm Laura Militello.
Speaker 3: And I'm Brian Moon. Learn more about Naturalistic Decision Making and where to find us by visiting naturalistic decisionmaking.org.
(Music)