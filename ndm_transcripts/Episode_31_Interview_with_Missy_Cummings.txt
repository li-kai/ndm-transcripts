Speaker 1: You know, like we talk about getting women into advanced positions, it's a struggle, but it's also a struggle to get people who are experts in naturalistic decision-making, human factors engineering, human systems engineering, that there's a whole another discussion we could have about education issues surrounding that. But until we get people in leadership positions that are mandating that these programs include these elements, then we're going to always be coming in to do the accident analysis instead of doing the proactive design to prevent it.
Speaker 2: The naturalistic decision making podcast with Brian Moon and Laura Millatello. This podcast series brings you interviews with leading NDM researchers who study and support people who make decisions under stress.
Speaker 3: Welcome to the Naturalistic Decision making podcast. This is Brian Moon from Perigeean Technologies.
Speaker 2: And I'm Laura Millatello from Applied Decision Science.
Speaker 3: We are very excited to welcome Missy Cummings to the podcast. Missy received her BS in mathematics from the US Naval Academy in 1988, her MS in space systems engineering from the Naval post graduate school in 1994, and her PhD in systems engineering from the University of Virginia in 2004. A naval officer and military pilot from 1988 through 1999, she was one of the Navy's first female fighter pilots. She is currently a professor in the Duke University Pratt School of Engineering, the Duke Institute of Brain Sciences, and as the director of the Human and autonomy Laboratory in Duke Robotics. Her research interests include human unmanned vehicle interaction, human autonomous system collaboration, human systems engineering, public policy implications of unmanned vehicles and the ethical and social impact of technology. So welcome Missy and thank you so much for taking time out to join us.
Speaker 1: Thanks for having me.
Speaker 3: All right. So uh let's let's walk back a bit. Uh as I just mentioned, you started your uh very illustrious career in the cockpit. Uh but I wonder if you could share for the listeners kind of how your path eventually brought you to human systems engineering.
Speaker 1: Well, it was definitely a circuitous path and not one that I think most people typically take um when they go down the human systems engineering path. But I was a pilot, I flew F4s and F18s and when I was in the military in the Navy specifically, especially when I was flying F18s, which are highly automated, at that time about one person I knew a month on average was dying, and they were dying because of the poor man machine interaction design. It could be anything from bad placement of buttons all the way to automation that was opaque and you couldn't understand it and mode confusion was rampant. Of course, I didn't know those terms then. All I knew is that people were dying in ways that they shouldn't be dying. And being one of the first female fighter pilots, it was also social upheaval. It was very clear the the men that I flew with for the most part were very bitter and angry that women were there and after flying hornets for about three years, I'd had enough of that and decided that I wanted to uh be in a more positive environment. So I decided to go back to school to get my PhD and it was at that time that I found out that you could get a PhD in human systems engineering uh at that time. Uh these, you know, were systems engineering departments and this just wasn't a possibility for me when I was getting my undergraduate and master's degree. So I just jumped right in and I think that my military background, it definitely gave me a leg up. You know, I have a great back story and great motivation and I was older and I think all of those things combined made me intensely focused. So I finished a PhD in three years and then transitioned MIT heard that uh I was coming on the scene. So I went over to MIT, was there for 10 years. I led the human and human and automation lab there. And uh then I became a mom unexpectedly at 40. I wasn't supposed to be able to have children. Uh turns out statisticians are wrong. probable doesn't not mean possible. Uh or improbable does not mean impossible. So then I would became a single mom, decided that life in Boston was too hard and uh Duke made me a fantastic offer to include paying me more and providing me better resources than MIT. So I made the jump to Duke where I then changed my name to the lab to the Human autonomy lab because I recognized at that point in time that there was a big shift with AI and so I wanted to update the name to reflect the fact that I was going to really put my stake in the ground for looking at humans and artificial intelligence collaboration. So that's kind of a long story to tell you how I got here and now my research really focuses on human interaction in complex systems with embedded artificial intelligence, which primarily is in drones, uh self-driving cars, driving assist systems, but also medical systems.
Speaker 3: Excellent. Yeah, that that there's so much to your story that um could make a podcast to each of the sections. I want I do want to get to um uh the uh human autonomous system piece. But I want to go back to so you you mentioned earlier, um, as you were a pilot yourself and you were seeing your your colleagues, um, as you said, some of them die because of uh of bad design. I'm wondering if you could color that scene for us a little bit. So, um, you know, what what sort of uh explanations were people offering uh and and and was it the case that those weren't sitting well with you because of your own experience, kind of flesh that out for us.
Speaker 1: So, the opinion I'm going to give you now is my opinion after having flown for not quite 10 years and then also now having been an academic for not quite 20 years, I think that looking back, the biggest problem with the systems that I was flying at the time, and this continues to be a problem today, is that the aircraft manufacturers, the Boeings, the Lockheed Martins, Northrope Gremans, when they decide to design new aircraft, they typically ask pilots who are retired recently left the military, for example. And so you when you go to these big companies, you find that they are populated with a lot of what we would call subject matter experts. So there's a lot of people who know what they did extremely well, but for the most part, they have no background in any kind of formal training, no understanding of other than being graded on situation awareness and they have a vague understanding of what situation awareness is, they've never really thought deeply about it. They've never thought about workload. They've probably never heard of mode confusion. And so I find that the design of aircraft by subject matter experts and this is true for manned aircraft and especially untrue for unmanned aircraft, that people don't realize that just because you are an expert in operating a system does not actually make you an expert in designing a system and I think it's a battle we're still fighting.
Speaker 3: Interesting. So, um, yes, so I wonder if we could dig into some of those critical issues that uh that that you think folks are getting wrong with with respect to uh, you know, today's designs and what what folks are getting wrong and what and what you see folks getting right?
Speaker 1: I'm really struggling to find what the right, you know, and this is a problem, right? And of course it's a problem because I think being an academic, we're just natural deconstructionist. I think things are improving. I think that people are understanding for unmanned aero vehicles that most people know of as drones, that people are starting to understand that more information may not be better, that the way and the amount of information that you present people needs to be modulated or you're going to overwhelm people. I mean, I say it's getting better, but I still have not seen really any transformational designs that that I would give an A to, you know, as an as a professor, I mean, I think everything's kind of in the C range. So it's not completely failing, but we could be doing better. And and in terms of manned aircraft, you know, I think that that there's still a it's very hard to get people to move away from what they have known for so long to understanding ways that you could design systems in the future, but largely especially in commercial aircraft, we still see problems driven by economics, right? And, you know, in in a perfect world, we would have all information grouped together, like information grouped together and, you know, lots of use of graphical tools, but sometimes text in the right place, but I think that still economics is driving a lot of system design, meaning that, you know, airlines don't want to have to go back and redesign interfaces uh, if it's going to cost them money in testing and certification. And I think the Boeing Boeing 737 Max Debacle is just a great illustration of how far we haven't come, right? So there's a whole new automated system put into an aircraft that the pilots didn't know about, that we weren't giving good pilots good visual feedback, forget auditory and any other kind of feedback and it certainly wasn't integrated into the systems. So when you see problems like the Boeing 737 Max crop up, you know, I I look around to my peers and say, you know what, we're going to have a job for a long time because we're still not getting the right lessons across to industry. And I think the military does a little bit better because they're more insistent on human systems engineering principle. As usual, the military leads, I think in some of these cutting edge areas, but it I still think that even that we have a long way to go because the lessons have not been learned by corporate America.
Speaker 3: Yeah, there's so much to unpack here. I very much enjoyed sort of working under your leadership during ourcus project, which was a Office Naval Research funded project and and I I felt from a research and design perspective that we had a lot of latitude there. I I felt very much that uh you were providing a lot of top cover there. Uh and it feels you know, I see you on Twitter and you're certainly not shy about expressing your perspectives. And so it feels like it feels like the the father there to do better, but but even uh what what you're suggesting with the economic uh factor being a strong pull on the commercial side that even with that uh even with the great work that's done, the things that we now know and um uh and can show how better design can work. Um, as you said, we're all going to have a job for a long time, but is there any, you know, where where are you seeing traction, um, that folks who are doing this kind of work can build on or are there different uh ways or audiences that we can present our work to, um, where these kinds of issues will will take hold, um, or or is it we're going to be banging our head against the wall for the next 10, 20, 30 years?
Speaker 1: Well, now I'm about to get deeply philosophical because, you know, I'm I'm a much older person than I was uh, you know, I'm 10 years older than I was when I led the autonomous aerial cargo utility system program out of the office of Naval Research. It was it was a great robotic helicopter program and it was the first time that I really took the lead. It was a $100 million project and it it it was and still is the Navy's most or in fact theD's most advanced operational autonomous system. Program won lots of awards. I'm super proud of it. You know, it was me leading it, but it was a whole team of people who were listening to me. And so I would tell people that the lesson to be learned from the success of Aus is it takes strong leadership and a willingness to engage with industry and government. I had a lot of high cover support and I had a lot of support in the trenches. And so I think that people in our field should say, how can we get better traction? The way we get that is to get people in leadership positions. And this is the hardest thing to do. So, you know, like we talk about getting women into advanced positions, it's a struggle, but it's also a struggle to get people who are experts in naturalistic decision making, human factors engineering, human systems engineering, that there's a whole another discussion we could have about education issues surrounding that. But until we get people in leadership positions that are mandating that these programs include these elements, then we're going to always be coming in to do the accident analysis instead of doing the proactive design to prevent it.
Speaker 3: Right. So, so you've been in these positions. How do we get folks in there?
Speaker 1: Well, we should start a whole new leadership training program, the Naturalistic Decision making leadership Academy or something like that, right? So, we do need this. This is how fields get ahead. We need to position people in places of power. It was great when Micah Ensley became the chief scientist of the Air Force. She potentially had a lot of influence there. I say potentially because in that position, uh it's a great position because you have no real responsibility and by that I mean you don't carry a checkbook. You you're just making suggestions. When I compare that to what I did as Aus that the head of Aus, I had a checkbook and when you have $100 million, it turns out people listen as opposed to somebody in a position that just, you know, is kind of making recommendations and guidelines, but, you know, you don't have any money to it. So what we really need to do as a group is start to get people to take these positions where they have influence and that means budget. I mean, it's a very cynical way to look at the world, but it's also, I think, if you want to make a difference, you have to put money where your mouth is. And so we need to get people in to beker program managers. We need more program managers. You know, I did this under an intergovernmental personnel Act agreement, which is, you know, I was on loan to the government. And and everyone can take advantage of that if you're a part of an academic institution, but even if you're with a company, you can go on a leave of absence and go to DC and take one of these positions. So this is critical, I think it because the government helps drive requirements. And so I do believe that you can have a lot of influence there. But the government isn't the only place. People need to get promoted internally inside companies. We need somebody with an Indian background to be a vice president at Boeing and Lockheed Martin. I'm just now, uh, I'm on the board of directors for a Fortune 500 company. Uh, it's Veneer. It's a joint Swedish US company. And I'll tell you what, being on a board of directors, that helps guide it. You know, I I'm not the most popular director because, um, I I I'm not the most popular director on the board, but I'm insanely popular with the company itself because I'm driving some of those changes because, you know, trying to make recommendations for how to make sure that we understand the right balance of autonomy and human interaction and be very proactive in defending the human. And so we've got to get more people with our background in power positions, boards of directors, vice presidents if not presidents of companies. So, you know, this is all pretty, you know, when you think about it, you're like, wow, that is a super big challenge and I don't know how to do that. Well, the way that we do it is that we have to grow our community, right? So it's it's a pyramid problem. You can't the bigger your your pyramid will get bigger, but you have to grow the base of the pyramid to do that. And so I think recruiting people and making sure that we're educating enough people with this kind of background is really the fundamental key to improving people's numbers in leadership positions.
Speaker 3: That sounds very important and and and a very um, a very steep hill to climb it. We we have a lot of founders. You know, I I back several NDM conferences ago, we we kind of did a roast of Gary and uh and, you know, we made a lot of jokes, but I also kind of made the point that, you know, 13 companies actually came out of by then the count was 13 companies came out of Klein Associates and so a lot of, you know, a lot of flowers bloomed out of uh uh out of what what Gary had there. And yet we're all still small businesses, uh, you know, sometimes we get to work with those larger companies, but but your point is uh is a bit more dramatic because it's, you know, we can provide the support and we can do the designs and and do the research, but I I think your point is super important that and until somebody is in that decision-making capability in these larger organizations, you know, we're always going to be helping out, uh, but it but it feels like, uh, it's it's not until you get into one of those C suite positions that uh and have a seat at the table that these kinds of things can actually firmly take hold.
Speaker 1: Yeah, and so I mean, Gary, we need Gary is awesome, you know, he we hold him up as, you know, the leader in the NDM field and it's great that he's had 13 small companies spawned, but we need, you know, a thousand Gary so that we can have 1300 small companies spawn and then out of those 1300 small companies eventually, you know, statistically, you know, a handful of those people that would then eventually bubble up to the top of leadership positions because there would be acquisitions and they would get to the place where they're making the right decisions. So, it's a numbers game and and I think that, you know, I we're all in the bottom of the boat rowing, but very few of us are on the, you know, top of the ship giving orders. And I think that, you know, this is why we need to think more strategically about if we can't grow those people from the ground up, for example, if we can't get people with cognitive systems engineering related degrees coming out of universities at at the undergraduate and graduate level, then we need to start getting much, much smarter about educating executives, educating, you know, winning people over. It's a hearts and minds issue. Every time I go go out and give a talk, you know, I I call it my come to Jesus meeting. You know, I'm really great at the come to Jesus meeting. I'm holding my um outreach sessions. I've got my traveling church going, you know, I'm winning hearts and minds. But I'm just one person and you know, I I I I'm having a hard time converting people at the rate that I think that we need to convert. And so what that means is we need to be much smarter about trying to get the word out in scalable ways that we can do more conversions.
Speaker 2: Yeah, so Missy, I mean, I love I I have actually never thought about this at this level. Um, and even you started out and saying we need to train leaders within the NDM community. I mean, that is part of it. I don't know that many of us are even comfortable talking to the leadership, let alone envisioning ourselves as the person giving orders on a ship. So, anyway, this is a really uh influential uh conversation we're having. It's it's making me think very differently. But I wanted to shift the conversation a little bit and uh kind of go back in time. So, when I first met you, you were starting to transition, um, more into your academic career, and you were really interested in NDM methods. So you had rolled up your sleeves and done a lot of work analysis. You'd come to Dayton to talk to Gary to learn more about cognitive task analysis, um, and and and were giving a series of talks at that time that were was really highlighting what you thought were some of the weaknesses in some of the NDM methods and what were strengths. And so I'm wondering, I know you still work with students and probably advise people, what kind of NDM methods are you finding most useful as as as you're thinking about human automation teaming and all those big issues?
Speaker 1: I think this is a a great question because I'm in a at Duke, I'm in the electrical and computer engineering department at MIT, I was in the Aeronautics and Astronautics department. So these are hardcore engineering departments. And this is part of my protizing to try to get people to understand the ways of of how you should think about humans, but particularly how to think about balancing humans and uh automation or autonomy. And so even though I am very, very deeply entrenched in the design of artificial intelligence and uh the building of systems that include AI and humans, every single project that I do that involves humans kicks off with some kind of cognitive task analysis. I really, really um hate the debate that happens between should you do a CTA or a cognitive work analysis. It seems to me like there is so much petty arguing about veracity of methods and and even myself, you know, I mean, I think in my early part of my career, I got sucked into that a little bit. But now that I've had this much broader perspective and I've worked across many, many um different projects and many different domains, I would tell you, look, we are just lucky if we can get anybody to do just some semblance of a CTA or a cognitive work analysis in a real domain and really think about those results and apply them going forward. So recently, my biggest project in this area and I have a paper coming out uh about this in the human factors and manufacturing journal edited by Neville Stanton, um, is showing Pfizer how to use cognitive task analysis to help with vaccine production. And I think it's it's you know, it's one of these just you would not believe the timing of this, but two years before COVID broke out, Pfizer came to me because they were having a problem in their research vaccine fermentation units. So these are the smaller versions of large scale vaccine fermentation units, you know, how we make real vaccines. They have to put them in smaller units first just to work out all the kinks with the vaccine to make sure it's coming out okay before it goes into full scale production and they were having a heck of a time balancing the human automation workload because the automation was failing in some key ways and causing some safety issues as well as efficiency issues. And Pfizer is here, you know, I'm at Duke, which is a big medical school. What's one of the reasons I came to Duke also was to be able to increase my medical footprint. So, the very first thing I told them is, okay, before we can help you with this, we have to come in and do a cognitive task analysis. And they're like, what is that? We don't even know what that is, we've never done that. So we send in teams of people, um, a couple different uh shifts to basically just observe and categorize and catalog and figure out what their work processes were so that we could under have an understanding of what exactly their problems were and who was responsible, who could help. And and and you will be able to read all about this in this upcoming journal article. And it led to an increased and a a brand new way of doing user initiated notification systems to help humans, um, effectively babysit the automation in a better way. Uh, you can go to my website and see a video on this. It also led to we're actually doing some research and the development of artificial intelligence vision algorithms. Uh, and so this one small cognitive task analysis led to a whole trickle of technology innovation and it was such a popular method with Pfizer, uh, that of course with COVID now everything then got expressed really fast. Now the project has been really sped up and now they want us to come in and do cognitive task analyses of um, related processes, work processes that are going along in parallel. And I've had similar response when I do these kinds of things for mining companies for example, when mining companies want to put in more automation, the very first thing we always do is go in and do some version of cognitive task work analysis to capture what is happening in the world. And so I look at the NDM methods and including things like critical event decision-making. We do these analyses all the time for almost every project I do. They are very much the core tools in my toolbox. They get used all the time. I do think that there's, you know, there are probably ways that we can think about doing research to improve these methodologies, but I think I would like to see the field kind of move away from arguing about what is the right method or um exactly what is the correct step and should you do an abstraction hierarchy, which I'm not a big fan of, but look, if you can get somebody to do an abstraction hierarchy, it means they're invested in it. So good for you, right? I think we need to move away from what is exactly the right method to how can we better develop a suite of methods that is uh more attractive to industry so that they can implement them and gain the benefits because every single company that I've done one of these for, as soon as I do it, they love it and they want to do more.
Speaker 2: So I'm I'm nodding along as you're talking. I I have had a similar trajectory where the differences between the methods have become less interesting and more interested in in in kind of grabbing the strengths of whatever is available for the project I'm working on right now. So, yeah, I I think I have a very similar perspective there.
Speaker 3: So so Missy, you you've given us sort of two views on a corporate environment. One the earlier one you spoke of which is they are engineers and then they are SMEs and it sounds like there's nobody in between uh and uh in this latest example, again, you have, you know, folks building processes uh and then folks who might use those processes and you were able to sort of step into that that gap between those two folks, two communities to help them, you know, translate and understand each other. So so back to the first example, why why don't we see more of of this sort of thing in the environments where they clearly have, they have plenty of people available to them who could tell them about their experiences uh and provide the data, but it doesn't seem to to make its way into the designs.
Speaker 1: So somebody told me once in the early part of my career that everyone thinks that they can do human factors because they're human. And, you know, and I still hear a lot, oh, it's just natural, it's just common sense, you know, well, if it were common sense, we wouldn't be killing people in Teslas and Boeing 737 Maxes and, you know, having issues with medical systems. So, it's not common sense. So I do think that there is first of all, an uphill battle in making people understand that this is a legitimate field, standalone field that people should consider spending resources on. And indeed, this is a big problem that I see with industry is, you know, they're for for companies that have not bought into it and I think this is a big problem with Silicon Valley startup cultures. You know, they're like, why should I pay for this? What does it do for me? And I think that getting people to have these dual backgrounds. So, I'm very fortunate that I have this SME background but also have the deep education component. You know, like like we need a thousand more Gary, we need a thousand more me where there's SMEs who also have the deep technical training.
Speaker 3: We're not going to get a thousand.
Speaker 1: Neither. I I'm not sure if the world could could um handle a thousand Missies and Gary all at the same time. Um, but what we can do and this is actually what I think is important for the survivability of this field. We must, must, must start integrating better in academic cultures. So we need to train engineers who have the dual background of I am very technical and can design a convolutional neural net, but I am also very competent and at least I understand what the problems are in human systems engineering and that this needs to be addressed by somebody with this expertise. So MIT hired me to do that. Uh and I stepped into a world at MIT where people already believed that. So I was training those engineers to come out with a dual understanding of the technical and the socio-technical elements. I was hired by Duke because the Dean at that time saw the importance of that and then wanted me to bring that level of expertise to Duke. It is a completely different podcast, um that and I think that this this is something that this community needs to admit and it or first of all just understand and then admit so that we can deal with it. Not everybody sees our field as legitimate. And when I say not everybody, uh depending on the school you're at, it can be not anybody. And, you know, I've done the conversion at Duke, it was a struggle, but people now understand that, you know, when you have if you're going to be designing safety critical systems that touch or impact the lives of humans either as users or bystanders, it's critical to have these um people in the loop. Uh there are some schools I think that that do this just fine. They typically tend to be state schools, the people that I think look down their noses at the NDM Human Systems engineering fields typically tend to be the elite universities. So I think this is something that we need to to work on because the real way to solve the problem of people not understanding how to incorporate these elements of design is because they're not exposed to it through their engineering programs. And so, um, you know, a few years ago, I proposed to the National Science Foundation to figure out a whole new education paradigm where engineers and social scientists would make diads going through uh some kind of program, a degree program together. And, you know, I'm everybody on the crowd. I'm speaking to my my church, you know, choir here. I know that you understand it. I know that you get it, but it was rejected by NSF and, you know, the reasons for the rejection were, you know, you know, even though we say we want interdisciplinary, it's hard to get people to fund interdisciplinary. And without the commitment to an interdisciplinary funding world, sets of collaborations, it's going to be really hard for us to get traction in this field. So I think that is also something, you know, and this is where the human factor Society should engage, but I think they failed is really moving forward on trying to get a collaborative education program moving for the future because if we don't do it, then we're going to keep moving towards irrelevancy.
Speaker 3: You're you're hitting home here on a number of levels for me. Actually, my son is about to graduate from your alma mater with a degree in mechanical engineering this spring and they've integrated uh a program, it's called STS, but it's basically, you know, sort of engineering and society and so, so kind of touching on the the dad idea that you're talking about, but I uh we had had a conversation and I I send him uh one of Dave Woods's videos that was very related to the conversation we have and he wrote back one sentence. He said, this is what we should be talking about in my STS class. So, um, so so it sort of feels like there's attempts made and um uh and folks are trying to push this, but getting it entrenched in the kinds of ways that you have feels like, um, yeah, we're we're not there yet. You've uh had a a lot of influence in the places you've been. I wonder if we could switch gears a few minutes and and just have you talking about the folks who have kind of influenced your thinking in these areas, you know, people who have influenced your approach throughout the years.
Speaker 1: Well, I think Gary has been a huge influence. You know, he's one of my favorite people and uh the thing I love about Gary is he just makes you feel great. You just have to be around him. He just makes you feel good and you can have conversations with Gary that you wouldn't be able to have with other people who don't have the same world views. So, I think Gary uh has been a huge influence on me. I would say another big person that's influenced me is a woman named Deb Johnson. She was one of my advisors at the University of Virginia. She is a philosopher and she really taught me how to think and I think that that is so important. I think it's something that a lot of PhDs if not most PhDs at all schools are missing uh who are coming out of engineering. This isn't about an algorithm, this isn't about designing a system and, you know, testing it to, you know, some runtime, some ridiculously low runtime or some high accuracy rate for some machine learning algorithm. You know, getting a PhD should be learning how to think, learning how to deconstruct systems, learning how to see a person's point of view and try to appreciate their point of view before you tear them apart. I would say, um, especially in human factors. This is a huge problem that we have. I think the reviews that are coming out of the journals, the human factors journals have turned into just a blood bath, uh, which is not helping young people. It's certainly and it's certainly not moving the field forward. So even in our own field, I think people are not learning how to think constructively and deconstructively uh to understand how to look at an issue from all angles. So she was very pivotal. Um, I think there and if I were to at MIT, I had a really, really uh I consider him a mentor today, a couple people, um, Rod Brooks. He was one of the co-inventors of the Roomba. Uh, he was the person that brought me into the computer science and artificial intelligence lab at MIT. That was also pivotal. And John Leonard who is an expert in autonomous vehicles. He's the one that really helped me see the light and helped me understand that these systems are deeply flawed in ways that really promote the need to do collaboration with people, yet somehow that message has been pretty much lost on all of Silicon Valley. So I do a lot of prosing out in Silicon Valley to try to get people to back away from that ledge. So, I've had a lot of really great influences, I would say on my thinking, the way that I see the world and think about the world. Uh, I would like that I I I think that in the cognitive systems engineering NDM communities that we need to promote more of a positive, inclusive, helping each other up mentality as opposed to tearing each other down. Um, I don't see that happening in the right direction right now.
Speaker 3: A leadership Academy might help those issues.
Speaker 1: I you know, I cannot emphasize. Of course, when somebody says leadership, you know, they it sounds like a cop out, but I do believe that, you know, we we need to have a come to Jesus meeting with ourselves. We should have some kind of summit where we look sit around and look at each other in the room and say, okay, where where do we want to go? Where have we been? What are the mistakes that we have made. I am firmly committed that that considering a human as a critical part of the system, you know, I will die on this hill. That is the most important hill to be on and I'm out there in the trenches every day trying to move very technical people off their algorithms hill over to the hill where humans are considered an important and potentially equivalent part of the system. But the NDM community, you know, Gary has provided so much great leadership, but Gary's not going to be here forever. And we need new Gary and we need more Gary and we need more people that are linking arms trying to push forward instead of trying to potentially one ups each other. So, I still think as a community, we have a long way to go and again another cop out to say, look, we can achieve more working together than we can alone. It's true and until we realize and I think this is an issue of admission, until we can admit that maybe we have not made the progress that we should have and then work hard to map out what that road would look like to make progress and and have the achievement and the impact that we want. Until we actually do that, you know, we're we're kind of adrift on the ocean. We're all in the bottom of the ship rowing, but we don't have a place to go to.
Speaker 3: So I I was going to ask you where you're taking where you're looking to take your research next but but I'm actually at this stage more interested in in where you're taking your protizing next and and your public policy work and your ethical and social impact work. I mean, what what kinds of directions do you see yourself pushing?
Speaker 1: Well, I'm uh I'm on my hamster wheel, uh despite COVID, in fact, COVID has allowed me to increase my protizing events. And so now you kind of see me in the virtual space everywhere protizing. Uh I just gave a talk. I'm not going to say where because I think they were actually unhappy. So a group asked me to give them a talk and they thought it was going to be me saying AI is wonderful and awesome and I basically showed them a picture a set of pictures and they were like, holy cow, we're all wrong. AI is going to kill us all unless we've listen to Missy and it was just it was an interesting, you know, you could actually tell because one of the researchers was like, wow, thanks for that sobering view of, you know, instead of the uh AI is going to, you know, save the day. And so, um, I think that uh I have to actually revisit that um because I don't want to I don't want to turn people away either, right? I don't want to scare people. So, I am trying to protize, but I'm not perfect and I'm trying to massage my message so that people hear me but also keep asking me to give talks. So, um, we'll see, you know, I I as as far as research goes, um, I am squarely moving into the world of assessing risk for autonomous systems where humans are make up some component of it. So, I have a lot of projects going on right now of where we do things like take real cars with real people and put them on real tracks. You know, I I've really upped my game and and that's one of the reasons I move to Duke because I have my resources at Duke are phenomenal compared to the resources that I had at MIT. So I have a great open Bay motion capture room. We've done we've done some what I consider groundbreaking work and how to assess how people are interacting with autonomous systems. Uh, it's kind of mathematically dense, so I'm not sure how many of my peers agree with me, but I do think that we've made some um big advances in this area, which can help bridge the gap between engineering and social science. Uh, but this thing about assessment, I'm just passionate about it because I I really cannot every time a Tesla has a crash and the driver dies and, you know, the driver was distracted and everybody wants to blame the driver. It just pains me because I know that these systems are being designed to really encourage bad driver behavior and there's so much more that could be done. And so, um, you know, I I do well at what I'm passionate about. So right now, I'm passionate about making sure companies understand that they're designing systems that are going to kill people. Uh, I, who knows where I'll be in five years? I I'm all I'm on many, many, many advisory boards. Too many. But I feel a responsibility to keep taking these advisory board invitations because again, that is a way to try to have the influence to make people, you know, move off their platform. If I could have any position in the country right now, um, I would love to be the National Science Foundation director. Unfortunately, it's a political appointee position and I have a I have a big mouth. So don't get your hopes up. There's no chance that I will ever get into a political appointee position because I do have a big mouth. So, you know, I'm constantly trying to work at one, two levels underneath and sit on these advisory boards to to try to influence those people who do have those kinds of decision-making capabilities that can potentially start defining programs where not only this community would benefit, but society would benefit because people are taking the human seriously.
Speaker 2: So, Missy, I want to switch gears and ask you kind of a fun hypothetical question.
Speaker 1: Okay.
Speaker 2: Imagine you meet a complete stranger who claims to practice human systems engineering. On pain of death, you're given one question to determine if they do indeed practice HSE. What would you ask them?
Speaker 1: So, this question is, it's interesting because you're asking me the day after I had what I consider to be a pivotal moment in my career. So I mean, at the time he is good or bad depending on how you think of it. Yesterday I was in a meeting of high untiemps where it's a there's this venture capital firm, they bring together all the thought leaders, um, around robotics and AI together and we basically we we do this once a year and we try to hammer out a path for the future. And I'm the only voice, you know, the human advocate. Well, I'm not actually, there are two of us. There's two of us on this committee that that try to make recommendations and there was a person in this virtual meeting. I I'm obviously not going to tell you who this is, but it's a person of great influence uh uh in in this world. And this person said that he believes artificial intelligence is pivotal for advancing the sentient of humans, something along those lines. And and my jaw dropped because it's not artificial intelligence is not going to do that. Artificial intelligence is a marginally legitimate mathematical approach to making statistical associations. It's not knowledge, it's not what Ray Kerrwell says it is. And anybody who is in in the AI world knows this. So the fact that this person who I really respected, I thought the world of this person, said something that I thought was so ridiculous, so buying the drinking the Jim Jones Koolaid that I I I'm actually at that point in my life where I'm like, I cannot believe this person who I thought the world of said something truly idiotic. And so, so you asked me that today and so my new test of whether or not you're an idiot is to tell me whether or not you think we can exceed human reasoning with artificial intelligence. And if your answer is yes, you're an idiot.
Speaker 3: We're going to have to revise that question.
Speaker 1: Well, I know it's pretty strong, but I have to tell you it's like, I am as a researcher, you know, and I'm I'm I'm at the curmugen stage of my career. I'm sure Gary would love this too because he would say the same thing. Like, look, I appreciate hype and I appreciate that hype can sometimes be good and you need it to drive research grants, you need it to get students, you need it to keep society engaged. So I'm all for some hype, but then there becomes a point where you over exceed hype because then you can potentially cause safety issues by your over hype. And if you know anything, if you work in the field of artificial intelligence at all for real, meaning you develop real algorithms, you really try to develop systems to try to make some kind of knowledge-based decisions, then you know that this is never going to happen. And the fact that, you know, the fact that people, that's what really bothers me, is that people who are smart enough to know better, don't know better. And when somebody says something like that in a closed room in front of other peers, I mean, it just means to me that they really believe it. And I really worry that somehow, you know, I've made a lot of religious references here, but it's a problem. If we're producing people now that fervently see AI in as a way that we're somehow going to surpass human reasoning and judgment, wew, we got that's what I'm saying. We're going to stay employed for a long time because these people are going to keep building systems that kill other people.
Speaker 3: So so looking in the other direction, uh so so that's what concerned you about your peers and and folks in in these kinds of positions. I'm wondering when you look at the students you work with, are you seeing a lot of Koolaid uh being taken in?
Speaker 1: Yeah, I think it's a great, it's a great question because I think I think the better question for students is, are we getting students that come to these programs because they're hearing or drinking some of the Koolaid? And I would say yes. That and that's not bad actually. I mean the fact that that we're seeing enrollments increase in machine learning and artificial intelligence. I think that's great because anytime we see enrollments increase anywhere in academia, people in theory want to learn. So I'm going to advocate for learning. Yes, yes, learning is a good thing. Uh, I think that most people who actually once they get into it, they're shocked, shocked and appalled. I have a whole set of projects where I can't I keep students keep quitting on me. One of the things I'm doing in research right now is I'm trying to develop simulations for safety so that if we're building systems with humans and autonomous vehicles inside the systems that we can actually develop simulations that would tell us, you know, maybe where some of these edge and boundary cases are so that we can prevent accidents and deaths. To do that, you have to have a ton of data to to to basically develop some of the artificial intelligence enabled sub systems underneath and to have a ton of data, it means you have to clean a lot of the data. And indeed, almost all of any kind of neural net AI is cleaning data. Like it's 95% cleaning data, 5% running the algorithm. And the students hate it. They keep quitting. They keep quitting because they're like, I didn't I didn't come into AI just to spend all this time cleaning data. And I have to tell you like it's myself and everyone else we're like, I got news for you. That's all it's ever going to be. And so we see people who have dreams of AI and and the and I also end up having to let a lot of students go because what they want to do is to do AI and consciousness. They don't actually want to do real AI, they want to somehow approximate consciousness with AI. This is Ray Kerrwell. We're not going to get there. We're not even close to getting there. So, there is a problem where expectations for students because of the Koolaid is a little different. I would tell you though that you can be sure that if I graduate a student, it means that they understand the real the reality of AI.
Speaker 3: So you might have some lessons for us for our leadership Academy. We're going to be putting together after this podcast.
Speaker 1: If only we could get, you know, $10 million dollars to develop this leadership Academy, I I would love to lead it.
Speaker 2: I think we could do it for half that.
Speaker 1: But you never know, you know, if we could get the right funding and the right advocacy in place, then, you know, I am very serious when I say this, we need to have our own internal come to Jesus meeting or we're going to continue to suffer.
Speaker 3: We are going to go out on a fun question so we can get past our uh our suffering. We have uh we want to hear from you two truths and one lie and we're going to try to guess the lie.
Speaker 1: about yourself.
Speaker 3: Ah.
Speaker 1: I love to play first-person shooter games. I love to box and I love snowboarding.
Speaker 2: Laura, you have to go first this time. Okay, I don't think you like to play first-person shooter games.
Speaker 3: And I don't think if you love snowboarding you would have chosen to live in North Carolina.
Speaker 1: Aha, you're both wrong. First of all, we actually surprisingly have very big mountains in North Carolina and we see all the time in North Carolina. Um, I love first-person shooter games. I'm actually COVID has I'm I'm part of a 13-year-old club and uh I just game with these 13-year-olds all the time. They're way better than I am, but I love them. I really I cannot stand boxing because I know this is this is why I said because I really do not I think that boxing is barbaric that we should never hit each other in the face. I know that sounds kind of rich coming from a person who would have dropped bombs from you remotely. Uh, but you know, warfare is very different than personal violence. And so I just support personal violence. So, um, I'm glad you guys fell for it.
Speaker 3: They all seemed possible. All right, I got one more fun question and and we can take this out if if you don't feel like answering it, but what did you think of the Captain Marvel movie?
Speaker 1: Which Captain Marvel movie?
Speaker 3: The the Captain Marvel movie about Marvel Comics Captain Marvel was a Air Force pilot turned superhero. Did you see?
Speaker 1: Oh, I don't I didn't see that one. I'm not even sure. So, you know, my movies are I have a 13-year-old daughter. So I'm only watching what my 13-year-old daughter wants to watch and she don't want to watch that.
Speaker 3: Fair enough. I figured it would have been on your radar. It's about uh, yeah, it's about an Air Force fighter pilot who turns into a superhero, which seems very befitting given this conversation you've just had because it sounds like your life story except for the aliens and the and the superpowers, but um,
Speaker 1: Oh, well, well, now maybe I'll I'll get it back on the radar. Um, uh, but yeah, this is the thing about being a single mom is my world of TV is very narrow and it's whatever she defines it as. Or occasionally, I will slip in an episode of Black Mirror.
Speaker 3: Nice. On that note, we will thank you so much. This has been um, not only super interesting but I I think very inspirational for the community. I think you've given us a lot to think about and um, and a lot to act on. Um, and we'll probably be following up at some point, um, for a couple of the ideas. I I think um, having someone like you engaged in the conversation, uh, can can only benefit uh the conversation and everybody who partakes in it. So thank you so much for for taking some time today.
Speaker 1: Well, I hope I'm I'm sure it's more off script than you thought it was going to be, but uh, I hope it was useful.
Speaker 3: I fully anticipated it would be off script. I am a I am a huge fan of your big mouth.
Speaker 1: Well, the you're in a small group, I think.
Speaker 3: On that note, thank you for joining us, uh, for the NDM podcast. I'm Brian Moon.
Speaker 2: And I'm Laura Millatello. Learn more about naturalistic decision making and where to follow us by visiting naturalisticdecisionmaking.org.