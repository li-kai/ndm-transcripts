(Music)
(Music ends)
Speaker 1: The Naturalistic Decision Making podcast with Brian Moon and Laura Militello. This podcast series brings you interviews with leading NDM researchers who study and support people who make decisions under stress.
Speaker 2: Welcome to the Naturalistic Decision Making podcast. This is Laura Militello from Applied Decision Science.
Speaker 3: And I'm Brian Moon from Paragean Technologies.
Speaker 2: Today, we're very pleased to welcome Micah Endsley. Dr. Endsley is widely recognized as a pioneer and world leader in the study and application of situation awareness and advanced systems. She is a former president of the Human Factors and ergonomics Society and former chief scientist for the US Air Force. Dr. Endsley is the author of over 200 scientific articles and reports on situation awareness, decision-making and human system integration. She is widely cited in professional journals. She co-authored the book designing for situation awareness and speaks extensively at conferences. Dr. Ensley has a PhD in industrial and systems engineering from the University of Southern California. She is a certified professional ergonomist. Welcome, Micah. Thank you for joining us today.
Speaker 4: Thanks Laura. I'm happy to be here.
Speaker 2: Great. Well, I love to hear stories about how people got started. So the first question I have for you is do you remember the first paper you ever published and can you tell us about that?
Speaker 4: I definitely do. It was a a paper that was in the Human factors and ergonomics Society annual meeting and it was uh an out outcome of my master's work and it was on the effective automation on people. And what I was really looking at was how do you implement technological change and trying to overcome resistance to technological change. So at the beginning, I was much more involved in the organizational uh design and management section of the society. And uh the uh the work that I did, we studied the automation of clean rooms at a company that made telecom equipment in the Midwest. And that's where I really first started getting interested in automation and decision-making and how automation affects people.
Speaker 2: Interesting. So it sounds like when you say um resistance to technological changes, um it it's it sounds to me a little bit like you've had a perspective shift like like initially the focus was on why people why won't people just use the technology and now you're all about how are we going to build technology that really supports the humans? Is that a fair?
Speaker 4: I I think that is fair. Um and you know, initially you start off with, you know, why won't they use this? This is this is wonderful stuff. And when you get into studying it and you realize really there were a lot of barriers to to people using the equipment. It wasn't this adage the old adage, you can't teach an old dog new tricks, that turned out to be not true at all. And that many people wanted to embrace the technology but but they were they were simple things that got in the way of them uh being able to use it properly. And so that really led me down a whole another path.
Speaker 2: Nice. Yeah. Yeah, it's so interesting to hear where people started and then how they where they wound up. And so now you, um, you know, you've done this research on situation awareness that has influenced the aviation, the healthcare communities among many others. And I am wondering if there are one or two insights about SA um that most people don't appreciate that you you'd highlight.
Speaker 4: Well, I I think a lot of it is is fairly well known. I mean, situation awareness is fundamental to decision-making. Pilots tend to intuitively understand this. They were at the the front line when I started studying this, uh, in the 80s. Pilots were the group who really talked about situation awareness and I was looking to figure out well, what really is situation awareness because I thought at the time that it might be part of the problem with this out of the loop uh problem that we saw with with automation and I was working with our AI group at Northrop Air Craft uh, who were who were developing pilot Associate. And at the time I needed a dissertation and so I I um, started looking very closely at this this whole issue of of the out of loop problem and I had to try and figure out what situation awareness was. Pilots sort of intuitively understood this to me they were really sort of like the Canary in the coal mine in terms of uh dealing with difficult systems, complex systems, overload, fatigue, having to uh, figure out how to deal with this to gather and integrate and understand situation awareness to make their decisions. So they were a wonderful group to to begin studying. And since then we've seen the same problem spread to to other environments as they've been flooded with the same kinds of problems, sensors, lots of technologies, automation that sort of have affected everyone. Um, you you asked me about, you know, what are what are some things people may or may not appreciate? David Meister once said to me, you know, isn't it all about attention? And and I had to say yes, it was but but it's also about more than that. It's it's about how people integrate and interpret that information. And so the role of mental models is huge in terms of influencing not just what people pay attention to, but also how they interpret that information. And we see the effects of this in things like anchoring and confirmation bias. it it it it's really looking at how people have to process that information that they perceive. And this happens not just in the world of pilots or physicians but it's happening every day in how people gather and interpret what's going on in public affairs. So what what we see in the news today where some people what what some people call fake news is is is the actual news or vice versa. They don't know how to how to interpret the information that they're perceiving because they're all viewing it from this very different lens and very different mental model. So it's it's it's fascinating to me from that perspective.
Speaker 2: Yeah, so this is this is interesting. I'm glad you raised this. Um so you have actually I've heard you give talks recently about how people are everyday people are processing this information in the news. Um and um and and so this link uh from uh you know, uh essays started out focusing on pilots and this, you know, very dynamic um life and death situation. And and now you're thinking about it in this in this um very different context. Um I guess I'm I'm not sure what my question is exactly. It's just that this is really an interesting space. Um yeah.
Speaker 4: It has been a very interesting space. Um, what what when I really started looking into this it it was it was quite dismaing because I've been studying ex expertise and situation awareness of experts for a very long time and we we're just typically dealing with people like physicians or pilots or air traffic controllers or military commanders, people who have a very strong vested interest in making sure the data they're getting is correct and that they're interpreting it correctly and and to make good decisions because they know lives depend on that. And they may not always be successful but at least they're trying very hard to make sure that they're as objective and accurate as possible. And then I look over here in this other space, the space that we're surrounded with every day, uh, how people are processing all of the news and information that they they receive and it could be from websites or traditional news sources or their neighbors. And I don't see the same kinds of processes being put into place where whereas I I and I don't think they realize that they're not being as objective about the data as they could be but I see them falling prey to a lot of the kinds of decision biases that Kaneman Tversky warned us about many many years ago. I'm seeing that much more in this space, uh where people uh don't seem to be alert to the the dangers of misinformation or they don't seem to have as good a of of facilities for being able to distinguish accurate from inaccurate information. So they fall prey much more readily to these kinds of decision biases.
Speaker 2: So I read something that um drew this um analog or or a metaphor to to like rooting for a sports team and they they said some people have become so entrenched in a perspective that they just they're looking for things that confirm that. Um so you want your team to win and and if if your team believes this thing then you're just looking for more evidence that supports that um and you're not being objective. Um does that resonate with you at all?
Speaker 4: Absolutely. It's the classic anchoring and then confirmation bias in terms of what information you look for and what information you um will discount. Uh, much of the information is available to say that this world view is incorrect but they they discount any information that conflicts with that. So so so there's a lot of cognitive dissonance that goes on a lot of confirmation bias. And the whole challenge of how do we break people out of that confirmation bias is really difficult. Um, the research that that people have done on this so far shows it to be a a very complex and difficult problem that people can get more and more entrenched and I think the issue is that their motivations are quite different. They're motivated to stick with that world view that they're happy with. They're not motivated to um deal with an objective reality the way that, um, the typical experts that we study are. And I think what I've come to recognize is that there's a whole social, emotional kind of thing going on in this this space and it's not strictly cognitive.
Speaker 2: Yeah, yeah, that really resonates with me too. I I have this sense that there's a yeah, social, emotional, cultural, like this is what my family believes or this is the kind of people we are or, you know, there's there's something more than just I want to understand the situation.
Speaker 4: they seem to be uh um dealing with it much more the way they would deal with a religion than they would with a with with with then with purely an information space.
Speaker 2: Yeah. Very interesting.
Speaker 4: So it's it's it's it's a fascinating it's a fascinating problem. And uh but I do think there are things that we can do to address it uh to help people better recognize the information. If you if you look at what's going on in the world today, people get bombarded with all kinds of conflicting data. It's largely verbal. Um, and they don't they don't know how to distinguish good from bad other than saying I'm going to believe the people that uh believe like I do, I have more faith in them. So their ability to sort out accurate from inaccurate information sources is really largely tied to that mental model of what they believe in the first place. Um, it's it's one way of dealing with conflicting information, but it doesn't necessarily, um, help to detect when you've got a faulty mental model. So what I've been looking at is what are ways to break people out of that to help them better understand information and being able to check the veracity of their information sources that that may be an avenue to help people uh recognize this. There there's also some really interesting research in how do we encourage people to be more open-minded? You know, some people are very closed-minded, this is what they believe, they're never going to change, uh and that influences then how they perceive information, but there is a a characteristic trait of open-mindedness and if we can help people to be more open-minded, they may be more accurate in how they process that information which which really is important to how people function in their role as citizens in in this country.
Speaker 2: So as you are learning about this, are the kinds of solutions, the kinds of things that help people break out or be more open-minded, are they um things that a person has to want to do? I mean, do you have to persuade someone to try to to to to take these things seriously?
Speaker 4: I think you do and and the literature shows that there are some people, quite frankly, you're never going to reach. They're always going to be very closed-minded and they're never going to change, but there is a persuadable set of people who I think probably are very open to learning into other viewpoints, um, but are just simply unaware of the limitations of of what's going on in their own processing of information. So I think there is a group of people that that can be uh reached or influenced in terms of better understanding the information that they're they're processing in in the world.
Speaker 2: And are the solutions about how we present information to them?
Speaker 4: I think that's one set of of solutions. It's how we how we present information to them, how we help people process and understand the reliability or or veracity of information. Uh, I think there is a lot of group dynamics that come into play though. So there's a lot of room for social psychology. Uh there's a lot of of issues with this whole open-mindedness in terms of how people process information. This is really a a wide open area. I I'd like to say, oh, we have all the answers here, but what's become obvious to me is we definitely don't have the answers. And uh, I think it's it's would be a great area for for anybody looking for a a doctoral dissertation or a master's thesis, there's a lot of really good uh research that's needed in this area.
Speaker 2: I agree and and part of what makes it um so studiable right now is that people are posting viewpoints publicly on on Twitter or Facebook or um so there's lots of information that already exists that's publicly available that can help you start to see how how different folks um are processing and what they take as credible and what they don't.
Speaker 4: Oh, yes, it's a it's a very easy field to to reach out. It's the people all around you so it's easy to gather data in the area. Um if you have a good model for how to approach uh collecting and and studying the problem set and maybe what some of those interventions might be. Um, I I would love to see some more work in this area.
Speaker 3: Micah, you mentioned both uh religion and social psychology. There's a classic text in social psychology called when prophecy fails. Uh and it is a it's a naturalistic study. Uh the methods actually are a little controversial because the uh the uh researchers didn't tell uh the participants in the study that they were actually studying them. But it's about uh a UFO, a small UFO religion. Uh, and they essentially laid out what happens when this, um, leader of this uh, small group uh puts forth a prophecy and says on such and such a date, this, that or the other things's going to happen and of course the time comes, it passes and so it's the book's really about what happens after. Right? And so so what what what can we learn about uh, how people will believe in something, uh, when it when it doesn't happen or when it goes wrong. So, um, it's kind of it's an interesting application of of naturalistic methods that uh are, like I said, a little controversial but it's a great little book and it's uh it talks about a lot of the things you're discussing.
Speaker 4: Yeah, that's that's like that's a classic study and that was I think where the term cognitive dissonance first came came from was when the UFOs didn't come, then they could explain that away by something that fit their model. That said, oh, they they they they did the the aliens didn't come because we had uh been so good in um praying for this problem to go away or something. So they they can they can explain everything away to fit that mental model. And we've seen this in in in uh expert decision makers, we see that we've seen this in physicians who continue to explain away symptoms that don't fit the original diagnosis. Uh, we've seen this in uh, uh pilots who will mismatch information that's in the environment to the wrong mental model. Uh, so it's it's a it's a common way in which we process information. And the real question is how do you break people out of that bubble?
Speaker 2: Hmm.
Speaker 4: Uh, one of my graduate students Deborah Jones did a did a whole study on this with air traffic controllers where we intentionally introduced a an incorrect mental model. This was in a simulation environment, not not not the real world in a simulation environment. And she threw all kinds of cues at them to to see what kind of cues would be most likely to help them catch the fact that they'd gotten the wrong piece of data at the beginning. And some were things that happened that shouldn't have happened if that was correct or things that that that did happen that you know, or didn't happen that should have happened and then there were subtle cues and then there were completely obvious things that just complete mismatch. And what I thought was really interesting about the study was that she found that even with the most obvious bizarre cues in 60% of the cases the air traffic controllers never never noticed or never corrected the problem. So the vast majority of time the problem went uncorrected. And and when she went back to question them after the fact, um, it turned out that many of them could say, oh yeah, I saw that but they would explain it away. So that my my favorite example was a a plane that was uh was a 7 737 or 747 at that time, uh going cross country at 10,000 feet. Well, 747s do not do not go cross country at 10,000 feet. That should have been an obvious queue that something was wrong there that they should have of questioned. And and the response from the controller was, oh yeah, I saw that but I just figured it was carrying the space shuttle.
Speaker 2: Wow.
Speaker 4: So it's like where did this come from? They they they'll they'll find some some uh reason out there that they can find to make the data fit what what what's there. And that's that's how difficult it is to break people out of these uh these these bubbles, these information bubbles. And and I worry about that when we see this problem we take it to the to the real world issues that we have right now with with fake news, they don't, people just don't take a step back and say is this accurate? Is this not accurate? How does this match with with with reality? They just keep getting sucked into things that will explain the data to to fit what they want to fit what they what they want to believe and rejecting all the external data. So we we have we have a perfect storm now that the internet was supposed to be a an information we're supposed to be in the information age and I think we're in the misinformation age as we've created all these unique misinformation bubbles and that to me is is a huge challenge.
Speaker 3: Right. Um, so that's a challenge we should be focused on now. I'm wondering if you can sort of reflect back on your career and just, are there any particular projects, any one project that stands out as particularly fulfilling for you?
Speaker 4: You know, there were so many projects that I was involved in, um, in the early days, I was involved in the cockpit automation technology program and the pilot associate program and those I think really were foundational to both the problems that I work on and to really understanding how human factors uh fits into the design process in industry. Um, I spent um nine years working with the army's advanced decision making uh collaborative technology Alliance with with some of you. And that was a wonderful time to be studying situation awareness and decision-making in teams in in the military, in very challenging situations that then rolled into uh our company's work on the Future Combat Systems Brigade combat team modernization program where we really got to put into practice all of these cognitive engineering tools and SA earn design tools that we had been working on, we were able to put it into practice for complex command and control systems. So to me those were were really uh gratifying experiences. We had a wonderful team at SA Technologies that was heavily involved in all of these programs and and that really made it fun.
Speaker 3: I noticed you didn't include your uh chief scientist role in there. Is that did you consider that um uh sort of outside of your your research work or is it part of what you think of as as projects?
Speaker 4: That was a, you know, that was a whole unique experience that was just sort of it was kind of existential. It was it was it was really very different than my role as a researcher. Um, you know, my my experience serving as the Air Force Chief scientist was was really very positive. Um, I went in there thinking that, you know, this is going to be very bureaucratic, there'll be a lot of entrenched thinking and people would be resistant to change. And what I found was completely the opposite. Um, people, the leadership was very open to new ideas. They were very actively seeking innovation. um, and and they were really open to what I would talk about in terms of human factors. They just largely didn't know a lot about it. Uh, and what I really discovered in my time there was that the kinds of problems that we would see in a particular system, whether it be a a display or a cockpit or what have you, the kinds of human factors problems that we would all pick up on right away were largely invisible to them. They they didn't just just weren't at tune to, they didn't have the same filter to to pick up and understand what the real challenges of these systems were for human performance or how those could have been solved in in the development process. So, uh, I've learned that we have a uh, uh, not just an educational role. I think we we talk a lot about the importance of what we do and the and people are very receptive to that. They nod their heads, but then they don't see where those problems are in systems. So it's it's a it's a practical gap. And uh what I learned from that is that we have to, um, get better at not just talking about what we do as a practical matter, but but trying to come up with ways, tools that allow them to understand where we fit in that organization and how to make those kinds of organizational changes that would uh prevent problems in the future.
Speaker 2: Interesting. So I'm going to um, I'm going to just follow on to that a little bit. So the chief scientist role is one example, um and another one I'm thinking about is is you started, you you were one of the folks who helped start the um journal for cognitive engineering and decision-making, which was really intended and is a a journal that um is pretty innovative. It it publishes um really important work that maybe doesn't fit in other journals. And so I think of you as someone who has kind of been at the forefront of some important movements, who's who's been uh someone who's who's kind of stepping up and introducing ideas based on what you've studied and what you know to folks who aren't used to thinking that way. And I just I wonder, do you have any insights as you you reflect on on those situations where you've kind of been standing up and talking to people who just aren't familiar with your background and making the case for them to think in a new way.
Speaker 4: Well, as I said there's I find that a lot of people are very receptive, they're open and receptive to the messages that that we provide and they're receptive to our ideas, but they don't they say, oh yes, that's interesting and they go back to doing work the way they've always done it. And what we have to do is to work harder to find ways to fit what we what we do into the organizational models and systems that are in place. Um, and that's that's difficult. Um, so for example, one of the things I I discovered was I I'll go back to this idea that human factors problems are invisible to them. They they they'll agree with us in in theory but then in the practical application, they don't they don't see what those issues are until of course, it's much too late. So one of the things that I decided to to focus on while I was Chief scientist was the idea of human readiness levels. In the military, NASA, DOE, uh a lot of our large organizations, they they all talk about technology readiness levels and I found that the leadership completely understood if you said something was a technology readiness level of six or five or or seven, they intuitively knew how ready that technology was and what it meant for uh including it in funding cycles and where programs had trouble and needed more management attention, a very simple one to nine number. And I thought, well, what we need is is to have a one to nine number for human readiness levels. And that is whether or not all the detailed human factor stuff that that we do had been applied to a given system. So, that was something I started talking about, um, it was actually an idea developed by Hector Acosta many years ago and there was some good work at the Naval post graduate school on it. Um, but I decided to let's take that idea and let's let's see if we can't implement that. And that's something that we're we're working on today. We actually have a Ansi uh standard effort going on to formalize these uh human readiness levels. Uh and I think that could be very exciting. This could be one of those leverage points, a tool that will allow non- practitioners of our field to at least understand at a basic level that, oh, this only this system is only at a two. It really needs a whole lot more human factors attention to get it up to a seven or a or a 10. Um, if they can just understand it at that level, I think those are the kinds of tools that might allow us to be more effective.
Speaker 2: That's yeah, that that feels like a great insight. I think um, one of the struggles we have in our field is um talking to uh not just talking to ourselves finding ways to communicate, uh our insights and our findings, um and our methods in ways that other folks that that that that really resonate with other folks that become usable to them.
Speaker 4: Yes, it's kind of like the old adage physician heal thyself. Um, we're not good communicators of what we do. And and I think we've always struggled with, you know, what's our elevator speech about, uh, human factors and what is our how how do we explain to people what we do and and I I think we can't do that without getting into these whole long complex discussions and by that time people have, you know, moved on that's they're not that interested. They don't want to hear the nitty-gritty details about our processes. So we have to find ways of communicating it in a much simpler, easier way.
Speaker 2: Yeah. Yeah, so on that note, when you reflect back on your career, are there barriers that have been more difficult to overcome than you anticipated? Uh points that have been just hard to make that never landed?
Speaker 4: Uh, well, one of the biggest barriers has been whether it's situation awareness or situational awareness. That's a debate that's still that still goes on 30 years later. Um, by the way, uh a grammarian once explained to me that situation awareness is awareness of the situation, situational awareness is awareness that just happens sometimes.
Speaker 2: Oh.
Speaker 4: Uhhuh, yes and that's why situation awareness is is grammatically correct, but uh we find I find situational awareness still gets used all the time. uh a lot of in in the military a lot. So that that remains a a common problem. I've given up uh worrying about that one but uh that's the way it is. I think I think the other bigger biggest barrier has been really true integration of human factors with the engineering community. Um, you know, I come from a design background, uh and engineering background, I'm used to working with other engineers and our processes are and and even our approaches are very different than the ones that they're used to, which are much more analytical, build it kind of of of they can fit their things in a box. And we have to find ways of doing our work earlier and having those uh design inputs into formats that they can use so that we can do things like participate in trade studies. Um, we have to be much better integrated with engineering processes than I think a lot of our work is today.
Speaker 2: Yeah, I I I would agree. I think that that does remain a challenge for our community.
Speaker 3: What is the most exciting thing you're working on today?
Speaker 4: Well, um, right now I'm actually collaborating with others to see if we can find some uh physiological correlates for situation awareness. Um, right the the the traditional measure of uh situation awareness has been the Sagat technique, which we've shown to be very effective in simulation environments, but some people want to find a measure of SA that they can use in real world settings that's a more continuous measure of SA. Uh, we know subjective measures don't work. They're uh really provide an index of people's confidence level, but not necessarily how accurate their situation awareness is. So subjective measures don't actually work that well for collecting data in the real world. Uh, we're thinking that physiological measures may be uh a candidate because they can you can collect continuous real-time data in ways that don't involve questioning the individual. Um, I don't I don't know how well this is going to work. Um, I don't think it's there's anything in the in terms of brain signals that's going to be a perfect representation of how accurate you are. But I do think that we can identify a number of states that we know are bad for situation awareness, things like fatigue, stress, overload, underload, um, you know, there's a number of states that are, uh, we can measure physiologically that we know create problems for situation awareness. Uh, there's also a number of states that we know uh, are correlate with high situation awareness. So things like engagement levels and expectancies that can be measured physiologically. So I think there are some things that we can do to try and and bound the problem some and at least be able to indicate some high and low SA states physiologically. So that's that's a really interesting and and new area that I'm currently working on.
Speaker 3: That does sound interesting. So what what kind of physiological measures?
Speaker 4: Well, right now we're starting with a um a whole set of measures and um are we have it's a it's a battery approach and uh we're going through and validating those measures against uh the Sagat tool so that we can uh assess and and predict SA in some of these more complicate complex environments.
Speaker 2: So things like variance in heart rate and and um all those kinds of things?
Speaker 4: EEGs, heart rates, eye trackers, F-ers, I mean there there's a variety of measures that people have proposed or have looked at and we're we're taking a a broad approach to those.
Speaker 3: So Micah, you you mentioned earlier your your graduate student's work and and I know you've mentored lots of people, um, as they begin their careers both in your company and and obviously in in graduate school. I'm wondering what kind of advice you give to people who are just starting out in the field?
Speaker 4: Well, my biggest advice is find interesting problems and work on them. Um, there are problems all around us, whether it's in the work environment, you know, I've talked about the the fake news problem that that's around us in in our regular lives. Um, there's always interesting problems. And if you just set yourself to working on one of those interesting problems, I think you'll you'll end up feeling very fulfilled. Um, my my my second piece of advice is is, you know, work with interdisciplinary teams. Too often we see people sort of stay in their own little uh bubbles of other like-minded individuals and I think it really behooves us to get out and work with these broader multi-disciplinary teams that have different perspectives because I think there's a lot of of low-hanging fruit that is involved at the intersection of different uh communities. And then the the third piece of advice that that I give to students and and or early career people is to get involved. Um, I think a lot of the best, uh, experiences that I've had have been my involvement in the Human Factors and ergonomics Society, uh, with the cognitive engineering and decision-making TG, with the journal, uh, being on council. Um, to me it's it's been a great way not just to give back to the profession, but it's been really, um, wonderful for me in terms of all the people that I've met and the um, opportunities I've had because of those uh experiences. So I I absolutely encourage people to to get involved in in their professional society.
Speaker 2: Nice. So, um, so looking at it from the other side, who who are three people that have really influenced your thinking and inspired you over the course of your career?
Speaker 4: Oh, there's been so many. Um, I would have to say first Jane Fraser, she was my professor at Purdue and she introduced me to the work of Stu Dryfus on decision-making that was very fundamental to my my later uh formulation of the SA model. Um, Hal Hendricks, who, uh, of course was a long time leader in the Human Factors Society and the organizational design and management work who who really, I think illustrated to me, uh, the real benefits of of involvement in in our society. And then uh Mark Shagne who was my mentor and advisor in my PhD work. And what I think all three of those people have in common is that I didn't just learn a lot from them technically, but I think I also that they mod to me, they modeled the best of academia and what it means to support researchers and students and other people. And uh I I I I found them to be real role models in that regard.
Speaker 3: We've talked about kind of fake news, but the other big issue our world is facing right now is is COVID. And I wonder if you have thoughts about what our community can and should be doing in the context of this global pandemic.
Speaker 4: Yeah, the the Covid Covid pandemic has been quite an experience for all of us. Um, one of the things that that we did on the Human Factors and ergonomics Society, um, what one of the hats I wear there is is our government relations, um, committee chair and we immediately got involved in trying to identify what can we as a society be doing to help with COVID and uh produce several policy papers that people can download. And and there were there a number of areas. One is just uh the whole issue of communications and we found that people who were on the front lines fighting COVID were having to deal with very antiquated uh information systems where they couldn't find what the latest procedure was for example to do something and it was changing very rapidly, uh particularly in the early days of of the pandemic. So we need to do things to help improve our communication systems in in hospitals, for example, and and our communication systems with the public uh as the public was getting all kinds of conflicting and misleading information, how do we how do people sort through that and have a a single site to go to that's useful and readable and understandable. Uh, I've been to the CDC website many times and useful and readable and understandable is probably not the words I would use.
Speaker 2: Right.
Speaker 4: So we need we need better, um, communication tools and and better for helping people to to sort through rapidly changing information.
Speaker 2: So I'm sorry, can I just ask a follow up there? You mentioned um communication in hospitals. So are you thinking about the administrators who are trying to figure out do they have enough beds or are you thinking about frontline workers? Um, are these is this the right treatment or all of the above? What are you thinking about there?
Speaker 4: Well, that there are issues for the administrators for things like just the logistics of of pandemic management. So tracking supplies against needs and so forth. Uh, but the communications problem we were we were finding was hitting people who were, you know, the nurse the scrub nurse who needs to dawn uh PPE and the rules and regulations for that were changing very rapidly as to what should they and shouldn't be doing. Uh so it was it was very much uh communications of changing policies and procedures that were happening at that time.
Speaker 2: I see. I see.
Speaker 4: Um the whole the logistics is is is another uh major issue that I think the our profession can be involved in, you know, just understanding where the needs are and what the supplies are and trying to do a a match of that. That was a real struggle uh early on as the states were in bidding wars against each other trying to get the uh protective equipment and the ventilators that they needed. Um, nobody really knew where the supplies were, what shape they were in when they'd last been checked. Um, they didn't know things like uh if if they had um masks, what size they were for example. Um, some some very basic kinds of things simply weren't being managed. Um, the other the other thing really is is a lot of it has pointed out our our healthcare system is not very effective. We have a real uh patchwork system. And you know, we are we're systems thinkers. That's that's a lot of what we do in our profession and I think that we could be involved in helping to better define what our healthcare system needs to be like going going forward once all of this is over. So there's a lot of areas that that we can get involved in, you know, the whole issue of information flow to the public, trying to help in those areas. uh but you have to be able to take a a very uh long-term view of it uh and I don't think a lot of that has happened.
Speaker 2: Yeah, yeah, I think we're in a very reactive mode at the moment.
Speaker 4: And once it's over, people say, oh great, we can go back to business as usual and they and they won't go back and fix the problems. So uh when we have another one of these possibly in the not too distant future, we could be in the same boat again.
Speaker 3: That's a depressing thought.
Speaker 4: It is. It is, which is why we have to we have to do the systems thinking and get these things addressed.
Speaker 3: So NDM could uh could help out here. Um, and I'm just wondering sort of to capture your your the essence of you for NDM. So a hypothetical question, let's say you meet a complete stranger who claims to practice NDM, and on the pain of death, you're given one question to determine if they do indeed practice NDM. What do you ask?
Speaker 4: My question would be, who do you study? And and I say that because sometimes we get focused in on, you know, where where it occurs and a lot of our work occurs out in the field and actual thick environments, but also a lot of good quality work occurs in complex simulation environments. So it could be in a field or it could be in a simulation environment. But I think one thing that is, um, very specific about NDM is we try to study people who are actually solving real world problems. So when I say who do you study, if you tell me, oh, you're studying nurses or you're studying pilots or you're studying air traffic controllers or you're studying, um, voters, you know, those are real world people. If you tell me you're studying college students, uh, then right away, I know you're not really getting at these real world tasks that that people are, um, addressing. So that to me is probably the biggest differentiator. Who do you study?
Speaker 3: So not even not even experts but people engaged in real world stuff.
Speaker 4: Real world, yeah, absolutely. I mean they could be novices and, you know, we we study experts, we also study novices and people and intermediates, people going through that learning process. Those are all fair game for I think the kinds of of, uh, people and problems and tasks we need to be looking at, um, but but it's it's not these, you know, abstract tasks in a Psyclab with with college sophomores is I think not what NDM has typically been about.
Speaker 3: Right. And we tell our participants that they're actually in a study. So that's another good thing we do.
Speaker 4: (laughing)
Speaker 3: All right, now a couple a couple of fun questions. The first one is uh, tell us one thing about you that you think the audience probably doesn't know.
Speaker 4: One thing that the audience probably doesn't know. Well, um, in my spare time, I do uh copper artwork. So I work with natural patinas on copper and create art.
Speaker 2: Wow.
Speaker 3: I did not know that.
Speaker 4: See, you didn't know that.
Speaker 3: will anyone have seen your work?
Speaker 4: Um, well, I did donate some to the um auction that we had at the Human Factors Economics Society, they have a little auction that is for raising money for college scholarships and so I've donated to that in the past and maybe I will next time. So go to the auction and maybe you'll see some.
Speaker 3: How did you get involved and and why copper?
Speaker 4: You know, it was just something I was intrigued with and I really like the I I'm I'm not a very artistic person. I can't paint or or draw very well, but I was just fascinated with the different types of patterns and colors that you can produce in uh copper all through natural chemicals and patinas. So maybe maybe it's my inner chemist.
Speaker 2: So, um, so the second fun question, um, so you have over the course of your career interacted with people who are expert in many, many different things. If you could instantly become an expert in something, what would you choose?
Speaker 4: I think if I could instantly become an expert, the thing that I would want to know a lot about is the whole issue of green energy and what we can do to develop new energy systems for replacing where we're at right now with fossil fuels. To me, this is the big question and the big issue that will be consuming us for the next couple of decades is changing our entire uh mechanism for for energy production.
Speaker 2: Interesting. So that maybe that's your next career.
Speaker 4: Maybe it's my next career. Well, I did I did buy a Tesla. Um, I I I said it was to study automation, but really I was I really liked the battery technology and I wanted to see if we were ready to to live in a world with electric vehicles and and I think the answer to that is yes, that we can easily convert uh if not every vehicle, a lot of vehicles to uh electric battery technology very successfully.
Speaker 3: Have you put your $100 deposit down on a cyber truck?
Speaker 4: I have not. Not sure I need one of those, but uh maybe you can do that.
Speaker 3: You have to you have to haul copper.
Speaker 4: I have to haul copper. Luckily, luckily copper doesn't weigh too way too much and we produce a lot of copper here in Arizona, so I'm close to the supply.
Speaker 2: Great. Well, um, this has been wonderful. Um, Micah. I want to thank you for speaking with us today.
Speaker 4: Happy to be here and it was very nice talking with you.
Speaker 2: Yeah, this has been a real pleasure. Um, so on that note, I want to thank you for joining us for the NDM podcast. I'm Laura Militello.
Speaker 3: And I'm Brian Moon. Learn more about naturalistic decision making and where to follow us by visiting naturalistic decisionmaking.org.
(Music)