Okay, here is the transcription of the podcast episode, with speaker diarization:

**Speaker 1:** The Naturalistic Decision Making Podcast with Brian Moon and Laura Millitello. This podcast series brings you interviews with leading NDM researchers who study and support people who make decisions under stress.

**Speaker 2:** Welcome to the Naturalistic Decision Making podcast. This is Laura Millitella from Applied Decision Science.

**Speaker 3:** And I'm Brian Moon from Perigian Technologies. Today we're doing something a little bit different. It's actually something we've done before. Back in 2020, we dedicated a podcast to the inaugural winners of the NDM Awards. Those are two awards, the Gary A Klein Award for best contribution to NDM theory, and the Robert R Hoffman Award for best contribution to NDM methodology. We've actually had a number of winners since 2020, and we are way overdue to showcase the outstanding NDM contributors from recent conferences. So today is our day to do that. We have a packed house, so with us today are the Gary Klein Award winners, Jordan Hat and Nista Jane, and the Robert Hoffman Award winners are Reza Jain and Carrie Hoy. So, welcome to you all and thanks for joining us. To get us started, here's how we're going to run things. Each of you are going to have about six minutes to introduce yourself, say a bit about your background and your current role, and then tell us about your award-winning paper, what it's about, you know, why you wanted to write it for the NDM conference, any connections from that paper to current work you might be doing. And then once everyone has shared their papers, Laura and I will circle back and ask you each a question about your paper. And we'll take some time at the end to hear about your life as NDM researchers. So, with that, let's get started. We're going to start with Reza and his 2022 Hoffman Award winning paper, Helping people to understand AI systems, the explanation scorecard. So Reza, over to you.

**Speaker 4:** Thank you, Laura and Brian. It's an honor to be part of this conversation today. Well, my full name is Muhammad Reza Jain, a research engineer currently at the Cognitive Systems Engineering Lab. It's in the Department of Integrated Systems at Ohio State University. My background is in learning system design, but by trade, I'm a cognitive system engineer. My work currently focus on improving human machine collaboration. The Cognitive Systems Engineering Lab or we call it Cso, I'm particularly focused on joint activity design and the development of joint activity design heuristics. These concepts are key to understanding and improving how human AI systems collaborate in real world settings. So, allow me to unpack these two concepts a bit more because I think they are central to how we approach human machine collaboration in complex environment. So, Jad or joint activity design is the process of creating systems that enable humans and machines to work seamlessly together and it's toward the common goal. When we talk about joint activity, we are referring to how humans and machines coordinate, share tasks, anticipate each other's actions in real world and of course in dynamic settings. The challenge in joint activity design I think is to ensure that the combined performance of human machine team is greater than what either the human or the machine could achieve on their own. In other words, the goal is to improve the overall performance, not simply by adding a technology to the mix and hoping for the best. To achieve this goal, we apply a set of joint activity design heuristic. These are evidence-based guidelines that we use to create systems in a way that supports key microcognitive functions, which are the high-level cognitive processes that happen in real world. These includes things like joint activity detection, joint sense-making, human adaptability, human machine coordination. For example, one of the heuristics may suggest that AI should offer explanations in real time that helps human operator understand not just what the system is doing, but also why it's taking that particular action or in broader term that refers to in the community in AI, XAI community as the local explanation. This kind of I think transparency is essential for building trust, reliance, sense-making and even coordination. If I want to give you an overview of the paper and then later I connect that to this brief overview that I give you about what I work actually and what I do in the cognitive system engineering lab, I would say the the paper that I'm going to discuss in more details later, helping people to understand AI systems, the explanation scorecard builds on an earlier research project that I co-authored with Robert Hoffman. That paper which is the foundation of this paper is called explanation, evaluating machine generated explanations, a scorecard method for XAI measurement science. So to give you a the general overview, the explanation scorecard in both papers is a structured way to measure how effectively an AI system explain itself to people. We created this scorecard to address the gap which was at least at the time we were working on the article in the AI fields where explanations often fell short in helping user understand the decision making process of the system. When people interact with AI, especially in high-stake fields, for example, in health care, it's critical that they understand why the AI made a certain decision. However, many existing explanations as we reviewed the literature, it shows us that most of these explanations are superficial, meaning they just give you hints and clues rather a deep cognitive understanding of what the machine is actually doing and why it's making that sort of decision. So, the motivation came from this observation that AI systems often use technical language or visual or complex visuals that are difficult for non-experts to understand these complex systems. For example, imagine a health care professional using an AI diagnostic tool that only provides a heat map of the areas it considered in its diagnosis. So, this may show where AI is looking, but it doesn't explain why or how it decided on a diagnosis. So, our research here recognized that the an explanation by AI is only helpful if it really supports users' sense-making. So, sense-making here is a key which I'll discuss more later through this conversation. Connection to my current work, I would say I think there are strong connections between what we discuss in the paper and my current work is in CSO. In general, both are rooted in this same overarching goal, which is enhancing human machine collaboration by designing AI systems that support microcognitive function such as sense-making. And to ensure that AI becomes a true partner in decision-making process and not just a technology she's added to the toolbox.

**Speaker 2:** Thanks Reza. That was a great summary, fascinating work. Next up is Jordan Hat, the 2022 Klein Award winner for a model of reframing for intelligence analysis teams.

**Speaker 5:** Yeah, thank you guys so much for having me. So, I'm Jordan Hat, I'm currently a civilian researcher with the Air Force Research Lab, research psychologist, human factors focus where, you know, I'm primarily interested in supporting effective human machine teaming through things like information visualization, through better understanding sort of the coordinative activities between humans and machines. And that's primarily been my focus here and then as well as my previous roles and a couple of different contracting companies where I kind of did things ranging from developing game-based learning platforms for the Navy to evaluating automated technologies implemented in the rail cab and then got to do some great work working with intel analysts which is the paper that we're discussing here. So, that was part of a contracting team where the Air Force kind of tasked us to try to better understand Intel analysis and the cognitive work associated with it, the challenges, performance implications of sort of this new highly collaborative intelligence analysis teaming structure. So, kind of traditionally, Intel in a ped cell might be something like, you know, an imagery analyst looking at video or images of people, you know, entering and exiting buildings, they might be looking for certain behaviors of transport vehicles in an area, sort of pattern of life stuff where, you know, they'd do the analysis and they'd pass that information kind of to the next person and then continue in that kind of work. The new construct kind of came from, you know, our inability to perform sort of direct collection efforts and get that type of information. And so the customers have started to ask broader questions. So, for example, you know, how are adversaries transporting weapons across borders? That's certainly, you know, less direct ways of going about doing that. There's a bunch of different ways you can go about doing that. It certainly requires a lot of collaboration across different intelligence expertise. And it's a super difficult thing to do because in this information environment that they work in, you know, they have more information than ever before, they have lots of tools that they use, it's characterized by high levels of uncertainty, high consequence. And so, I think that fit really well into, you know, presenting that work at NDM because, you know, that's sort of the nature of a lot of the domains that we're interested in. And so, we've conducted a lot of knowledge licitation, CTA type work with intelligence analysts. We got to visit the DGS site, which was really cool to kind of see the work happening and unfolding on the floor. And ultimately, you know, we started to see patterns that were consistent with kind of two models of sensemaking. And we were trying to, yeah, again, figure out, you know, how best to support future analysis work through development of new technologies or, yeah, new processes to help them. And one of those models is was the data frame model of sensemaking, which I'm sure a lot of folks are familiar with. So it kind of describes, you know, when we're responding to a problem, we have some hypothesis or some frame about how to approach that problem. So if we're interested in, you know, determining how weapons are being transported across borders, we might think that weapons are being transported over say shipping ports on ships. And so that's going to guide our information search. And based on what we find, you know, it might support our frame, our current hypothesis or it might cause us to reframe. So say if someone passes us information that suggests that maybe there's increased activity at rail yards or something. And so we kind of reframe to to then go look at data related to rail yards. And for more on that, you know, more and Hoffman have a great paper that kind of details sensemaking through the lens of the data frame model for intelligence analysis. I think what's less familiar to folks here is probably the application of signal detection theory as an alternative framing to sensemaking. So it kind of emphasizes some unique factors compared to data frame model. And I think, you know, in general intelligence is a you know, a signal detection problem. We have lots of data and information, we have to pick out a signal or meaning from that information. So, you know, in signal detection, you're judging information that you encounter based on kind of two choices, right? You have representations of two probability distributions that are overlapping at the tails, which means you can never make perfect judgments. There's always some degree of uncertainty or unpredictability in your judgments. So, you have to determine whether information that you encounter is part of that signal probability distribution in which case it's meaningful and supports your frame, or whether that information is part of the noise distribution and is not meaningful or does not support your frame or causes you to sort of reframe. And that judgment is going to be based on kind of two things simultaneously. So, one is virtue is D prime in signal detection. And that's the discriminability of information. So if you have an image of a ship with a weapon on the deck, you know, you can be pretty confident that that suggests you should maintain your frame. But if you have an image of a ship that has sort of like a high water line, that might indicate weapons are present, there's certainly something on board that's potentially unexpected, but there's more sort of ambiguity about that judgment. So basically you have to set a decision threshold, which is beta in signal detection theory. And that represents, you know, how conservative or aggressive you are with the judging that information as supporting or contradicting your frame. So, if you're more conservative, then you're not going to be as willing to update or to maintain your frame. You're going to need more the information that causes you to maintain your frame is going to need to reach a higher criterion. And so, the real implications of that are, you know, how much risk or the type of errors that you're willing to accept. So you you have to accept either an increase in misses when you're more conservative or an increase in false alarms if you decide to be sort of less conservative or, you know, engage in reframing more often. So, there's, you know, implications of that from a teaming perspective is that, you know, you can support your ability to make those discriminations or that, you know, have a better optimal D prime through pulling in additional perspectives from different team members, different types of experienced analysts are going to have more sort of information that'll help you kind of triangulate the problem better. And then you'll also be able to sort of better tune your decision criteria to strike the the right balance for tradeoffs. So, you know, you can imagine the difference between the tradeoffs required for judging sort of surface to air missiles which are a little more mundane than say like nuclear weapons. You're not going to want to sort of maybe miss as many nuclear weapons as surface air missiles. But then there's also implications with, you know, increasing the false alarms related to nuclear weapons. So that balance is really important to strike and takes a lot of experience to do that. And then those implications can also be negative too. So the the teaming could be set up in a way that, you know, there's not the right mix of expertise or there's not the right social dynamics. And so information sharing isn't optimal. And, you know, that can negatively impact the ability to engage in high levels of sense-making within a intel team. And I think all of this is continuing to leverage current work more recently, trying to build decision support aids through software for intelligence analysis. And so this has been really helpful and I'm excited to kind of continue, you know, pursuing this and working towards implementing these lessons learned into that.

**Speaker 3:** Awesome. Well, thanks Jordan. I have lots of questions, so I'm going to have to constrain myself later, but for now, we're going to move on to Carrie, who's going to discuss her 2024 paper, Cognitive complexity tool, an instrument for identifying cognitive challenges in health care, which was the winner of the Hoffman Award. Carrie, over to you.

**Speaker 6:** Thanks Brian and thank you both Laura and Brian for having me on the call today. So my name is Carrie Hoy. I am a senior research and project assistant at Shadowbox LLC. I've been with the company for three and a half years now. I have a little bit different of a background from the others on the call, I think, but my background is actually in the service industry. I spent 16 years managing restaurants. My focus extended beyond daily operations to building cohesive, productive and efficient teams. And so during this time, I became very interested in adherence to policies and procedures. A core part of my role was to kind of identify the roots of non- compliance and then work to reduce it. So to do that, I used in depth discussions with managers across the different locations that we had to understand their reasons for non- compliance. I wanted to know more of the why behind their actions where there are obstacles that they made following the the procedure difficult, where competing demands of factor, things like that. I didn't actually realize it at the time, but I was essentially conducting cognitive interviews with the managers. I had no prior knowledge of cognitive interviews at that time. So looking back on it is really interesting to see that that's kind of what was happening without me actually knowing it or being intentional with it. So these conversations actually told me that our procedures at the time often did not align with the reality of daily operations. And that was the first time that I was introduced to the work as imagined versus work as performed concept. So that was actually the basis of my interest in the project work that we did, which led to the paper, complexity tool paper. So the project that we were working on originally focused on air flow management in operating rooms, particularly health care professionals mental models of air flow and the implications for infection prevention and control in operating rooms. So as part of that, which Reza you might remember because you were part of this from the beginning stages. We were looking at exploring the written OR policies from a human factors perspective. And as we started getting into looking at the written policies and getting into doing the observations in the ORs and interviewing the health care professionals, it came to light that the OR tasks are extremely cognitively demanding. And this led us to kind of question whether the existing written policies supported or hindered health care professionals as they tried to follow those complex procedures. So for this study, we actually focused on account protocol, which was identified previously in the literature as being a a very cognitively demanding task. And the count protocol is essentially it's just a manual procedure for tracking the surgical tools and instruments and soft goods, so sponges that are introduced to the surgical field during a surgery. There's two team members particularly involved in that execution of the council, the surgical technician and the circulating nurse are the two that execute that. The protocol aims to reduce instances of unintentionally retained foreign objects which are UFROs, meaning items unknowingly left in the patient's body during a procedure. And in 2023, the Joint Commission actually listed UFROs as the third most common sentinel event, so causing direct harm to patients resulting in severe or permanent injury, sometimes even death. So recent efforts have been focused more on designing adjunct technologies, but not as much on emphasizing the cognitive complexities that are involved in executing the the count protocol and what else is going on in the operating room when they are actively executing that. So the count protocol itself is very cognitively complex, not just because it involves coordination and communication across teams in a high stress, high stake environment, but depending on the type of surgery, hundreds of tools and instruments and soft goods may need to be counted in a very, very short amount of time. And the surgery does not stop while the counting is happening. So the surgical technician is still being asked for different instruments and tools that the surgeon needs. the circulating nurse is simultaneously as well, keeping track of all the other things that are happening in the the operating room at the time while completing that. And again, it's a critical procedure that can have direct impact on a patient and their safety if something is unknowingly left in the patient. So, for the study, we developed an interview tool to help health care workers articulate the cognitive challenges of the count procedure. So this tool uses probes designed to explore the macro cognitive functions and includes examples to prompt health care workers to think about their work in different ways. So we had three main goals in creating this tool. The first was to help the health care workers articulate the complexities of the work and then also think about the implications of these challenges. The second was to kind of reveal the gaps between work as imagined, the written protocols, policies and the work as actually performed. And the third was to provide policy makers with direct feedback from the frontline workers, the ones who are actually doing the work to improve written policies and support aids, things like that. So we tested the tool by interviewing eight circulating nurses and five surgical technicians who were actively involved in executing the count protocol and this was across a large academic hospital as well as a smaller community hospital. We walked each participant through 14 different categories and responding to corresponding probes, asking them to rate their agreement for each category on a one to three like scale. participants then shared specific examples that they could remember from their experiences to kind of explore those complexities a little bit more. And results showed that teamwork, competing demands and a lack of standardization were frequent challenges that kind of got in the way of their strict adherence to protocols to the the count protocol specifically. So feedback on the tool was pretty positive. Overall, everybody found it easy to use, effective for capturing diverse perspectives, provided them a platform for frontline workers to influence policies. Some gave comments and feedback about a desire for more anonymity, while others were talking about kind of implementing this in a broader department review meeting. So our team was really, really excited to share this work at NDM in New Zealand in part because the tool and the insights we gained were kind of unexpected, it wasn't really the goal of the project initially. So we were all pretty excited about that. But also as I've learned, this particular theme of misaligned work as imagined versus work as performed seems to be a recurring theme in the NDM community and that came up in several of the the presentations across many of the domains at at the NDM conference. So, currently, I'm working in a project with field and panel operators in the Century Panel Go Refinery and we're working to develop training scenarios that basically tap into the tacit knowledge of their senior operators. So, this is a very technical and proceduralized domain. We constantly hear operators referencing policies and procedures. Most of the time it's, oh, the policy states, you know, we should do X, Y and Z, but sometimes it's, well, the policy states this, but So, I think that as long as those moments of divergence from policy exist, there's always going to be opportunities to dig deeper into the cognitive challenges of the frontline workers and to kind of gain insights that that can really improve the support for real world work across a variety of domains. So, I think that the paper and the work that we do is is really exciting. So I was happy to have an opportunity to share it.

**Speaker 2:** Thanks Carrie. That was really, really interesting and very cool work. So finally, our last person today is joining us from India. Nita Jane is going to share her 2024 Klein Awarded paper, understanding the complexity of the decision process for doctors in the treatment of cancer.

**Speaker 7:** Hi, thanks Laura and Brian. Thank you for inviting me. So, I'm Nita Jain and my background is in psychology with a very deep dive into decision making. The work that I presented at NDM was uh sort of part of my PhD dissertation that I conducted at IIT Delhi. And also I'm very excited to share this that I have my PhD defense next week. So I'll be finally getting awarded. My PhD basically talks about understanding decision process in the treatment of cancer. I'm currently also working as a lecturer at OP Jindal Global University in India. Uh but coming to the work that I presented at NDM, the idea behind that paper was in fact very simple. What I wanted to understand was how doctors make decisions in the treatment of cancer. More specifically, if I talk about, so uh what are the real critical decisions that they face when when taking the treatment? And also, how do they proceed with such decisions? Now, what happened was that when I explored these questions in the research literature, the decision research literature, I was slightly taken aback because there is this humongous, huge body of research. And yet this simple question of, you know, what are the real decisions that the stakeholders face, that that the experts face was almost missing from the literature. Instead, what I found was these hypothetical scenarios, hypothetical decision scenarios that barely connected with the Indian context. And when I say the Indian context, what I mean is there are certain cultural and they are very unique by the way. The cultural and the systemic factors uh in which the doctors work here, right? So for instance, there is this huge patient load, very low paying capacity of patients, the doctor patient ratio is also very haywire. And interestingly, this might come as a surprise to most of the listeners for this podcast. So, there's very high prevalence of collusion where the family members of the patients, the family caregivers, they often insist the doctors to not share the diagnosis with the patients. So what this means is that the patients often undertake the treatment without even knowing that they are suffering from cancer. And now, what happens is that when I look at all these factors that are there in this context, and I look at the decision research literature, I am at a loss, right? Where do I locate my research questions? And this is when I would say uh that NDM sort of rescued me. Gave me the foundation, it gave me the ground to bring in these crucial aspects into the picture. As NDM also focuses on the features of the decision setting, it acknowledges the need to actually talk to the stakeholders, to the real decision makers, right? But again, I would say that NDM uh though it acknowledges the role of the context in which the decisions are taken, but that focus is still slightly narrow because what we mean by context is experience level of decision makers maybe or the quality of information that is available, the time pressures that are available. But what about the probable role of these metacontextual factors, these cultural and uh systemic challenges that are often present in the decision making setting, right? So to explore the research questions, we conducted interviews with 25 doctors working in a government hospital. Now government hospitals in India are very highly loaded, and they work with very limited resources also. There's like a huge resource crunch that the doctors face because the patient numbers are very high, but the resources are sort of limited. Now, if I just reflect back on the very important findings from this piece of research was, there are two. We need to understand two things. So the analysis suggests that the doctors did not focus on choosing the best treatment option. They did not, so this is the patient A and this is what the symptoms are, the best treatment available is sort of treatment A, right? But this is not how they go on about decision making. Instead, they have to personalize the decisions, they have to fit, they have to tailor the decisions according to the needs of that patient, right? And there are so many other things, the complexities that are involved in the decision. But when we look at the literature, there is a huge gap because we do, I guess, underplay these complexities to some extent. So one thing to understand is that there is probably no best choice, no best decision when making decisions. And the second would be that okay, so these decisions are not linear, they're very, there are a lot of aspects that are going on that are into play, they're not entirely technical either, all right. But, more than that, these decisions are not taken by a single individual either, right? These decisions are often taken into communication, when we communicate with others, they're rarely taken by a single individual, right? So, these two aspects, that there is probably no best choice taken by a single individual. This concept perhaps needs to be revisited. The research, the study also showed that there are so many intersecting factors, intersectionality and decision research is also something that perhaps we need to look at more closely. And now, on the basis of this work, my next phase of research would be to take these interviews, take these analysis and my understanding from the NDM conference which just opened my eyes in a way, to take all this learning and to build certain cost-effective decision aids for the doctors that would help them make the decision process a little bit more efficient and hopefully less traumatic for the stakeholders. So yeah, thank you.

**Speaker 3:** Ah, that's terrific Nita. Appreciate you sharing and for joining us always from India and thanks to everybody else for sharing. This is such a deserving bunch of papers and a talented group. I I want to just circle back to everybody and and have us ask a question. Let's think of this as a lightning round and we're going to ask questions and hopefully get abbreviated answers. I I think we could spend a whole podcast on each of these, but we'll try to get through it. So circle back to Reza, I wanted to ask, you mentioned about the explanation sort of going in both directions, humans explaining to machines and machines explaining to people. I'm curious on the machine to people side if machines, you know, they don't necessarily understand what errors they might be making or where they might be weak. How do they explain things like I'm having a bad day that you might expect to hear from a human collaborator or I'm not, you know, strong in this particular area of analysis. How do you see machines doing that explanation?

**Speaker 4:** That's a great question, Brian. I think so on the explanation scorecard, we have defined seven levels. And each level generate some sort of explanation to the users. And if I briefly go through these levels from surface features at level one to instances of success, instances of failure, AI reasoning, diagnosis of failures, exploration and finally interactive adaptation. So I think what you are referring to really applies at level three when there are instances of failure, when the machine at this level, AI provides some examples of mistake, helping users understand when and why the AI might getting things wrong. And even diagnosis of failure. The AI not only just show its mistakes, but it shows also the diagnosis, why, why these mistakes have occurred and it helps users anticipate where the AI might be less reliable. But more importantly than this, that the AI explains or provides some sort of explanation to the users that uh is having some bad day, there are some failures or here is why the failure has happened. I think the nature of that explanation to the users is what makes the the heart of this paper. And that comes from extensive review of the literature from cognitive psychology to to even instructional design and learning science that how we should explain how AI really should communicate that sort of explanations to the users. So, at the lower levels, the explanations explanation tends to be simpler. Explanations at higher levels, however, tend to be more cognitively, more complex, which means requires deeper levels of analysis and offer deeper insight. But this doesn't mean that more mental effort is required simply because the explanations are more detailed and technical. It means that the explanations provided richer supports for sense. So, yes, the machine provide some sort of explanations that they're having some bad day, that the machine is having bad day, the machine has failed or the system has failed. But more importantly, the way that the machine communicates that to the user, so the user understand where is the problem, why the machine failed and how they basically comes out of this failure successfully and how users can anticipate future failures. Those are really the main topics that we discussed in the paper. And I think in that sense, this really aligns well with the concept and theories in human learning and cognitive science. One key ideas in the paper was to show the scorecard aligned with established frameworks from both cognitive psychology and artificial intelligence. Even though a scorecard was specifically designed to evaluate AI explanations, but in cognitive psychology and instructional design, for example, there is a well-known taxonomy, Bloom's taxonomy, which organizes types of knowledge and cognitive processes like remembering, understanding and analyzing. The scorecard level is also equals these categories, but instead of ranking explanation by difficulty from level one to level seven, we focus on how well they support sense-making, which is how well the users understand and apply what the AI is doing. If the AI is making mistake, how well the user can understand that mistake? How how well the user understand that system failure and how well they anticipate future similar failures. The emphasized on sense-making here is consistent with emphasized of the learning science on meta cognition, that is deliberative reasoning about your own understanding. Research has shown that the deeper processing and comprehension of the meaning by meaning, I mean the comprehension of the explanation statements results in better recall and better recognitions compared to processing of surface features of the learning materials. So, deliberate self explanation here plays a significant role in learners understanding whether it is in school, whether it's a student learning in school about their own understanding or learning about complex system. So,

**Speaker 2:** Hey Reza, I want to jump in here. This is really fascinating. I had a follow-up question that I think is related. So, I am wondering in your lit review and your examination of explanation, if you have come across any positive examples, tools or systems that really do a nice job of explaining to the user what's going on. Have you come across any bright spots that you like to use to describe as a positive example?

**Speaker 4:** Absolutely. Let me just clarify here if I understood correctly, by positive, do you mean good example of explanations?

**Speaker 2:** Yes.

**Speaker 4:** Absolutely. We found studies that first of all, what we found from our extensive review of literature was explanations at at the AI developers levels is not limited to one level. Sometimes we could find multiple levels of explanations. For example, we could find instances of success, we could find AI reasoning, which is level four, we could find diagnosis of failure, we could find exploration all at the same time in discussing the paper by the developers. But we usually when we wanted to score that generated explanation, we we chose the higher level. So, we found that majority of these explanations created by XAI developers are addressing level one, two, three and some of them level six. So, we could find level six exploration which is the level allows user to explore what if scenarios, meaning where they can modify certain features to see how the AI decisions would change. So, this exploration is amazing in terms of sense-making. So, it it gives users a leverage to work with the AI, to make some sort of exploration. So, yes, absolutely we found a lot of articles, a lot of papers in the field that explanations generated where scored at level six.

**Speaker 2:** Very, very encouraging. I feel like we're always talking about what isn't working, it's good to have some examples where things really do work well. Thank you. I wish we had more time, but we have to move on to the next topic. So Jordan, I wanted to ask about your paper. So, you talked about this new highly collaborative Intel structure. And so I was wondering as you were thinking about sense-making in a collaborative sense or across a team, I feel like a lot of sense-making is very focused on an individual and I'm just wondering about the team component. Can you talk about that at all?

**Speaker 5:** Yeah, absolutely. So, yeah, and that was sort of one of the challenges too, but the data frame model isn't, you know, exclusive to individual sense-making, right? It does sort of make room for team sense-making. I think, yeah, I mean the the most important factors that we kind of got into that came out of some of our interviews were related to, you know, the makeup of the teams in both sort of size and background. So, the group that we interviewed, there was, you know, 8 to 10 analysts working within a team at any given point in time with a variety of, you know, there was a handful of imagery analysts, signals analysts, human folks and I think the sort of variety of background really helped provide additional, I guess frames if you will into the nature of the problem. So we can get strength and confidence in the the variety of information that we're kind of pushing and pulling through the team. So, you know, it's based on that, it's also based on sort of like the social dynamics of the team. So not only, you know, what someone's background, what's their expertise and but also like personality wise, how amenable are they to working in a team setting? So there's sort of like these softer skills related to being open to someone, you know, suggesting a new hypothesis for you or kind of having the ability to push and pull information when necessary. So I think, you know, there's a lot there because of certain changes in leadership, the project kind of shifted away from that a bit unfortunately, so we didn't get to dive into that as much as we would have liked to. But yeah, those are certainly things that I think would be really interesting to kind of keep exploring.

**Speaker 3:** Jordan, I want to ask and this is maybe a weighty question, but so you talked about the data frame model, you talked about signal detection theory, who wins? Are there opportunities in NDM to actually test models against one another and this is more a philosophical question. I mean, are we doing science with these models or are we just kind of seeing explanations everywhere we look?

**Speaker 5:** That is a weighty question, Brian. I think, you know, there's it's not a sort of zero sum game if you will. I think particularly in this case, each provides a frame of reference that's different than the other. Like the data frame model kind of helps us think about the continuous kind of iterative nature of sense-making as it happens. There's a number of papers that sort of indicate useful pieces of information that we can present to people engaged in sense-making to help sort of optimize that. I think in relation to reframing, signal detection provides the additional kind of perspective that these tradeoffs that people are having to make are really important and we need to also consider those. And just by sort of shifts in a decision criteria based on, you know, your willingness to accept misses or false alarms, kind of really impacts your information search and your judgment about information that you're seeing. Yeah, I mean that's a great question. Again, I think just more time to sort of explore this and I'm not sure that that really fully answered it, but yeah, really interesting question.

**Speaker 3:** You successfully reframed my question, which is I guess is appropriate given your paper title. All right, moving on to Carrie. So UFROs, good Lord, that's frightening stuff. Have you been able to circle back or bring your work to the attention of the folks that are concerned about that issue or do you have any plans for doing so? It seems like that commission you mentioned would really benefit from your work.

**Speaker 6:** Yeah, absolutely. So we haven't yet. I think that what our work could benefit from is when we were in the study, we kind of, like I said, this was not the original focal point of the study, so we came into it a little bit late. And so by the time we got through data collection, going through all the interviews, stuff like that, we only got through really one round of interviews and data collection with that surgical count protocol. So, I think it would benefit us to kind of go through another round with a different infection prevention protocol as well. But you're right, UFROs continue to be an issue. I think that along with them being the third sentinel event cause, they also cited that sponges, lap sponges are the most retained foreign object and that could be intentionally like the health care workers intending to leave it in there for a period of time and then planned to go back and get it afterwards. And then the other thing is surgical needles which can maybe too small to even be detected by X-rays. So I think that the work we're doing is very important specifically to mitigating that. And I do kind of want to just bring the point back that the focus right now seems to be on creating adjunct technology. So we're putting on scannable tags on these sponges that'll help us locate those, but our research showed that the complexities that health care workers were citing revolved around issues like teamwork, issues like lack of standardization. So creating the adjunct technology is is good and it's needed, but it doesn't really address the cognitive challenges that the health care workers, the frontline workers are citing as being the main challenges that they're experiencing when executing this specific protocol.

**Speaker 2:** Okay, Carrie, I wanted to ask you full disclosure, I was peripherally involved in this work and got to review some versions of the questionnaire that you used. And so I suspect this questionnaire has relevance beyond health care. There are lots of industries that use written policies and protocols that then have to be applied in many circumstances. And I just wonder, have you thought about adapting this for your work in petrochemical or going back to food service, you know, other industries?

**Speaker 6:** Yeah, absolutely. I mean, that comes to mind all the time. I flash back and think about, you know, situations that we had in restaurants where we were dealing with managers that are working a variety of volumes. So I'll have a one restaurant that's doing, you know, a million dollars a year and then another restaurant that's not even touching 300,000. There's no way that I can get the same experiences that the higher volume location had into that manager with the lower volume because it's just not they don't have the same experiences, flows, situations don't arise, but they still need to know and understand and be knowledgeable of those situations that could potentially arise. So I think that having these kind of conversations and taking some of what we've learned with this tool and using it in higher volume locations and getting those insights from those managers could easily be kind of transferred and used as a training tool for those lower volume location managers to kind of get them the experiences that they're not getting organically. But yeah, it comes up all the time and I I think that that's another arm of like where we want to take this work, I think, is trying to generalize it into into other domains.

**Speaker 2:** Very nice.

**Speaker 3:** Awesome. Yeah, it seems like generalizability is something we should be talking more about with all of our work. All right, so Nisha, I wanted to ask you some of the cultural systemic issues you raised, it would seem to me that, you know, some of them are probably publicly known, things like high case load, limited resources, but I'm curious if there were any sort of systemic issues that you discovered that really surprised you or or weren't widely known or that surprised others when you told them about it?

**Speaker 7:** Oh, right. So I think the first one was about collusion. Even though people do, even in India, they do know that yes, sometimes the patients do not know, but they don't know the extent of it. So, when I say that the patients do not know their diagnosis and prognosis, they really don't know. For example, this one doctor, she told me, she's a surgical oncologist and she told me that the family was insisting upon her to not disclose the diagnosis even though the patient was just about to go into a very major surgery related to the voice lyrics, right? So he could possibly lose his voice also, right? Those were the potential risks of that surgery. And even then the family was insisting of about like not disclosing the diagnosis and prognosis. So, I think this is the most most difficult issue communication. Communication between doctor and patient. There's so much literature research talking about trust. How do you build trust without telling the diagnosis, without sharing the prognosis of the treatment, right? Again, another thing, widely known, yes, but it's just how it plays out in actual decision scenarios. Another thing is low education at times. Low patient literacy, right? Yes, this would be widely known, but how this plays out in actual decision scenarios is slightly difficult to understand that even for me, right? Where we want to talk about shared decision making, the patients, the doctors, they want to explain the pros and cons of the treatment, we want the patients to make informed choices, right? But how is that possible? How to make that possible, right? So, you really have to go into the scenario, the the system to understand how things are truly playing out, right? And these are not single individuals. There's communications are happening in so many layers, right? The doctors they communicate in a team and these teams are also horizontal and vertical in terms of expertise, right? Now, in these teams, they make a decision, they go back to the patients and family caregivers, very difficult to explain the decisions or the pros and cons of the treatment due to all these factors that I have discussed, right? And then the patient tells you something, you understand, okay, so there's no insurance, which is another layer to it, right? So, this is India's missing middle. 40 crore people do not have any sort of health insurance, right? And then we come to know, okay, so this drug is so expensive that the patients can't even pay for it. Back to the team, what to do now, right? And again, you think of some idea or something, you go back to the patient. And then you realize that there's no after care support structure. So maybe there is a treatment, uh for example, chemo, radiation and then surgery. And this rural lady is coming in, she's dependent on her relatives for finances, for practical support, for logistical support of taking the treatment. And you realize that okay, so coming 70 or 100 kilometers from her home place to the health care center is just like not possible, not feasible. So the best decision in this scenario would be to maybe go for a radical surgery instead of the best treatment plan that would be would be better given her symptoms, right? So well, yeah, these are some cultural and systemic challenges that are just there and some are widely known, but even then, I feel that the extent to which that these influence the final decision, it is not, you know, that well considered, especially in literature.

**Speaker 2:** Nita, one of the things I really appreciate about your work is the emphasis you are understanding that has placed on this, you call it the macro culture, the larger culture. When I think about clinical decision support in the US, it's generally about helping people determine the best clinical treatment option. And so when you, I mean maybe this is not a fair question because you haven't designed decision support yet, but I'm wondering as you are kind of imagining this, do you have thoughts about how to incorporate things like communication and the patient's goals or the patient's family's goals in terms of, do you share the diagnosis or not? Like have you thought about how to include some of those elements?

**Speaker 7:** Uh, Laura, what I believe is that any decision aid that would be effective in this particular context has to talk about communication, right? Because that is the core issue, the critical uh decision where the, you know, the doctors are really challenged. It's not about deciding which treatment to go for, but really like how to share it, how to go about it. So, yeah, that's what I feel at least from my understanding of my work and from the field, now that I've been into the real field, any decision aid that we build for this particular context has to talk about communication, it has to take into account very specifically these metacontextual factors, whether the patient is coming from a rural background or an urban background, literacy level of that patient, the extent of patient agency, the extent of their agency. Gender was also, I did not talk about this earlier, but gender was also played a huge role in whether the patient will have any agency or not, right? So many factors whether you have uh finances, you know, to pay for it or are you dependent on some others or you just don't have any, that's that's also a possible scenario, right? So, yeah, any decision aid that we build has to take into account all these factors because if it's not, then it's just it doesn't matter then, right? So, and one other thing that needs to be taken care of that India is geographically and culturally huge, linguistically diverse, right? So any decision aid that we are building, it has to also reflect that, right? It has to be also take into account all these because rural patients and rural family caregivers, the communication that looks like with them for the doctors is very different from those with the urban settings, those in the urban settings, right? So yeah, I am into talks with uh some people that I met at the NDM conference. Uh Robert was of huge help, right? But yeah, I'm yet to like, you know, full fletchedly get into this right now. because I've just recently started my first job as a lecturer, right? So, I'm waiting for the semester to get over and then finally come back to my research.

**Speaker 3:** Yeah, that's great Steph Nisha, I'm super excited to see folks in the community think about those macro macro cognitive factors and the things out there that sometimes even we forget about even though we're so focused on context. So and since you mentioned Robert, it kind of leads us to our last question, which is a bit of a fun question, but I think it'd be great to hear from you all be your favorite bit of guidance or most important lesson that you've gotten from our award namesake, Gary and Robert. Gary and or Robert. Might be work related or not, be something they shared either in person or in print. So let's just go around and have folks kind of reflect on what what's something that's really stuck with you from either. So we'll we'll start with Jordan for this one.

**Speaker 5:** Yeah, both obviously very very influential. I think, you know, the most salient thing with Gary is just, you know, the importance of he's really demonstrate the importance of framing things in terms of stories, right? He is sort of an expert at taking what he sees in the field, you know, unpacking that, understanding, you know, in terms of the cognitive work and some of the challenges that they experience and then reformulating that into compelling stories that both get sort of the attention of, you know, the right people in terms of future work and that also helps explain kind of the the importance of kind of what we're doing as a community. So I think that's what I'm appreciate most of Gary and all his books are great and extra examples of that. I will also say I haven't worked directly with Robert before. I have chatted with him and met him on a number of occasions and he's always extremely generous with his time and his and guidance and and always kind of open to conversations and giving feedback and and sharing ideas. And yeah, usually always wears a a really cool bolo tie. So that's a uh but yeah, he's he's they they are both great and and have been highly influential on me.

**Speaker 3:** Awesome. Nita, how about you?

**Speaker 7:** I just want to jump in and say I did not expect anyone to talk about Robert's fashion contributions. so thank you for that Jordan.

**Speaker 3:** If anyone talks about Gary's fashion, then we know we're really breaking new ground here. Nisha, how about you?

**Speaker 7:** Right. Well, I have to say a nice, fun conversation that I had with Robert, fun in retrospect, I would say. So we were discussing my work and he told me on email, just pin me down whenever you see me, right? And he's always surrounded by like so many people and I just finally I caught him at lunch this one day at the conference and we're talking about my work and how I can go about designing decision aids because I am also unsure about how to go about it, right? And and I tell him that, you know, I wish I would have conducted CDM interviews. after attending that CTA workshop, I really thought that you know, I could have done so much more with my work. And then he told me that, no, no, you have done CDM. It's like, no, no, I have not done CDM. I don't know where I miscommunicated, but I've definitely not done CDM. Then from that lunch area, he took me to a classroom with a board and a pen and then he's showing me all these decision scenarios and how to go about it. And then I say, um okay, so I have done this. He's like, yes, I know you have done this. I'm like, all right, so maybe I have done to some extent, I have done CDM. He's like, I know you have done CDM. I'm like, all right, so maybe I am, you know, in retrospect and then he just gave me this all knowing look. This look that is there very deep, straight into the eyes and then I realize that, you know, it's not just about the interviews also. I have been feeling quite outside of the community. Even when I came to the conference, I was just like, okay, I'm I am talking about NDM, but I don't really know it that well or you know, I I don't know CDM that well and all of that. Then he made me realize that I'm not outside of it. I am also a part of NDM community, right? And everyone in fact, they were they were all so welcoming and so encouraging. and I realized, okay, I'm also very much a part of this community, just need to nurture and grow in that way. So, and yeah, that really shifted my perspective for my work also, right? That I know that okay, so I have people to help me out in doing this work and you know, be there as a support. So that was very, very nice.

**Speaker 3:** Well, that's a great story and we are glad to have you as part of the community. Carrie, you're up next.

**Speaker 6:** So I have two brief, I've got one NDM related insight and then one life insight, both from Gary. since I've had the most exposure with him. The NDM related insight is just emphasizing the value and importance of qualitative data. We all know the quantitative data is important. I don't think there's any arguing that, but some of the most important insights that I've come across have been when we are digging into the qualitative data and conducting deep cognitive interview techniques. So I think that that cannot be understated or overstated enough. So the life insight, this came from Gary when he and I were in New York filming the masterclass in practical decision making online course that we've got. I it was my first time hailing a cab. I've never done it before. Gary's standing next to me and he's like, if you want to hail a cab right, you got to in New York, you have to make yourself taller. I'm five foot two. I think I'm as tall as Laura. So I so that's a big ask Gary, but I'm doing my best.

**Speaker 3:** Some New York lessons. Everyone needs them. Especially the first time there. All right, Reza, you're up last.

**Speaker 4:** Thank you, Brian. So yeah, I think that's great question. At least for me, I've been fortunate enough to receive a lot of valuable guidance and mentorship from both Gary and Robert. And that's invaluable. from Gary, one of the most important things I learned from Gary's, he always focus in on that concept of discovery and encourage people to do so. He always stress that research shouldn't be just done for the sake of research. It should be driven by curiosity and the pursuit of meaningful insights. And that's really a valuable lesson I learned from Gary. That advice keeps me really focus on asking the right questions and ensuring that the work that I do not only contributes to the field, but also makes a real difference. And with Robert, I was lucky enough to work with Robert during my time at Shadow box on micro cognition. Robert is a fantastic mentor. He has a very practical, realistic approach to his work. One of the biggest lessons that I can say I learned from Robert working with Robert Hoffman is his openness to learning from all kinds of perspectives. He he never limit himself to one way of thinking or one area of expertise. He's always open to new ideas, always curious, and he really encouraged that same mindset in others. When you work with Robert, you quickly pick up that there is always more to learn. That professional collaboration with Robert was absolutely amazing to learn all these things from Robert and the the friendship is continued. I I still sometimes talk with him and and I'm really honored to build up that that relationship and connection with both Roberts and Gary.

**Speaker 3:** Well, thank you so much for sharing everybody. I'm sure that they'll appreciate your words and help others get a slice of that experience as well. Congratulations again on your awards, well deserved and it's been a lot of fun. We hope that the audience finds their way to your papers and your other work. We'll put that up on the site for everybody to find. So thanks again for joining us and with that, for the NDM podcast, I'm Brian Moon.

**Speaker 2:** And I'm Laura Millitello. Learn more about naturalistic decision making and where to follow us by visiting naturalistic decisionmaking.org.